> 经过之前寒假的Netty开发。打算对操作系统的知识重新回顾一遍。俗话说，温故而知新，可以为师矣。
>
> 最近把操作系统回顾了一遍，同样是OneNote笔记再整理一份Markdown的版本。没有图床，所以就文字将就了。
>
> <small>笔记整了将近一半之后，发现CSDN有个老哥跟我学的课内容差不多，他2018年的笔记里图还挺多的，正好我这缺图片。还省得去网上百度找图了，哈哈。附上这位博主的操作系统的CSDN笔记链接[操作系统--作者Alatebloomer](https://blog.csdn.net/alatebloomer/category_7565411.html)</small> <==发现github上的这个笔记加载不出来图片，F12看了下被403了，估计是CSDN避免CSRF攻击，就禁止了github的图片加载请求，悲。（不过我本地还能看到图片，倒是还行，哈哈）
>
> 中间自己补了一些课程没有提及的章节，比如Linux的线程机制等（毕竟每个操作系统实现还是不一样的，还是最好有点针对性的了解一下）

# 1. 基本概念

## 1.1 操作系统-概述

> [操作系统 （计算机管理控制程序）]([https://baike.baidu.com/item/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/192?fr=aladdin](https://baike.baidu.com/item/操作系统/192?fr=aladdin))
>
> 操作系统(Operating System，简称OS)是管理[计算机](https://baike.baidu.com/item/计算机)[硬件](https://baike.baidu.com/item/硬件)与[软件](https://baike.baidu.com/item/软件)资源的[计算机程序](https://baike.baidu.com/item/计算机程序)。操作系统需要处理如管理与配置[内存](https://baike.baidu.com/item/内存)、决定[系统资源](https://baike.baidu.com/item/系统资源/974435)供需的优先次序、控制[输入设备](https://baike.baidu.com/item/输入设备/10823368)与[输出设备](https://baike.baidu.com/item/输出设备/10823333)、操作网络与管理文件系统等基本事务。操作系统也提供一个让用户与系统交互的操作界面。

### 1.1.1 操作系统的层次结构

操作系统位于硬件之上，应用程序之下。

操作系统的最重要部分即Kernel，俗称内核。

内核Kernel向外提供底层硬件的抽象接口实现，即系统调用System Call。

再外层的Shell、共用函数库lib则是对系统调用的简单封装。

最外层应用程序通过Shell编程or调用共用函数库来申请使用系统调用，进而与硬件进行交互。（实际应用层编程，无需关心硬件的差异性，仅需要调用操作系统提供的系统调用即可。）

![img](https://pic3.zhimg.com/80/416d6b469bda642f1edf7ff56c1aea4d_720w.jpg?source=1940ef5c)

> [shell、操作系统、内核是一个东西吗？](https://www.zhihu.com/question/37695460)
>
> 上图就是来自这个链接，没有图床就这样了。

## 1.2 OS内核-Kernel

> [内核和操作系统的区别](https://zhuanlan.zhihu.com/p/54665833)
>
> [Kernel (operating system)--wiki](https://en.wikipedia.org/wiki/Kernel_(operating_system))

操作系统OS内核Kernel的特征：

+ 并发

  计算机系统中同时存在多个运行的程序，需要OS管理和调度

+ 共享

  + "同时"访问
  + 互斥共享

+ 虚拟

  利用多道程序设计技术，让每个用户都觉得有一个计算机专门为他服务

+ 异步

  + 程序的执行不是一贯到底，而是走走停停，向前推进的速度不可预知
  + 只要运行环境相同，OS需要保证同一个程序的运行结果相同

操作系统的三大关注点：

+ CPU--OS--进程
+ Memory--OS--地址空间
+ 文件--OS--磁盘

![img](https://img-blog.csdn.net/20180426131125758?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0FsYXRlYmxvb21lcg==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)

> [一、绪论](https://blog.csdn.net/Alatebloomer/article/details/79819975)
>
> <small>估计看的网课都是同一个，正好缺图床。</small>

## 1.3 VMM虚拟机监视器

> [虚拟机监视程序--百度百科]([https://baike.baidu.com/item/%E8%99%9A%E6%8B%9F%E6%9C%BA%E7%9B%91%E8%A7%86%E7%A8%8B%E5%BA%8F/20839241?fromtitle=VMM&fromid=7047240&fr=aladdin](https://baike.baidu.com/item/虚拟机监视程序/20839241?fromtitle=VMM&fromid=7047240&fr=aladdin))
>
> 监控系统行为是[虚拟机](https://baike.baidu.com/item/虚拟机/104440)系统的核心任务监控系统可用于调度任务、[负载均衡](https://baike.baidu.com/item/负载均衡/932451)、向管理员报告软硬件故障，并广泛控制系统的使用情况。
>
> 虚拟化是从逻辑角度出发的资源配置方案，是对物理资源的一种抽象。抽象的结果是，在只有一台计算机硬件的情况下、通过虚拟化技术、可以让多个操作系统同时运行在此计算机硬件上，并且让这些操作系统都认为自己独享整个硬件，资源划分对操作系统是透明的。

![img](https://bkimg.cdn.bcebos.com/pic/b3119313b07eca801ea8eee99b2397dda144830b?x-bce-process=image/watermark,image_d2F0ZXIvYmFpa2U4MA==,g_7,xp_5,yp_5)

VMM将单独的机器接口转换成很多的幻象。每个这些接口（虚拟机）是一个原始计算机系统的有效副本，并完成所有的处理器指令。

```none
|VMs0、VMs1、VMs2...(多台虚拟机VMs)   |
|虚拟机监视器(VMM),也称为管理程序       |
|平台硬件(内存、CPU处理器、I/O设备)     |
```

常见的使用场景，就是VMware，大家常用的虚拟机程序啦。(物理硬件用同一套，VMM层抽象出接口，然后上层再搭载多个虚拟机操作系统)

![img](https://img-blog.csdn.net/20180426135244759?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0FsYXRlYmxvb21lcg==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)

> [一、绪论](https://blog.csdn.net/Alatebloomer/article/details/79819975)

## 1.4 虚拟机硬件

### 1.4.1 vCPU

+ **一个Socket套接字对应一个CPU。**

+ 然后一个CPU里面可以多核，每个核有可能超线程。

（假如一个CPU是4核8线程，那么每个CPU核超线程，一个核心能2个线程=>实际就是2个ALU计算单元模拟双线程）

---

举例：

+ 一个服务器4个CPU，每个CPU（4核4线程，`cpu cores:4`，`siblings:4`）， 那么这个CPU不具有超线程技术，总的vCPU即（4 * 4 = 16）。

+ 4个CPU，每个(`cpu cores:4`，`siblings:8`)，那么CPU超线程，总的vCPU即（4 * 4 * (8/4) ) = 32。

**vCPU即对应一个超线程（或线程）**。

---

#### 1. **物理CPU与VCPU的关系梳理**

> [物理CPU与VCPU的关系梳理](https://support.huawei.com/enterprise/zh/knowledge/EKB1000080054)

##### 1. **系统可用的VCPU****总数计算**

+ **系统可用的vCPU总数(逻辑处理器) = Socket数（CPU个数）x Core数（内核）x Thread数（超线程）** 

+ **1个VCPU = 1个超线程Thread**

如下图所示：

![img](https://download.huawei.com/mdl/image/download?uuid=b0015393a45e40d3b020aa5620bdd862)

+ **在不做VCPU预留的条件下，系统可以创建或运行VM的VCPU总数可远远大于实际可提供的VCPU数目**。

##### 2.  虚拟机VCPU的分配与调度

​	**对虚拟机来说，不直接感知物理CPU，虚拟机的计算单元通过vCPU对象来呈现。虚拟机只看到VMM呈现给它的vCPU**。在VMM中，每个vCPU对应一个VMCS（Virtual-Machine Control Structure）结构，当VCPU被从物理CPU上切换下来的时候，其运行上下文会被保存在其对应的VMCS结构中；当VCPU被切换到PCPU上运行时，其运行上下文会从对应的VMCS结构中导入到物理CPU上。通过这种方式，实现各vCPU之间的独立运行。

​	从虚拟机系统的结构与功能划分可以看出，客户操作系统与虚拟机监视器共同构成了虚拟机系统的两级调度框架，如图所示是一个多核环境下虚拟机系统的两级调度框架。**客户操作系统负责第2 级调度,即线程或进程在vCPU 上的调度（将核心线程映射到相应的VCPU上）**。虚拟机监视器负责第1 级调度, 即vCPU在物理处理单元上的调度。两级调度的调度策略和机制不存在依赖关系。vCPU调度器负责物理处理器资源在各个虚拟机之间的分配与调度,本质上即把各个虚拟机中的vCPU按照一定的策略和机制调度在物理处理单元上可以采用任意的策略来分配物理资源, 满足虚拟机的不同需求。<u>vCPU可以调度在一个或多个物理处理单元执行（**分时复用或空间复用物理处理单元**）, 也可以与物理处理单元建立一对一固定的映射关系（限制访问指定的物理处理单元）</u>。

![img](https://download.huawei.com/mdl/image/download?uuid=862b7280a78141ce9e3d0662ddf898db)

##### 3. CPU QoS说明

​	Hypervisor层根据**分时复用**的原理实现对VCPU的调度，**CPU QoS的原理是定期给各VCPU分配运行时间片，并对各VCPU运行的时间进行记账，对于消耗完时间片的虚拟CPU将被限制运行，直到获得时间片**。以此控制虚拟机获得物理计算资源的比例。<u>以上分配时间片和记账的时间周期很短，对虚拟机用户来说会感觉一直在运行</u>。

+ CPU预留定义了分配给该VM的最少CPU资源。

+ CPU限制定义了分配虚拟机占用CPU资源的上限。

+ **CPU份额定义多个虚拟机在竞争CPU资源的时候按比例分配**。

+ **CPU份额只在各虚拟机竞争计算资源时发挥作用，如果没有竞争，有需求的虚拟机可以独占主机的物理CPU资源**。

​	<u>如果虚拟机根据份额值计算出来的计算能力小于虚拟机预留值，调度算法会优先按照虚拟机预留值分配给虚拟机，对于预留值超出按份额分配的计算资源的部分，调度算法会从主机上其他虚拟机的CPU上按各自的份额比例扣除</u>。

​	**如果虚拟机根据份额值计算出来的计算能力大于虚拟机预留值，那么虚拟机的计算能力会以份额值计算为准**。以一台主频为2800MHz的单核物理机为例，如果满负载运行3台单VCPU的虚拟机A、B、C，分配情况如下。

<table>
  <tr>
  	<th>主机CPU</th>
    <th>VM</th>
    <th>份额</th>
    <th>预留</th>
    <th>按份额分配</th>
    <th>最终分配</th>
  </tr>
  <tr>
    <td rowspan="3">2800MHz</td>
    <td>A</td>
    <td>1000</td>
    <td>700MHz</td>
    <td>400MHz</td>
    <td>400+100+200=700MHz</td>
  </tr>
  <tr>
    <td>B</td>
    <td>2000</td>
    <td>0</td>
    <td>800MHz</td>
    <td>800-100=700MHz</td>
  </tr>
  <tr>
    <td>C</td>
    <td>4000</td>
    <td>0</td>
    <td>1600MHz</td>
    <td>1600-200=1400MHz</td>
  </tr>
</table>

#### 2. CPU是按什么原则划分为VCPU的

> [CPU是按什么原则划分为VCPU的](https://bbs.51cto.com/thread-1118522-1.html)

1. vCPU和pCPU的关系不是数量，当被底层虚拟化之后，任何一个vCPU都是用到所有的pCPU核心总体的百分比，不是某一个核心这么去看的，并没有对应的关系，也不是一个很绝对的分配到具体某个核心。虽然底层虚拟化是直接用到了底层硬件，但是底层毕竟也是一个系统，并不是旁路的。

2. xenserver是权重，vmware也是，hyperv是可以有频率的指定的，其实无效

3. 目前citrix，vmwar和hyperv都没有vCPU对应核心的绑定功能。

4. 虽然有最佳化计算公式，但是每一个项目都以POC为主导，计算公式说到底只是一种预销售的工具。

![BaiduShurufa_2014-8-4_16-1-13.png](https://s3.51cto.com//wyfs02/M01/44/1D/wKioL1PfQJaSvu87AAHhuuWPJMk208_small.jpg)

![BaiduShurufa_2014-8-4_16-5-37.png](https://s3.51cto.com/wyfs02/M01/44/1D/wKiom1PfP5HTqQwWAAGHdjrcV4I825.jpg)

![BaiduShurufa_2014-8-4_16-7-26.png](https://s3.51cto.com/wyfs02/M01/44/1D/wKiom1PfP7uAoMEoAAGwzfI1cK0894.jpg)

![BaiduShurufa_2014-8-4_16-9-56.png](https://s3.51cto.com/wyfs02/M02/44/1E/wKioL1PfQR6ijYPgAAEr_sjg400988.jpg)

![BaiduShurufa_2014-8-4_16-10-36.png](https://s3.51cto.com/wyfs02/M01/44/1E/wKioL1PfQVSiDd2xAAJbONrPrwo619.jpg)

**但最后重复一句，任何的项目都要先做poc，然后是压力测试，没有这些的话，公式也不会给你带来什么帮助，一切都要看实际情况**！

#### 3. vcpu和cpu的关系

> [vcpu和cpu的关系](https://blog.csdn.net/tiantao2012/article/details/78019064)

在qemu中用每个thread来代替一个cpu

![img](https://img-blog.csdn.net/20170918144748578?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdGlhbnRhbzIwMTI=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center)

#### 4. AWR报告中的CPUs、Cores是哪个是物理核数、哪个是逻辑核数？

> [AWR报告中的CPUs、Cores是哪个是物理核数、哪个是逻辑核数？](http://blog.itpub.net/28853590/viewspace-2147565/)

# 2. 启动；中断、异常、系统调用

## 2.1 启动

### 2.1.1 计算机体系结构概述

> [bios--百度百科](https://baike.baidu.com/item/bios/91424?fr=aladdin)
>
> BIOS是英文"**Basic Input Output System**"的[缩略词](https://baike.baidu.com/item/缩略词)，直译过来后中文名称就是"**基本输入输出系统**"。在[IBM](https://baike.baidu.com/item/IBM/9190) PC兼容系统上，是一种业界标准的[固件](https://baike.baidu.com/item/固件/627829)[接口](https://baike.baidu.com/item/接口/2886384)。 [1] BIOS这个字眼是在1975年第一次由[CP/M](https://baike.baidu.com/item/CP%2FM)操作系统中出现。  **BIOS是[个人电脑](https://baike.baidu.com/item/个人电脑/3688503)启动时加载的第一个软件。**
>
> 其实，它是一组固化到[计算机](https://baike.baidu.com/item/计算机)内[主板](https://baike.baidu.com/item/主板)上一个[ROM](https://baike.baidu.com/item/ROM)[芯片](https://baike.baidu.com/item/芯片)上的[程序](https://baike.baidu.com/item/程序)，它**保存着计算机最重要的基本输入输出的程序、开机后自检程序和系统自启动程序**，它可从CMOS中读写[系统设置](https://baike.baidu.com/item/系统设置)的具体信息。其**主要功能是为计算机提供最底层的、最直接的硬件设置和控制**。此外，BIOS还向作业系统提供一些系统参数。系统硬件的变化是由BIOS隐藏，程序使用BIOS功能而不是直接控制硬件。**现代作业系统会忽略BIOS提供的抽象层并直接控制硬件组件**。
>
> 当今，此系统已成为一些病毒[木马](https://baike.baidu.com/item/木马/530)的目标。一旦此系统被破坏，其后果[不堪设想](https://baike.baidu.com/item/不堪设想/1768458)。

处理器CPU、内存Memory、其他I/O设备等，通过总线连接和通信。

DSIK（硬盘/磁盘）：存放OS。

BIOS：基本I/O处理系统

Bootloader：加载OS

POST(加电自检)：寻找显卡和执行BIOS

> [开机自检--百度百科]([https://baike.baidu.com/item/%E5%BC%80%E6%9C%BA%E8%87%AA%E6%A3%80/5199677?fr=aladdin](https://baike.baidu.com/item/开机自检/5199677?fr=aladdin))
>
> 也称[上电自检](https://baike.baidu.com/item/上电自检/10976730)(POST，Power On Self Test)。 指计算机系统，接通电源，（BIOS程序）的行为，包括对CPU、系统主板、基本内存、[扩展内存](https://baike.baidu.com/item/扩展内存/7353990)、系统ROM BIOS等器件的测试。如发现错误，给操作者提示或警告。简化或加快该过程，可使系统能够[快速启动](https://baike.baidu.com/item/快速启动/5173174)。

### 2.1.2 启动大致流程

> [BIOS在POST时 最先检测的硬件是内存还是CPU?](https://zhidao.baidu.com/question/2137750054738218748.html)
>
> [BootLoader--百度百科](https://baike.baidu.com/item/BootLoader/8733520?fr=aladdin)
>
> 在嵌入式操作系统中，BootLoader是在[操作系统](https://baike.baidu.com/item/操作系统)内核运行之前运行。可以初始化硬件设备、建立内存空间映射图，从而将系统的软硬件环境带到一个合适状态，以便为最终调用[操作系统内核](https://baike.baidu.com/item/操作系统内核/297824)准备好正确的环境。

大致加载步骤：

1. 计算机启动时，首先执行BIOS程序中的自检程序，俗称POST（Power On Self Test），对计算机硬件设备进行检查，确定无故障后再将CPU执行权交还给BIOS主体。
2. BIOS加载Bootloader程序，把CPU控制权交给Bootloader。
3. BootLoader从硬盘Disk加载OS，使操作系统在内存中运行，把CPU控制权交给OS。（为了方便BootLoader记载OS，一般把OS放在硬盘的第一个扇区，比如Windows的C盘）

+ BIOS
  + 将Bootloader从磁盘的引导扇区（512字节）加载到0x7c000。
  + 跳转到CS:IP = 0000:7c00
+ Bootloader
  + 将操作系统的代码和数据从硬盘加载到内存中
  + 跳转到操作系统的起始地址

![img](https://img-blog.csdn.net/20180406172135760?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0FsYXRlYmxvb21lcg==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)

> [二、操作系统启动、中断、异常和系统调用](https://blog.csdn.net/Alatebloomer/article/details/79834924)

## 2.2 系统调用、异常、中断

### 2.2.1 定义

+ 系统调用（来源于应用程序）

  应用程序主动向操作系统发出服务请求

+ 异常（来源于不良的应用程序）

  非法指令或者其他坏的处理状态（如：内存出错）

+ 中断（来源于外设）

  来自不同的硬件设备的计时器和网络的中断

从"产生源头"、"处理时间"、"响应处理"三个角度分析三者的异同点：

1. 产生源头
   + 系统调用：应用程序请求操作系统提供服务<small>（比如操作文件）</small>
   + 异常：应用程序意想不到的行为<small>（比如程序出现除零操作，系统执行指令出错等）</small>
   + 中断：外设<small>（比如网卡获取数据，网卡驱动程序将数据加载到内存后，要求CPU处理。更常见的就是键盘、鼠标操作）</small>
2. 处理时间
   + 系统调用：异步或同步<small>（同步：打开文件，等待返回文件描述符，得到结果前阻塞；异步：查看socket是否有接受到数据，没有则直接返回，不阻塞）</small>
   + 异常：同步<small>(除零操作，直接抛出异常。如果没有设置异常处理函数，程序进程崩溃并异常退出。)</small>
   + 中断：异步<small>（用户进程无感知，毕竟什么时候别的外设要占用CPU是完全随机的）</small>
3. 响应处理
   + 系统调用：等待和持续<small>（同步的系统调用则等待OS返回结果；异步则执行向下执行，OS返回结果时执行回调函数）</small>
   + 异常：杀死或者重新执行意想不到的应用程序指令<small>（一般没有预设异常处理函数，就是进程崩溃。）</small>
   + 中断：持续，对用户程序是透明的<small>（很好理解，你完全无需关心网卡等外设什么时候有数据被CPU处理了。）</small>

![img](https://img-blog.csdn.net/20180406175715834?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0FsYXRlYmxvb21lcg==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)

> [二、操作系统启动、中断、异常和系统调用](https://blog.csdn.net/Alatebloomer/article/details/79834924)

### 2.2.2 系统调用

> [系统调用--百度百科]([https://baike.baidu.com/item/%E7%B3%BB%E7%BB%9F%E8%B0%83%E7%94%A8](https://baike.baidu.com/item/系统调用))
>
> 由[操作系统](https://baike.baidu.com/item/操作系统/192)实现提供的所有系统调用所构成的集合即[程序接口](https://baike.baidu.com/item/程序接口/150383)或应用编程接口(Application Programming Interface，API)。是[应用程序](https://baike.baidu.com/item/应用程序)同系统之间的接口。

​	在计算机运行中，内核Kernel是被信任的第三方，所以只有内核可以执行特权指令（内核态）。为了方便应用程序（用户态）与硬件交互，操作系统提供了一套系统调用System Call接口API。应用程序不能直接操作硬件，因为权限不足（用户态），需要通过系统调用请求，让具有权限的内核Kernel代替用户程序执行特权指令与硬件设备交互（内核态），并把结果返回给用户程序。

​	实际开发中，程序访问的主要是高层次的API接口而不是直接进行系统调用。

+ WIN32 API用于Windows
+ POSIX API用于POSIX-based systems（包括UNIX、Linux、Mac OS X的所有版本）
+ Java API用于Java虚拟机（JVM）

*（POSIX提供一些可移植的系统调用标准。UNIX等遵循这种标准开发，使得操作系统具有可以执行。)*

*（Java虚拟机JVM提供的API不是系统调用，但是最后也是调用的POSIX API，所以说JVM跨平台，移植性好。）*

*（POSIX之所以说以执行好，就是因为UNIX、LINUX等都按照它这个标准去实现，那么高层应用调用POSIX API就根本不用管底层操作系统OS是UNIX内核还是Linux内核等。也就是说POSIX的可移植性强，源自大家都按它标准实现，使得上层应用无需关系底层差异性，反正用的都是同一套标准的POSIX API。）*

![img](https://img-blog.csdn.net/2018040619195766?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0FsYXRlYmxvb21lcg==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)

> [POSIX-可移植操作系统接口--百度百科]([https://baike.baidu.com/item/%E5%8F%AF%E7%A7%BB%E6%A4%8D%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F%E6%8E%A5%E5%8F%A3/12718298?fromtitle=POSIX&fromid=3792413&fr=aladdin](https://baike.baidu.com/item/可移植操作系统接口/12718298?fromtitle=POSIX&fromid=3792413&fr=aladdin))
>
> <small>**可移植操作系统接口**（英语：Portable Operating System Interface，缩写为**POSIX**）是[IEEE](https://baike.baidu.com/item/IEEE)为要在各种[UNIX](https://baike.baidu.com/item/UNIX)[操作系统](https://baike.baidu.com/item/操作系统)上运行软件，而定义[API](https://baike.baidu.com/item/API)的一系列互相关联的标准的总称，其正式称呼为IEEE Std 1003，而国际标准名称为[ISO](https://baike.baidu.com/item/ISO)/[IEC](https://baike.baidu.com/item/IEC) 9945。此标准源于一个大约开始于1985年的项目。POSIX这个名称是由[理查德·斯托曼](https://baike.baidu.com/item/理查德·斯托曼)（RMS）应IEEE的要求而提议的一个易于记忆的名称。它基本上是Portable Operating System Interface（可移植操作系统接口）的缩写，而**X**则表明其对Unix API的传承。</small>
>
> [二、操作系统启动、中断、异常和系统调用](https://blog.csdn.net/Alatebloomer/article/details/79834924)

### 2.2.3 异常

> [异常(计算机术语)--百度百科]([https://baike.baidu.com/item/%E5%BC%82%E5%B8%B8/5952477#viewPageContent](https://baike.baidu.com/item/异常/5952477#viewPageContent))
>
> 异常指的是在程序运行过程中发生的异常事件，通常是由外部问题（如硬件错误、输入错误）所导致的。在Java等面向对象的编程语言中异常属于对象。

​	一般进程出现异常时，会产生对应的异常编号。操作系统尝试保存现场、异常处理（杀死产生异常的程序or重新执行异常指令）、恢复现场。

*（开发中，如果没有异常处理函数，那一般遇到的就是程序崩溃。如果在一些容器、服务下运行，好一点就是会保留一些错误提示。常见的WEB开发就是会保留错误提示。）*

### 2.2.4 中断

> [中断--百度百科]([https://baike.baidu.com/item/%E4%B8%AD%E6%96%AD/3933007?fr=aladdin](https://baike.baidu.com/item/中断/3933007?fr=aladdin))
>
> 中断是指计算机运行过程中，出现某些意外情况需主机干预时，机器能自动停止正在运行的程序并转入处理新情况的程序，处理完毕后又返回原被暂停的程序继续运行。

中断还可以细分为**硬中断**和**软中断**。

硬件中断（Hardware Interrupt）：

- 可屏蔽中断（maskable interrupt）。[硬件中断](https://baike.baidu.com/item/硬件中断)的一类，可通过在中断屏蔽寄存器中设定位掩码来关闭。
- 非可屏蔽中断（non-maskable interrupt，NMI）。硬件中断的一类，无法通过在中断屏蔽寄存器中设定位掩码来关闭。典型例子是时钟中断（一个硬件时钟以恒定频率—如50Hz—发出的中断）。
- 处理器间中断（interprocessor interrupt）。一种特殊的硬件中断。由处理器发出，被其它处理器接收。仅见于多处理器系统，以便于[处理器](https://baike.baidu.com/item/处理器)间通信或同步。
- 伪中断（spurious interrupt）。一类不希望被产生的硬件中断。发生的原因有很多种，如中断线路上电气信号异常，或是中断请求设备本身有问题。

软件中断（Software Interrupt）：

- 软件中断。是一条CPU指令，用以自陷一个中断。由于软中断指令通常要运行一个切换CPU至内核态（Kernel Mode/Ring 0）的子例程，它常被用作实现[系统调用](https://baike.baidu.com/item/系统调用)（System call）。

中断处理时，硬件、软件的相关要点：

1. 硬件：
   + 设置中断标记[CPU初始化]
     1. 将内部、外部事件设置中断标记
     2. 中断事件的ID
2. 软件：
   + 保存当前处理状态
   + 中断服务程序处理
   + 清理中断标记
   + 恢复之前保存的处理状态

### 2.2.5 跨越操作系统边界的开销

在执行时间上的开销超过程序调用。

开销：

+ 建立中断/异常/系统调用号与对应服务例程映射关系的初始化开销（一般是维护一张表来表示关系）
+ 建立内核堆栈（暂存当时的CPU运行状态）
+ 验证参数
+ 内核态映射到用户态的地址空间，更新页面映射权限（内核态操作，如果数据是用户态需要的，还需要把内核态内存区域的数据拷贝一份到用户态）
+ 内核态独立地址空间TLB刷新（缺页中断等，刷新TLB快表，TLB用于缓存常访问的内存区域地址映射关系）

# 3. 内存、地址空间、内存分配

## 3.1 内存和地址空间

### 3.1.1 计算机基本硬件结构

![img](https://img-blog.csdn.net/20180406210026897?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0FsYXRlYmxvb21lcg==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)

```none
CPU        内存          设备(I/O)
|           |             |
<-----------总线------------->

CPU:进程在这里执行操作(CPU有运算器、寄存器、控制器、缓存Cache、存储处理单元MMU)
内存:执行的程序(进程)和数据驻留于此
设备(I/O):磁盘设备、鼠标、键盘、网卡等
```

虽然不同类型的硬件没有可比性，但是一般来说CPU工作速率>内存>设备(I/O)，这里只是模糊又不专业的说法。

CPU取指or取数据时，一般按照如下顺序查找，如果找不到就一直往下，找到了就不再往下了，实在找不到那就抛除异常了。

+ 寄存器（最快）
+ L1缓存
+ L2缓存
+ L3缓存<small>（多核CPU，3级缓存是多核共享的。一般1级和2级缓存都是KB大小，这个3级缓存MB大小）</small>
+ L4及等多的缓存，一般都是MB大小，如果有的话...<small>(据说L4缓存带来的速度提升不明显，我的电脑就有4级缓存)</small>

+ 主存<small>（也就是内存Memory）</small>
+ 虚拟内存<small>（也就是硬盘，需要操作系统支持虚拟化内存的技术。最慢）</small>

*<small>（对自己的电脑硬件配置不清楚的，推荐可以下载个[HWiNFO](https://www.hwinfo.com/)，算是各种参数比较齐全的硬件检测软件了吧。）</small>*

> [MMU--百度百科](https://baike.baidu.com/item/MMU/4542218?fr=aladdin)
>
> MMU是Memory Management Unit的缩写，中文名是[内存管理](https://baike.baidu.com/item/内存管理)单元，有时称作**分页内存管理单元**（英语：**paged memory management unit**，缩写为**PMMU**）。它是一种负责处理[中央处理器](https://baike.baidu.com/item/中央处理器)（CPU）的[内存](https://baike.baidu.com/item/内存)访问请求的[计算机硬件](https://baike.baidu.com/item/计算机硬件)。它的功能包括[虚拟地址](https://baike.baidu.com/item/虚拟地址)到[物理地址](https://baike.baidu.com/item/物理地址)的转换（即[虚拟内存](https://baike.baidu.com/item/虚拟内存)管理）、内存保护、中央处理器[高速缓存](https://baike.baidu.com/item/高速缓存)的控制，在较为简单的计算机体系结构中，负责[总线](https://baike.baidu.com/item/总线)的[仲裁](https://baike.baidu.com/item/仲裁)以及存储体切换（bank switching，尤其是在8位的系统上）。
>
> [三、计算机体系结构与内存体系、内存分配](https://blog.csdn.net/Alatebloomer/article/details/79836751)

### 3.1.2 物理地址和虚拟(逻辑)地址-简述

> [物理地址 （CPU中相关术语）--百度百科]([https://baike.baidu.com/item/%E7%89%A9%E7%90%86%E5%9C%B0%E5%9D%80/2901583?fr=aladdin](https://baike.baidu.com/item/物理地址/2901583?fr=aladdin))
>
> 在[存储器](https://baike.baidu.com/item/存储器/1583185)里以[字节](https://baike.baidu.com/item/字节/1096318)为单位存储信息，为正确地存放或取得信息，每一个字节单元给以一个唯一的[存储器地址](https://baike.baidu.com/item/存储器地址/7874173)，称为物理地址（Physical Address），又叫[实际地址](https://baike.baidu.com/item/实际地址/1061693)或[绝对地址](https://baike.baidu.com/item/绝对地址/573580)。
>
> [虚拟地址--百度百科]([https://baike.baidu.com/item/%E8%99%9A%E6%8B%9F%E5%9C%B0%E5%9D%80/1329947?fr=aladdin](https://baike.baidu.com/item/虚拟地址/1329947?fr=aladdin))
>
> 虚拟地址是Windows程序时运行在386保护模式下，这样程序访问[存储器](https://baike.baidu.com/item/存储器/1583185)所使用的[逻辑地址](https://baike.baidu.com/item/逻辑地址/3283849)称为虚拟地址，与实地址模式下的分段地址类似，虚拟地址也可以写为“段：[偏移量](https://baike.baidu.com/item/偏移量/9180391)”的形式，这里的段是指段选择器。

​	其实不管用了什么技术，一般而言，CPU找数据顶多向下找到内存，不会再去找硬盘，就算真有从硬盘上取数据，那其实也是把硬盘数据先取到内存，然后CPU再从内存中取数据。简言之，**CPU除了从寄存器、CPU缓存中取数据，那么就只能从内存中取数据**。

1. 逻辑地址范围

   32位操作系统，理论上可以寻址2<sup>32</sup>Byte内存，也就是4GB内存，这种理论上的出的就是**逻辑地址**的寻址范围了。*<small>（64位理论上可以寻址2<sup>64</sup>Byte的内存，但是由于内存寻址一般需要页表等机制管理，这个数量级太大了，所以其实也没操作系统真的用到那么大范围的逻辑地址）</small>*

2. 物理地址范围

   物理地址，那就是看内存具体多大了。要是内存条就256MB，那就是256MB的**物理地址**的寻址范围了。*<small>（早期计算机的内存条贵，所以很小，现在一般至少都8GB内存，夸张的也有上百GB内存的计算机）</small>*

物理地址空间——硬件支持的地址空间

逻辑地址空间——一个运行的程序所拥有的内存范围

*这里不展开逻辑地址和物理地址的具体描述，知道物理地址对应内存上的地址即可，而逻辑地址不过是进程中使用的地址，实际还需要翻译成物理地址，才能真正从内存上读取数据。过去32位系统，逻辑地址范围>物理地址范围，所以才会衍生出这种技术。*

​	CPU从内存（主存）取数据时，如果寄存器和缓存中没有，那么需要知道数据在内存的哪个<u>物理地址</u>上，而CPU是不清楚什么<u>逻辑地址</u>、<u>物理地址</u>这些乱七八糟的。CPU根据执行的指令中的地址寻找数据时，CPU把这个地址交给MMU，MMU会帮CPU把代码中的**逻辑地址**翻译成**物理地址**，进而CPU能够访问到内存上实际物理地址的数据。（实际CPU访问MMU之前，会先看看TLB快表看看有没有对应的逻辑地址-物理地址的映射记录，有的话就不用劳烦MMU转换了。）

更具体一些的描述：

1. 操作系统，事先建立逻辑地址和物理地址的映射关系。（维护页表等）
2. CPU运算器需要某个逻辑地址的内存内容
3. 内存管理单元MMU寻找在逻辑地址和物理地址之间的映射（这个需要操作系统事先建立好）
4. CPU的控制器往**总线**发送在物理地址的内存内容的请求
5. 内存收到总线上来自CPU的请求，发送物理地址内存的内容给CPU
6. CPU获取数据后继续执行指令

**操作系统维护程序虚拟地址与物理地址之间的映射关系**。如果CPU请求的数据在内存中不存在并触发异常时（比如超出物理地址的范围之类的），需要操作系统去处理异常。

![img](https://img-blog.csdn.net/20180407142744589?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0FsYXRlYmxvb21lcg==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)

![img](https://img-blog.csdn.net/20180407143919790?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0FsYXRlYmxvb21lcg==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)

> [TLB-转译后备缓冲区--百度百科]([https://baike.baidu.com/item/%E8%BD%AC%E8%AF%91%E5%90%8E%E5%A4%87%E7%BC%93%E5%86%B2%E5%8C%BA/22685572?fromtitle=TLB&fromid=2339981&fr=aladdin](https://baike.baidu.com/item/转译后备缓冲区/22685572?fromtitle=TLB&fromid=2339981&fr=aladdin))
>
> **转译后备缓冲器**，也被翻译为**页表缓存**、**转址旁路缓存**，为[CPU](https://baike.baidu.com/item/CPU)的一种缓存，由存储器管理单元用于改进[虚拟地址](https://baike.baidu.com/item/虚拟地址)到物理地址的转译速度。当前所有的桌面型及服务器型处理器（如 [x86](https://baike.baidu.com/item/x86)）皆使用TLB。TLB具有固定数目的空间槽，用于存放将虚拟地址映射至[物理地址](https://baike.baidu.com/item/物理地址)的标签页表条目。为典型的结合存储（content-addressable memory，首字母缩略字：CAM）。其搜索关键字为虚拟内存地址，其搜索结果为物理地址。如果请求的虚拟地址在TLB中存在，CAM 将给出一个非常快速的匹配结果，之后就可以使用得到的物理地址访问存储器。如果请求的虚拟地址不在 TLB 中，就会使用标签页表进行虚实地址转换，而标签页表的访问速度比TLB慢很多。有些系统允许标签页表被交换到次级存储器，那么虚实地址转换可能要花非常长的时间。
>
> [四、地址空间与内存分配](https://blog.csdn.net/Alatebloomer/article/details/79841088)

### 3.1.3 操作系统的内存管理-简述

在操作系统中，管理内存有多种方式：

+ 程序重定向
+ 分段
+ 分页
+ 虚拟内存
+ 按需分页虚拟内存

内存的管理和使用，需要硬件也参与实现。可以说内存管理是高度依赖于硬件的。（比如MMU内存管理单元，作为硬件组件负责处理CPU的内存访问请求）

![img](https://img-blog.csdn.net/20180406210626311?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0FsYXRlYmxvb21lcg==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)

![img](https://img-blog.csdn.net/20180406211817815?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0FsYXRlYmxvb21lcg==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)

上图中的P2、P3和P4的相关数据被放在磁盘中。<u>程序看到的空间是逻辑地址空间，主存和磁盘是物理地址空间</u>。

操作系统的内存管理：

1. 抽象，逻辑地址空间；
2. 保护，独立地址空间；
3. 共享，访问相同内存；
4. 虚拟化，更多的地址空间，最需要的数据放在内存中，暂时不需要的放在磁盘。

操作系统管理内存的不同机制： 
程序重定位，分段，分页，虚拟内存（ 目前多数系统(如Linux)采用按需页式虚拟存储）

管理的实现高度依赖于硬件，要知道内存架构，MMU（内存管理单元）——硬件组件中负责处理CPU的内存访问请求

> 推荐几篇文章<small>(内容比较多，就不像之前的计网笔记那样给出概述了。)</small>
>
> [MMU和cache学习](https://blog.csdn.net/chinesedragon2010/article/details/5922324)
>
> [物理地址和虚拟地址1 （MMU）](https://www.cnblogs.com/leaven/archive/2011/04/18/2019696.html)
>
> [三、计算机体系结构与内存体系、内存分配](https://blog.csdn.net/Alatebloomer/article/details/79836751) <= 上面2大图和下半部分文字来自这个文章

## 3.2 连续内存分配

### 3.2.1 连续内存分配的内存碎片问题

内存碎片问题，简单来说就是内存中存在某些空闲却又没法被使用的内存。

内存碎片一般可以分为两种：

+ 外部碎片

  在分配单元间未使用的内存

+ 内部碎片

  在分配单元中未使用的内存

外部碎片举例：200MB剩余内存，有两个99MB程序运行后，只剩2MB内存。这2MB虽然是空闲内存，但是太小，别人也用不上，就是外碎片了。

内部碎片举例：200MB剩余内存，有个程序申请要占用这200MB内存。<small>（其实内存一般也不可能完全用上，这里就假设）</small>，这时候不存在外碎片了。但是这个进程其实一开始初始化时用了200MB，后来也就只保留100MB常用数据在内存中，导致还有100MB内存空间浪费，这100MB空间就是内碎片。

![img](https://img-blog.csdn.net/20180407145218898?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0FsYXRlYmxvb21lcg==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)

> [四、地址空间与内存分配](https://blog.csdn.net/Alatebloomer/article/details/79841088)

### 3.2.2 分区的动态分配

简单的内存管理方法：

+ 当一个程序准许在内存中时，分配一个连续的内存区间。
+ 分配一个连续的内存区间给运行的程序以访问数据。

常见分配策略：

+ 首次适配（第一适配）
+ 最优适配
+ 最差适配

---

下面拿一个举例来看看首次适配、最优适配、最差适配，这三者的区别：

假设现在内存中，有3段连续且空闲的内存空间（100MB、50MB、200MB），这时候有一个40MB的程序想要到内存中运行。

+ 首次适配：使用100MB的内存空间，即使用内存寻址最先寻找到的空闲区域。占用后，内存空闲空间分布（60MB，50MB、200MB）。
+ 最优适配：使用50MB的内存空间，即寻找和程序所需要的内存空间按最接近的一个区域。占用后，内存空闲空间分布（100MB，10MB、200MB）。
+ 最差适配：使用200MB的内存空间，即使用当前最大的空闲内存区域。占用后，内存空闲空间分布（100MB，10MB、160MB）。

---

对比：

+ 第一适配：
  + 优势：
    1. 简单
    2. 使得地址空间的结尾部分易于产生更大的空闲块
  + 劣势：
    1. 外部碎片
    2. 不确定性
+ 最优适配：
  + 优势：
    1. 比较简单
    2. 当大部分分配是小尺寸时非常有效
  + 劣势：
    1. 外部碎片
    2. 重分配慢
    3. 易产生很多没用的微小外碎片（不怎么好）
+ 最差适配
  + 优势：
    1. 简单
    2. 假如内存分配主要是中等尺寸，效果最好
  + 劣势：
    1. 外部碎片
    2. 重分配慢
    3. 内存空间要求大的进程可能分配不到内存（因为每次都把大的连续空间拆了）

### 3.2.3 压缩式碎片整理

​	重置程序以合并孔洞（指合并内存外碎片）。要求所有程序是动态可重置的（也就是要求程序能保证在内存挪动位置后还能正常运行，不会出现寻址异常等问题）。

​	需要考虑重定向的时机，比如运行时挪动可能导致代码寻址错误。再者需要考虑开销，如果频繁重定向，挪动内存分配的空间，也会浪费很多CPU事件。

```none
|进程1|                              |进程1|
|20MB|                              |进程2| 
|进程2|   ===== 压缩式碎片整理 =====>  |进程3| 
|30MB|                              |20MB|
|进程3|                              |30MB|
|40MB|                              |40MB|

这样就把空闲的20MB、30MB、40MB合并成一个连续的90MB的内存空间了。
```

### 3.2.4 交换式碎片整理

​	优先给需要更多内存的程序加载到内存，其余的等待。比如多个程序申请且占用内存后，剩余的内存空间很小，不够接下去的程序运行，就把一些程序申请后多余没使用的内存回收，交给新的程序使用。要是还不够，先把程序挂载到硬盘（虚拟内存）。

​	简言之，把内存中的程序换下来（挂载），硬盘的程序加载到内存中。

# 4. 非连续内存分配

## 4.0 引言

​	出现非连续内存分配，那么肯定是因为单纯的连续内存分配不能满足复杂操作系统的内存分配需求。

连续内存分配的缺点：

+ 分配给一个程序的物理内存是连续的
+ **内存利用率较低**
+ 有外碎片、内碎片的问题

非连续分配的优点

+ 一个程序的物理地址空间是非连续的
+ 更好的内存利用和管理
+ **允许共享代码与数据**（共享库等......）
+ 支持动态加载和动态链接

非连续分配的缺点

+ 需要考虑如何建立虚拟地址和物理之间的转换<small>（其实现在实现都很成熟了，应该也不是啥大问题了）</small>
  + 软件方案
  + 硬件方案
    + 分段
    + 分页

连续内存分配，可以说进程就是完全互相隔离的，要实现进程间通信比较难。后来的分连续分配，对进程间通讯的支持就很好了。

非连续内存分配，如果纯软件方案实现，难度大开销大，不现实。一般采取软硬件相结合的方式，即硬件方面设计时就考虑分段、分页等问题，然后软件按照硬件规定的分段、分页标准具体设计规划内存的非连续分配。

## 4.1 分段(Segmentation)

### 4.1.1 程序的分段地址空间	

​	早期设计时，按照应用的特点来进行分离和管理内存空间。（比如进程的函数调用栈，都统一放到某一段空间内去分配）

​	虽然程序员在编程时获取的是连续的逻辑地址空间，但是实际在内存中运行时，根据进程不同的组成部分，把其拆分到不同的物理地址空间中。（进程由堆、运行栈、程序数据、程序text字段组成，那么把每个进程的堆都放到内存某一片区域，运行栈、程序数据同理。而程序text段还可以细分成库和用户代码，再放到不同物理地址空间。库是多个进程可以共享的，之后出现进程调用相同的库就不用重复加载了。）

​	这样分段，使得进程虚拟地址空间划分到物理地址空间的映射有一定可控性，也可以实现部分物理空间共用。再者，这样方便把控权限，保证安全，比如把库等需要系统调用的，放到"内核态"使用的内存空间。

*（一般而言，操作系统会把虚拟地址划分成内核态区域和用户态区域。）*

![img](https://timgsa.baidu.com/timg?image&quality=80&size=b9999_10000&sec=1596452791980&di=241d98dc422946a96a4c667505d766fa&imgtype=0&src=http%3A%2F%2Finews.gtimg.com%2Fnewsapp_bt%2F0%2F12010650392%2F641.jpg)

![img](https://img-blog.csdn.net/20180407160437973?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0FsYXRlYmxvb21lcg==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)

![img](https://img-blog.csdn.net/20180407160527174?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0FsYXRlYmxvb21lcg==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)

> [四、地址空间与内存分配](https://blog.csdn.net/Alatebloomer/article/details/79841088)

### 4.1.2 分段寻址方案

​	在分段的内存空间中，一个段对应一个内存"块"，一个段本身是一个逻辑地址空间。

​	程序访问内存地址，需要一个二维的二元组`(s，addr)`，s段号，addr段内偏移

```none
# 下面就打个比方，具体取几位数没有严格按照分段方案来
[010101] [1100101010]
n2     0 n1         0 
   s          addr
段寄存器+地址寄存器实现方案(也就是用两个寄存器存分段寻址的地址，组合后对应真实物理地址)

[1000101010010]
n             0
 | s |  addr  |
单地址实现方案(也就是前半段是s段号，后半部分是段内偏移,组合后经过计算获得对应的真实物理地址)

# 一般都是 s 左移几位，然后加上 addr获取对应的物理地址(指分段方案下，逻辑地址到物理地址的转换方式)
```

![img](https://img-blog.csdn.net/20180407161207970?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0FsYXRlYmxvb21lcg==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)

> [四、地址空间与内存分配](https://blog.csdn.net/Alatebloomer/article/details/79841088)

### 4.1.3 内存分段的硬件实现方案

*<small>(大致找了张图，文字描述不一定和图一样，毕竟没图床，就这样吧。哈哈)）</small>*

![img](https://timgsa.baidu.com/timg?image&quality=80&size=b9999_10000&sec=1596451014660&di=5d5ddf351b24f93e40514786fd8d6a39&imgtype=0&src=http%3A%2F%2Fa4.att.hudong.com%2F72%2F22%2F01200000144761134439220160183_s.jpg)

​	首先，段表需要操作系统建立并维护，然后再与硬件建立联系（比如段寄存器、地址寄存器、MMU等）。当然，段表的建立，操作系统也不是随便整的，必须按照硬件支持的段表寻址方式去实现和维护段表。

​	程序P运行=>CPU执行=>遇到需要从内存寻址的数据(段号s+段内偏移addr)=>段表查询，MMU转换逻辑地址=>CPU获取物理地址后，往总线向内存发起物理地址空间对应的数据获取请求=>内存响应CPU请求，并返回数据到总线=>CPU从总线获取到来自内存的数据，继续执行。

*（分段这种机制，基本被淘汰了，基本采用后面的分页方式，分页本身也是建立在分段思想上的。）*

![img](https://img-blog.csdn.net/2018040716232263?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0FsYXRlYmxvb21lcg==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)

> [四、地址空间与内存分配](https://blog.csdn.net/Alatebloomer/article/details/79841088)

## 4.2 分页

### 4.2.1 分页地址空间

​	前面的分段，需要考虑段号和段内偏移；而分页，同样需要考虑页号和业内偏移。

​	**分页和分段两种机制的最大区别是：段是变长大小的，而页是固定大小的**。

+ 划分物理内存至**固定大小**的帧
  + 大小是2的幂，例如512、4096、8192
+ 划分逻辑地址空间至**相同大小**的页
  + 大小是2的幂，例如512、4096、8192

+ 建立方案转换逻辑地址为物理地址（pages to frames）
  + 页表
  + MMU/TLB

注意，物理地址PA（Physical Address）划分成固定大小的帧frame，而虚拟地址VA（Virtual address）划分成固定大小的page。

**frame和page必须大小相同**，如果frame是512，那么page也必须是512，如果frame=4KB，那么page也必须=4KB。

操作系统维护页表，记录VA到PA的转换。<small>（有VA作键key，PA作值value的；也有逆向页表PA作key，VA作value的。后者相对省空间，但是实现和维护更复杂。）</small>

MMU同理还是做VA和PA之间的转换工作，而TLB是页表的快表。<small>(也就是页表缓存，MMU转换后缓存结果到TLB，这样CPU要是遇到相同的虚拟地址就不用MMU重复到主存查页表然后转换地址。即TLB省去一次访问主存查页表找页号对应的页帧的时间，直接从TLB获取页号对应的页帧，然后就直接向主存申请对应物理地址的数据。这样明显比访问主存查页表+后续访问主存取数据快一点。)</small>

---

帧Frame：

物理内存被分割为大小相同的帧。

一个内存物理地址是一个二元组`(f,o)`，f帧号（F位，共有2<sup>F</sup>个帧），o帧内偏移（S位，每帧有2<sup>S</sup>字节）。
$$
物理地址PA=2^S*f+o
$$

---

页Page：

一个程序的逻辑地址空间被划分为大小相等的页。

+ **页内偏移的大小=帧内偏移的大小**
+ **页号大小<>帧号大小**

一个逻辑地址是一个二元组`(p,o)`，p页号（P位，2<sup>P</sup>个页），o页内偏移（S位，每页有2<sup>S</sup>字节）
$$
虚拟地址VA=2^S*p+o
$$

---

地址计算示例：

16-bit的物理地址PA，9-bit（512byte）大小的页帧，已知物理地址二元组表示为（3，6），那么计算物理地址为`2^9*3+6`，即1542。

![img](https://img-blog.csdn.net/20180407170551483?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0FsYXRlYmxvb21lcg==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)

![img](https://img-blog.csdn.net/20180407170838613?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0FsYXRlYmxvb21lcg==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)

![img](https://img-blog.csdn.net/20180407170648103?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0FsYXRlYmxvb21lcg==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)

> [四、地址空间与内存分配](https://blog.csdn.net/Alatebloomer/article/details/79841088)

### 4.2.2 页寻址方案

![img](https://timgsa.baidu.com/timg?image&quality=80&size=b9999_10000&sec=1596454096480&di=924a1703a50aef6a166ec1379c818003&imgtype=0&src=http%3A%2F%2Finews.gtimg.com%2Fnewsapp_bt%2F0%2F12010650396%2F641.jpg)

![img](https://ss2.bdstatic.com/70cFvnSh_Q1YnxGkpoWK1HF6hhy/it/u=206931261,4704473&fm=15&gp=0.jpg)

​	和前面分段机制差不多。页表存储页号-帧号的对应关系。由于页大小和帧大小相同，所以只要页号p查找到最终对应的帧号f后，通过计算公式，就可以得出具体的物理地址了。（页大小和帧大小相同，所以页偏移o和帧偏移o是一样的数值）

​	同样，页表由操作系统建立，操作系统初始化的时候就得加载好页表管理系统。

在页寻址机制中，注意几个要点：

+ 页映射到帧
+ 页是**连续**的虚拟内存<small>(也就是程序开发视角来看，地址是连续的）</small>
+ 帧是**非连续**的物理内存<small>(也就是说实际连续的虚拟地址对应的物理地址不一定是连续的，这样才能充分利用内存外部碎片，也方便一些减少内部碎片的内存管理机制实现）</small>
+ 不是所有的页都有对应的帧<small>(二种情况，物理内存全用完了，虚拟地址还有剩；或者虚拟地址没用完，物理地址也没用完。一般而言虚拟地址范围>=物理地址范围）</small>

## 4.3 页表

### 4.3.1 页表概述

> [请问在操作系统中PTBR和PTLR分别指什么？](https://iask.sina.com.cn/b/1H4Spm1zSq5n.html)
>
> 页表基寄存器（PTBR）指向页表. 页表长度寄存器（PTLR）指示页表的大小
>
> [操作系统概念学习笔记 16 内存管理(二) 段页](https://blog.csdn.net/sunmc1204953974/article/details/46859785)
>
> [读懂操作系统之虚拟内存基本原理篇（一）](https://www.cnblogs.com/CreateMyself/p/12969171.html)
>
> <small>上面这篇很好，主要图多，哈哈。下面的图片也是来自上述文章。</small>

![img](https://img2020.cnblogs.com/blog/589642/202005/589642-20200528215647997-211397649.png)

![img](https://img2020.cnblogs.com/blog/589642/202005/589642-20200528221912276-1423676717.png)

​	每个运行中的程序都有一个页表，其也算是程序运行状态，会动态变化。PTBR页表基址寄存器起到页表指针的作用，存放页表的索引key。

​	页表项除了存储 页号-帧号对应关系，还会存储一些状态信息。（比如这个页号对应的帧号数据是否被执行过写操作，是否被访问过等）操作系统需要根据页表中各个页表项的状态参数信息来实时维护、修改页表信息。（这个与后续介绍的页置换算法有关，也就是每个进程能分配到的实际内存物理空间有限，需要考虑把一些暂时不用的页帧置换出来，减少缺页中断次数，提高CPU运行效率）

```none
#页表项的内容(伪代码，只描述部分)
+ Flags(标志位)
  + dirty bit
  + resident bit
  + clock/reference bit
+ 帧号f

#示例，下面假装页表某一项
[010 | f]

dirty bit = 0 说明数据没有被执行过写操作(因为计算机很笨，一般你执行写操作，不管数据变不变，它直接当你变过了。)
resident bit = 1 说明该页号对应的页帧存在(也就是内存有对应的物理帧与该页表项的页号对应,0则说明后面帧号f无效)
clock/reference bit = 0 说明该页没被访问过(这个与后面的页置换算法有关。没有访问过的页表项可能被置换出来)
f 页帧号，就是对应物理地址的帧号
```

![img](https://img-blog.csdn.net/20180407171021168?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0FsYXRlYmxvb21lcg==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)

![img](https://img-blog.csdn.net/20180410001844981?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0FsYXRlYmxvb21lcg==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)

> [四、地址空间与内存分配](https://blog.csdn.net/Alatebloomer/article/details/79841088)

### 4.3.2 转换后背缓存区(TLB)

> [TLB-转译后备缓冲区--百度百科]([https://baike.baidu.com/item/%E8%BD%AC%E8%AF%91%E5%90%8E%E5%A4%87%E7%BC%93%E5%86%B2%E5%8C%BA/22685572?fromtitle=TLB&fromid=2339981&fr=aladdin](https://baike.baidu.com/item/转译后备缓冲区/22685572?fromtitle=TLB&fromid=2339981&fr=aladdin))
>
> TLB 用于缓存一部分标签页表条目。TLB可介于 CPU 和[CPU缓存](https://baike.baidu.com/item/CPU缓存)之间，或在 CPU 缓存和[主存](https://baike.baidu.com/item/主存)之间，这取决于缓存使用的是物理寻址或是虚拟寻址。如果缓存是虚拟定址，定址请求将会直接从 CPU 发送给缓存，然后从缓存访问所需的 TLB 条目。如果缓存使用物理定址，CPU 会先对每一个存储器操作进行 TLB 查寻，并且将获取的物理地址发送给缓存。两种方法各有优缺点。

分页机制存在一个明显的性能问题。即**访问一个内存单元，需要2次内存访问**<small>（没有TLB之前）</small>。

+ 一次用于获取页表项<small>(因为页表本身就是存放在内存中的)</small>
+ 一次用于访问数据<small>(从页表获取到页号具体的页帧后，还需要根据实际物理地址从总线向内存发起数据请求)</small>

再者，页表可能非常大。64位机器理论可以有2<sup>64</sup>Byte的寻址能力，如果只用一级页表，那么光是访问这张表，时间开销就超级大。*<small>（实际上常见的64位操作系统，也没有真的用2<sup>64</sup>Byte大小的虚拟地址，因为没必要，物理内存本身都没有那么大的数量级。）</small>*

为了解决这些问题，就出现了缓存Caching、间接访问等解决方案。而**TLB就是采用了缓存方案，使得页表机制下的内存访问能更快**。

---

![img](https://timgsa.baidu.com/timg?image&quality=80&size=b9999_10000&sec=1596462377005&di=89e4a520e3f96e4b0d9a181145bb40e0&imgtype=0&src=http%3A%2F%2Foenhan.com%2Fwp-content%2Fuploads%2F2013%2F09%2Fpage_table_plus_tlb-1.jpg)

Translation Look-aside Buffer（TLB)，缓存近期访问的页帧转换表项

+ TLB使用associative memoty（关联内存）实现，具备快速访问性能
+ 如果TLB命中，物理页号可以很快被获取
+ 如果TLB未命中，对应的表项被更新到TLB中

前面提到过页表项存在Flags多个状态标识符，其中`resident bit`，判断是否存在 虚拟地址VA 对应的 物理地址PA。

CPU寻址，先看看存储器内的快表TLB是否有页表缓存，没有再取更慢的内存memory中查找页表项。如果连内存的页表也不存在虚拟地址对应的物理地址，那么抛出内存地址访问异常，交由操作系统去处理异常。

根据不同的CPU设计，TLB未命中时，从内存获取页表项并缓存到TLB的这个动作可能由CPU硬件完成，也可能由操作系统OS完成。*<small>(32位x86的CPU能硬件完成该动作，也有别的CPU需要依靠软件，也就是需要操作系统来完成填充TLB表的工作)</small>*

---

![image](http://blog.chinaunix.net/attachment/201112/18/16361381_1324218546w802.jpg)

​	**由于CPU首先接到的是由程序传来的虚拟内存地址，所以CPU必须先到物理内存中取页表，然后对应程序传来的虚拟页面号，在表里找到对应的物理页面 号，最后才能访问实际的物理内存地址，也就是说整个过程中CPU必须访问两次物理内存(实际上访问的次数更多)。因此，为了减少CPU访问物理内存的次 数，引入TLB**。

​	TLB在X86体系的CPU里的实际应用最早是从Intel的486CPU开始的，在X86体系的CPU里边，一般都设有如下4组TLB:

+ 第一组：缓存一般页表（4K字节页面）的指令页表缓存（Instruction-TLB）；

+ 第二组：缓存一般页表（4K字节页面）的数据页表缓存（Data-TLB）；

+ 第三组：缓存大尺寸页表（2M/4M字节页面）的指令页表缓存（Instruction-TLB）；

+ 第四组：缓存大尺寸页表（2M/4M字节页面）的数据页表缓存（Instruction-TLB）；

​	图中可见，当CPU执行机构收到应用程序发来的虚拟地址后，首先到TLB中查找相应的页表数据，如果TLB中正好存放着所需的页表，则称为TLB命中（TLB Hit）,接下来CPU再依次看TLB中页表所对应的物理内存地址中的数据是不是已经在一级、二级缓存里了，若没有则到内存中取相应地址所存放的数据。如果TLB中没有所需的页表，则称为TLB失败（TLB Miss），接下来就必须访问物理内存中存放的页表，同时更新TLB的页表数据。

​	既然说TLB是内存里存放的页表的缓存，那么它里边存放的数据实际上和内存页表区的数据是一致的，在内存的页表区里，每一条记录虚拟页面和物理页框对应关系的记录称之为一个页表条目（Entry），同样地，在TLB里边也缓存了同样大小的页表条目（Entry）。由于页表条目的大小总是固定不变的，所以TLB的容量越大，则它所能存放的页表条目数越多（类似于增大CPU一级、二级缓存容量的作用），这就意味着缓存命中率的增加，这样，就能大大减少CPU直接访问内存的次数，实现了性能提升。

> [TLB是否在多个内核之间共享？(Is the TLB shared between multiple cores?)](https://www.it1352.com/1845896.html)
>
> [多核CPU是有几个TLB？一个核有一个独有的TLB还是多核共享一个TLB？](https://www.zhihu.com/question/313913862/answer/635433592)
>
> [什么是TLB?](https://www.cnblogs.com/linhaostudy/p/10347288.html)
>
> 简言之，每个CPU内核都是有独立的TLB和MMU的。CPU一般用到的缓存就1~3级，L1和L2有TLB，L3是多个内核共享的缓存。
> L1里面还具体分 L1i和L1d，也就是 指令 和 数据的缓存。
>
> **TLB分为指令页表缓存（Instruction-TLB）和数据页表缓存（Data-TLB）**
>
> 一般主流Cache都是物理Cache，即要求访问Cache之前要先访问TLB进行VA到PA的转换。

---

![img](https://img-blog.csdn.net/2018041000331384?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0FsYXRlYmxvb21lcg==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)

> [四、地址空间与内存分配](https://blog.csdn.net/Alatebloomer/article/details/79841088)

### 4.3.3 二级/多级页表

![img](https://timgsa.baidu.com/timg?image&quality=80&size=b9999_10000&sec=1596465825786&di=432cb3dfcda3fa3d0cda4a79be9aa42b&imgtype=0&src=http%3A%2F%2Finews.gtimg.com%2Fnewsapp_bt%2F0%2F12010650398%2F641.jpg)

拆分二级页表，那么一级页表存储的值value是二级页表的初始地址；然后二级页表才是具体的页号-页帧（虚拟地址-物理地址）。

这样虽然开销依然很大，但是有个好处，就是一级页表的页表项如果`resident bit`值为0，那么说明不存在该一级页表项-二级页表项的映射关系，那么就可以少创建一个二级页表。*（即理论上一级页表某项表明没有对应的二级页表项时，其二级页表就可以不用创建了。这样就可以省下部分创建页表的空间)*

多级页表同理，就是把原本的页表拆分成更多层次/级别。

```none
#下面都是打比方，位数什么的随便取的
#一级页表
[8位p|8位o] 8位标识页号,8位标识页内偏移

#二级页表
[4位p1|4位p2|8位o] 4位标识一级页表(一级页号-二级页号),4位标识二级页表(二级页号-页帧号)

#三级页表
[2位p1|3位p3|3位p3|8位o]
...
```

---

![img](https://img-blog.csdn.net/20180410004657551?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0FsYXRlYmxvb21lcg==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)

![img](https://img-blog.csdn.net/20180410004741295?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0FsYXRlYmxvb21lcg==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)

> [四、地址空间与内存分配](https://blog.csdn.net/Alatebloomer/article/details/79841088)

### 4.3.4 反向页表

> [反向页表--百度百科]([https://baike.baidu.com/item/%E5%8F%8D%E5%90%91%E9%A1%B5%E8%A1%A8/3565803?fr=aladdin](https://baike.baidu.com/item/反向页表/3565803?fr=aladdin))
>
> 反向页表（ inverted page table ）一般被视为使用正常的系统内存的TLB的片外扩展。与真正的页表不同，它不需要容纳目前所有的映射。
>
> 反向页表对于每个真正的内存页或帧才有一个条目。每个条目包含保存在真正内存位置的页的虚拟地址以及拥有该页的进程的信息。因此，整个系统只有一个页表，对每个物理内存的页只有一条相应的条目。因为系统只有一个页表，而有多个地址空间映射物理内存，所以反向页表的条目中通常需要一个地址空间标识符，以确保一个特定进程的一个逻辑页可以映射到相应的物理帧。
>
> [Linux 匿名页的反向映射](https://www.cnblogs.com/linhaostudy/p/10350326.html)

​	由于虚拟地址范围通常比物理地址范围大，比如64位操作系统，理论虚拟地址范围总大小可以达到2<sup>64</sup>Byte，就算4KB作为页大小，也大概需要5~6级页表才能表示所有的虚拟地址。

​	我们反过来项，不让页表与逻辑地址空间大大小相对应，而是让页表与物理地址空间大小项对应，那么页表就可以缩小好几个数量级。

​	即页表原本(key-页号，value-帧号)变成（key-帧号，value-页号），这就是反向页表了。

*(操作系统要是有使用反向页表机制，那大多也是采用传统页表为主，反向页表为辅的方式。)*

---

基于页寄存器（Page Register）的方案

*<small>（找不到图，就大致说说吧。自己OneNote是有图，但是毕竟不是图床，额。）</small>*

​	进程运行时，CPU像往常一样，需要翻译逻辑地址，才能再去请求获取内存中对应物理地址的数据。这时在页寄存器（Page Register）维护一张特殊的页表（key帧号，value页号），先从页寄存器搜寻看看能不能通过value虚拟地址页号找到对应的key帧号。由于使用帧号做索引，页表的大小就与虚拟地址范围无关，而与物理地址范围有关（更小，同时也没必要硬性要求页寄存器存有所有帧号-页号对应关系，起到缓存作用就好了）

​	使用页寄存器时，每个帧和一个寄存器关联，寄存器内容包括：

+ Residence bit：此帧是否被占用
+ Occupier：对应的页号p
+ Protection bits：保护位

​	举例：

+ 物理内存大小4096\*4096=4K\*4KB=16MB

+ 页面大小：4096bytes=4KB

+ 页帧数：4096=4K

+ 页寄存器使用的空间（假设 8byte/register）：

  8*4096=32KB

+ 页寄存器带来的额外开销：

  32K/16M=0.2%（大约）

+ 虚拟内存的大小：任意

明显，如果采用 帧-页对应关系，需要的页寄存器大小比传统页表小很多，但是需要考虑怎么通过这种数据结构快速地由value页号查找到对应的key帧号。

优点：

+ 转换表的大小相对物理内存来说很小
+ 转换表的大小跟逻辑地址空间的大小无关

缺点：

+ 需要的信息对调了，即根据帧号可找到页号（需要考虑如何加快索引，如何转换结果等，实现更加复杂）

---

基于关联内存（associative memory）的方案

与基于页寄存器的方案类似，不过是把存放反向页表的容器从页寄存器变成了关联内存。这个特殊的存储器也是在CPU内部的，同样不能做得太大，太大成本高+查询速度下降。

在反向页表中搜索一个页对应的帧号

+ 如果帧数较少，页寄存器可以被放置在关联内存中

+ 在关联内存中查找逻辑页号

  + 成功：帧号被提取
  + 失败：页错误异常（page fault）

+ 限制因素：

  大量的关联内存非常昂贵、难以在单个时钟周期内完成、耗电......

---

基于哈希（Hash）查找的方案

根据进程号PID和CPU获取到的虚拟地址页号p，经过特殊的函数`h(PID,p)`计算，得到key，在反向页表中再根据key查找对应的帧号f。

在反向页表中通过哈希算法来搜索一个页对应的帧号

+ 对页号做哈希计算，为了在"帧表"（每帧拥有一个表项）中获取对应的帧号。
+ 页i被放置在表中f(i)位置，其中f是设定的哈希函数
+ 为了查找页i，执行下列操作：
  + 计算哈希函数f(i)，并且使用它作为页寄存器表的索引
  + 获取对应的页寄存器
  + 检查寄存器标签是否包含i，如果包含，则代表成功
  + 否则失败

*<small>（Hash计算的反向表，也是根据物理地址范围大小决定表大小，与虚拟地址无关，这样表占用空间更小。但是这个表同样需要放在内存中，所以还需要配合TLB使用。再者，哈希函数的设计需要硬件集成，和软件系操作系统配合，而且需要考虑Hash碰撞的处理。现在有些高端CPU就有采用基于Hash查找的反向表，能提高CPU寻址的效率。）</small>*

![img](https://img-blog.csdn.net/20180410014023104?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0FsYXRlYmxvb21lcg==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)

问题：

- 会有碰撞，多个逻辑地址对应一个值，加入参数PID==ID of running program来缓解冲突；
- 哈希表在内存中，要访问内存，内存的时间开销还是很大

优点：**本身物理存址小省空间，不再是每个应用程序都要page table了，整个系统只用一个**。 

缺点：需求高，有高效哈希函数和解决冲突的机制，要硬件软件配合，仍然需要TLB机制。

> [反向页表（基于hash表）](https://blog.csdn.net/qq_41841130/article/details/102995118)
>
> [四、地址空间与内存分配](https://blog.csdn.net/Alatebloomer/article/details/79841088)

# 5. 覆盖技术、交换技术、虚拟技术

## 5.0 引言

### 5.0.1 存储器层次结构

> [存储层次--百度百科]([https://baike.baidu.com/item/%E5%AD%98%E5%82%A8%E5%B1%82%E6%AC%A1/3923789?fr=aladdin](https://baike.baidu.com/item/存储层次/3923789?fr=aladdin))
>
> 存储层次是在[计算机体系结构](https://baike.baidu.com/item/计算机体系结构/10547223)下[存储系统](https://baike.baidu.com/item/存储系统/944115)层次结构的排列顺序。每一层于下一层相比都拥有较高的速度和较低延迟性，以及较小的容量。大部分现今的中央处理器的速度都非常的快。大部分程序工作量需要存储器访问。由于高速缓存的效率和存储器传输位于层次结构中的不同档次，所以实际上会限制处理的速度，导致中央处理器花费大量的时间等待存储器I/O完成工作。

理想中的存储器：更大、更快、更便宜的非易失性存储器。

实际中的存储器：

![img](https://timgsa.baidu.com/timg?image&quality=80&size=b9999_10000&sec=1596513928144&di=a357db7d614359b30487ed1a3a90a7eb&imgtype=0&src=http%3A%2F%2Fpic3.zhimg.com%2F50%2Fv2-f32dcdf4aab4499994157642a25fbd30_hd.jpg)

![img](https://ss3.bdstatic.com/70cFv8Sh_Q1YnxGkpoWK1HF6hhy/it/u=4207278865,2453922396&fm=15&gp=0.jpg)

![img](https://img-blog.csdn.net/20180410095034672?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0FsYXRlYmxvb21lcg==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)

> [五、虚拟内存](https://blog.csdn.net/Alatebloomer/article/details/79876498)

### 5.0.2 多道程序运行引起内存不足

​	早期计算机硬件发展不如现在，硬件贵又容量小。在个人电脑内存只有256MB的年代，计算机往往跑不了几个程序就内存不足了。为了解决多道程序运行引起的内存不足，有以下几种解决方案。

+ 如果是**程序太大**，超过了内存的容量，可以采用**手动的覆盖（overlay）技术**，只把需要的指令和数据保存在内存当中。
+ 如果是**程序太多**，超过了内存的容量，可以采用**自动的交换（swapping）技术**，把暂时不能执行的程序送到外存中。
+ 如果想要在有限容量的内存中，以<u>更小的页粒度</u>为单位装入更多更大的程序，可以采用**自动的虚拟存储技术**。

## 5.1 覆盖技术

### 5.1.1 覆盖技术的目标

​	目标即在较小的可用内存中运行较大的程序。常用于多道程序系统，与分许存储管理配合使用。

*（上世纪80-90年代的常见技术手段。当时代表性的操作系统是DOS操作系统，硬件内存只有640KB左右）*

### 5.1.2 覆盖技术的原理

​	原理，把程序按照其自身逻辑结构，**划分为若干个功能上相对独立的程序模块**，那些不会同时执行的模块共享同一块内存区域，按时间先后来运行。

+ 必要部分（常用功能）的代码和数据常驻内存
+ 可选部分（不常用功能）在其他程序模块中实现，平时存放在外存中，在需要用到时才装入内存
+ <u>不存在调用关系的模块不必同时装入到内存，从而可以相互覆盖，即这些模块共用一个分区</u>

---

思想:不相互调用的程序段可以共享同一内存区

例子：

设某进程的程序正文段由A，B，C，D，E和F等6个程序段组成.
由于程序段B不调用C，程序段C也不调用B，因此程序段B和C无需同时驻留在内存，它们可以共享同一内存区。
同理，程序段D、E、F也可共享同一内存区。

内存可以分为两个部分:

- 常驻内存区:常驻内存部分，与所有被调用程序段有关，不能被覆盖。这一部分称为根程序。图 (b)中，根程序是程序段A
- 覆盖区（这里B和C共用覆盖区0，D、E、F共用覆盖区1，按照不同时间顺序进入覆盖区即可）
  ![img](https://img2018.cnblogs.com/blog/1734701/201911/1734701-20191122163549323-1363685626.png)

覆盖区0由程序段B、C共享，容量50K。
覆盖区1为程序段F、D、E共享，容量40K。
进程正文段要求内存空间：
A(20K)+B(50K)+F(30K)+C(30K)+D(20K)+E(40K)=190K
采用覆盖技术，只需（常驻部分）20K+（覆盖区0）50K+（覆盖区1）40K=110K的内存空间即可开始执行。

另一种覆盖方案（100K）：A占一个分区20K；B、D和E共用一个分区50K；C和F共用一个分区30K。

> [存储管理-覆盖技术和交换技术](https://www.cnblogs.com/mengxiaoleng/p/11912571.html)  <small>上述样例部分来自该文章，为啥？因为没图床，哈哈。</small>

### 5.1.3 覆盖技术的优缺点

优点：在程序能够拆分成多个模块代码的前提下，能够减少进程占用的内存空间。

缺点：

+ 由程序员来把一个大的程序划分成若干个小的功能模块，并确定各个模块之间的覆盖关系，费时费力，增加了编程的复杂度。
+ 覆盖模块从外存装入内存，实际上是以时间延长来换取空间节省。

*（Turbo Pascal的Overlay系统单元支持程序员控制的覆盖技术）*

## 5.2 交换技术

### 5.2.1 交换技术的目标

目标，多道程序在内存中时，让正在运行的程序或需要运行的程序获得更多的内存资源。

### 5.2.2 交换技术的方法

方法：

+ 将暂时不能运行的程序送到外存，从而获得空闲内存空间。
+ 操作系统把一个**进程的整个地址空间**的内存保存到外存中（换出swap out），而将外存中的某个进程的地址空间读入到内存中（换入swap in）。换入换出内容的大小为整个程序的地址空间。

*（把暂时不运行的程序导出到硬盘，需要运行的程序从硬盘导入到内存。一般涉及的空间对应页表至少成百上千页。早期页大小通常定为4KB。）*

交换技术实现中的几个问题：

+ 交换时机的确定：何时需要发生交换？一般只当内存空间不够或有不够的危险时才换出。<small>*(毕竟硬盘和内存存取速度相差好几个量级，要是频繁交换，很影响操作系统整体的运行效率。)*</small>
+ 交换区的大小：必须足够大以存放所有用户进程的所有内存映像的拷贝；必须能对这些内存映像进行直接存取。*<small>（一来内存要本身至少够存放整个进程本身，二来换入到硬盘时要保证硬盘足够大。）</small>*
+ 程序换入时的重定位：换出后再换入的内存位置一定要在原本的位置上吗？最好采用<u>地址动态映射</u>的方法。*<small>（因为程序换出再换入，如果对应进程的页表不修改，要是进程这次换入的位置不同，将导致进程出错。所以需要地址动态映射，保证即时下次换入位置不同，也能正常进行虚拟地址-物理地址的转换。）</small>*

![img](https://img-blog.csdn.net/20180410102326240?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0FsYXRlYmxvb21lcg==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)

> [动态地址映射--百度百科]([https://baike.baidu.com/item/%E5%8A%A8%E6%80%81%E5%9C%B0%E5%9D%80%E6%98%A0%E5%B0%84/6239909?fr=aladdin](https://baike.baidu.com/item/动态地址映射/6239909?fr=aladdin))
>
> 动态地址映射：在程序运行期间，随着每条指令和数据的访问自动地，连续的进行映射。
>
> [五、虚拟内存](https://blog.csdn.net/Alatebloomer/article/details/79876498)

### 5.2.3 交换技术与覆盖技术比较

+ 覆盖只能发生在那些<u>互相之间没有调用关系</u>的程序模块之间，因此程序员必须给出程序内的各个模块之间的逻辑覆盖结构。
+ 交换技术是以在内存中的程序大小为单位来进行的，它不需要程序员给出各个模块之间的逻辑覆盖结构。换言之，**交换发生在内存中程序与管理程序或操作系统之间，而覆盖则发生在运行程序内部**。

*（在内存连完整的单个程序都没有办法一次性加载时，就只能考虑用覆盖技术了，毕竟交换技术是一次加载整个程序的。）*

*<small>（交换，对于程序员来说工作量更轻，因为不用考虑内存到底是怎么分配的，交由管理程序或者操作系统去安排。）</small>*

*<small>（覆盖，需要程序员事先设计好内存覆盖的顺序和位置，虽然效率上比起交换可能好一些，因为只对个别部分进行覆盖，而交换直接整个程序空间进行交换，但是实现更加复杂。）</small>*

## 5.3 虚拟技术

### 5.3.1 虚拟技术的目标

> [虚拟内存--百度百科]([https://baike.baidu.com/item/%E8%99%9A%E6%8B%9F%E5%86%85%E5%AD%98/101812?fr=aladdin](https://baike.baidu.com/item/虚拟内存/101812?fr=aladdin))
>
> 虚拟[内存](https://baike.baidu.com/item/内存)是[计算机系统](https://baike.baidu.com/item/计算机系统/7210959)[内存管理](https://baike.baidu.com/item/内存管理/5633616)的一种技术。它使得[应用程序](https://baike.baidu.com/item/应用程序/5985445)认为它拥有连续的可用的[内存](https://baike.baidu.com/item/内存/103614)（一个连续完整的[地址空间](https://baike.baidu.com/item/地址空间/1423980)），而实际上，它通常是被分隔成多个[物理内存](https://baike.baidu.com/item/物理内存/2502263)碎片，还有部分暂时存储在外部[磁盘存储器](https://baike.baidu.com/item/磁盘存储器/2386684)上，在需要时进行[数据交换](https://baike.baidu.com/item/数据交换/1586256)。目前，大多数[操作系统](https://baike.baidu.com/item/操作系统/192)都使用了虚拟内存，如Windows家族的“虚拟内存”；Linux的“交换空间”等。
>
> [内存管理--虚拟内存管理技术](https://www.cnblogs.com/Leo_wl/p/12781450.html)

+ 在内存不够用的情形下，可以采用覆盖技术和交换技术，但是：
  + 覆盖技术：需要程序员自己把整个程序划分成若干个小的功能模块，并确定各个模块之间的覆盖关系，增加了程序员的负担；
  + 交换技术：以进程为交换的单位，需要把进程的整个地址空间都换进换出，增加了处理器的开销。
+ "四海之内的"解决之道：虚拟内存管理技术——虚拟技术。

目标：

+ 像覆盖技术那样，不是把程序的所有内容都放在内存中，因而能够运行比当前的空闲内存空间还要大的程序。但做得更好，由操作系统自动来完成，无须程序员的干涉。
+ 像交换技术那样，能够实现进程在内存与外存之间的交换，因而获得更多的空闲内存空间。但做得更好，只对进程的部分内容在内存和外存之间进行交换。

Physical Memory + Disk = Virtual Memory

![img](https://img-blog.csdn.net/2018041010551261?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0FsYXRlYmxvb21lcg==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)

虚拟存储器是由硬件和操作系统自动实现存储信息调度和管理的。它的工作过程包括6个步骤： 

①[中央处理器](https://baike.baidu.com/item/中央处理器)访问主存的逻辑地址分解成组号a和组内地址b，并对组号a进行地址变换，即将逻辑组号a作为索引，查地址变换表，以确定该组信息是否存放在主存内。 

②如该组号已在[主存](https://baike.baidu.com/item/主存)内，则转而执行④；如果该组号不在主存内，则检查主存中是否有空闲区，如果没有，便将某个暂时不用的组调出送往辅存，以便将这组信息调入主存。 

③从辅存读出所要的组，并送到主存空闲区，然后将那个空闲的物理组号a和逻辑组号a登录在地址变换表中。 

④从地址变换表读出与逻辑组号a对应的物理组号a。 

⑤从物理组号a和组内字节地址b得到物理地址。 

⑥根据物理地址从主存中存取必要的信息。

> [五、虚拟内存](https://blog.csdn.net/Alatebloomer/article/details/79876498)

### 5.3.2 程序的局部性原理

> [程序的局部性原理--百度百科]([https://baike.baidu.com/item/%E7%A8%8B%E5%BA%8F%E7%9A%84%E5%B1%80%E9%83%A8%E6%80%A7%E5%8E%9F%E7%90%86/8412331?fr=aladdin](https://baike.baidu.com/item/程序的局部性原理/8412331?fr=aladdin))
>
> 程序的局部性原理是指程序在执行时呈现出局部性规律，即在一段时间内，整个程序的执行仅限于程序中的某一部分。相应地，执行所访问的存储空间也局限于某个内存区域。局部性原理又表现为：时间局部性和空间局部性。时间局部性是指如果程序中的某条指令一旦执行，则不久之后该指令可能再次被执行；如果某数据被访问，则不久之后该数据可能再次被访问。空间局部性是指一旦程序访问了某个存储单元，则不久之后，其附近的存储单元也将被访问。

​	程序的局部性原理（principle of locality）：指程序在执行过程中的一个较短时期，所执行的指令地址和指令的操作数地址，分别局限于一定区域。这可以表现为：

+ 时间局部性：一条指令的一次执行和下次执行，一个数据的一次访问和下次访问都集中在一个较短时期内。
+ 空间局部性：当前指令和邻近的几条指令，当前访问的数据和邻近的几个数据都集中在一个较小区域内。

程序的局部性原理表明，从理论上来看，虚拟存储技术是能够实现的，而且在实现了以后，应该是能够取得一个满意的效果的。

---

程序的编写方法对缺页率的影响：

例子：也面大小为4KB，分配给每个进程的物理页面数为1。在一个进程中，定义了如下的二维数组`int A[1024][1024]`，该数组按行存放在内存，每一行放在一个页面中。

```c
// 程序编写方法1
for(j=0;j<1024;j++)
    for(i=0;i<1024;i++)
        A[i][j]=0;

// 程序编写方法2
for(i=0;i<1024;i++)
    for(j=0;j<1024;j++)
        A[i][j]=0;
```

由于前提假设数组按行存放在内存中，每一行放在一个页面中。（32位操作系统，int占4字节，1024个int正好占4096B，即4KB）

编写方法1，每次跨行访问，也正好是跨页访问，总共触发1024\*1024次中断。

编写方法2，只有外循环跨行（跨页），总共触发1024次中断。

（每次换行正好换页，为一次中断，内循环跨行1024次，外循环1024次，总1024\*1024次缺页中断，因为这里假设程序只占用一个物理页，每次跨页等于触发一次缺页中断）

### 5.3.3 虚拟技术的基本概念

> [虚拟内存--百度百科]([https://baike.baidu.com/item/%E8%99%9A%E6%8B%9F%E5%86%85%E5%AD%98/101812?fr=aladdin](https://baike.baidu.com/item/虚拟内存/101812?fr=aladdin))

​	调度方式有分页式、段式、段页式3种。页式调度是将逻辑和物理地址空间都分成固定大小的页。主存按页顺序编号，而每个独立编址的程序空间有自己的页号顺序，通过调度辅存中程序的各页可以离散装入主存中不同的页面位置，并可据表一一对应检索。**页式调度的优点是页内零头小，页表对程序员来说是透明的，地址变换快，调入操作简单；缺点是各页不是程序的独立模块，不便于实现程序和数据的保护。段式调度是按程序的逻辑结构划分地址空间，段的长度是随意的，并且允许伸长，它的优点是消除了内存零头，易于实现存储保护，便于程序动态装配；缺点是调入操作复杂。**将这两种方法结合起来便构成段页式调度。**在段页式调度中把物理空间分成页，程序按模块分段，每个段再分成与物理空间页同样小的页面。**段页式调度综合了段式和页式的优点。其缺点是增加了硬件成本，软件也较复杂。<u>大型通用计算机系统多数采用段页式调度。</u> 

**页式调度**

在页式虚拟存储系统中，虚拟空间被分成大小相等的页，称为逻辑页或虚页。主存空间也被分成同样大小的页，称为物理页或实页。相应地，虚拟地址分为两个字段:高位字段为虚页号，低位字段为页内地址。实存地址也分为两个字段：高位字段为实页号，低位字段为页内地址。同时，页的大小都取2的整数幂个字。  

通过页表可以把虚拟地址转换成物理地址。每个程序设置一张页表，在页表中，对应每一个虚页号都有一个条目，条目内容至少包含该虚页所在的主存页面地址(实页号)，用它作为实存地址的高位字段;实页号与虚拟地址的页内地址相拼接，就产生完整的实存地址，据此访问主存。  

**段式调度**

页面是主存[物理空间](https://baike.baidu.com/item/物理空间/4804325)中划分出来的等长的固定区域。分页方式的优点是页长固定，因而便于构造[页表](https://baike.baidu.com/item/页表/679625)、易于管理，且不存在外碎片。但分页方式的缺点是页长与程序的逻辑大小不相关。例如，某个时刻一个子程序可能有一部分在主存中，另一部分则在辅存中。这不利于编程时的独立性，并给换入/换出处理、存储保护和存储共享等操作造成麻烦。 

另一种划分可寻址的存储空间的方法称为分段。段是按照程序的自然分界划分的、长度可以动态改变的区域。通常，程序员把[子程序](https://baike.baidu.com/item/子程序/3941697)、操作数和常数等不同类型的数据划分到不同的段中，并且每个程序可以有多个相同类型的段。 

在段式虚拟存储系统中，虚拟地址由段号和段内地址组成，虚拟地址到实存地址的变换通过段表来实现。每个程序设置一个段表，段表的每一个表项对应一个段，每个表项至少包括三个字段：有效位(指明该段是否已经调入主存)、段起址(该段在实存中的首地址)和[段长](https://baike.baidu.com/item/段长/5742657)(记录该段的实际长度)。 

**段页式调度**

段页式虚拟存储器是段式虚拟存储器和页式虚拟存储器的结合。 

首先，实存被等分成页。在段页式虚拟存储器中，把程序按逻辑结构分段以后，再把每段按照实存的页的大小分页，程序按页进行调入和调出操作，但它又可按段实现共享和保护。因此，它可以兼有页式和段式系统的优点。它的缺点是在地址映像过程中需要多次查表，虚拟地址转换成物理地址是通过一个段表和一组页表来进行定位的。段表中的每个表目对应一个段，每个表目有一个指向该段的页表的起始地址(页号)及该段的控制保护信页表指明该段各页在主存中的位置以及是否已装入、已修改等标志。 

---

虚拟技术，可以在页式、段式或者段页式也存管理的基础上实现

+ 在装入程序时，不必将其全部装入到内存，而只需将当前需要执行的部分页面或段装入到内存，就可让程序开始执行；
+ 在程序执行过程中，如果需执行的指令或访问的数据尚未在内存（称为缺页或却段），则由处理器通知操作系统将相应的页面调入到内存，然后继续执行程序。（也就是缺页中断，用户态转内核态）
+ 另一方面，操作系统将内存中暂时不使用的页面或段调出保存在外存（硬盘）上，从而腾出更多空闲空间存放将要装入的程序以及将要调入的页面或段。

### 5.3.4 虚拟技术的基本特征

+ 大的用户空间：通过把物理内存与外存相结合，提供给用户的虚拟内存空间通常大于实际的物理内存，即实现了两者的分离。如32位的虚拟地址理论上可以访问4GB，而可能计算机上仅有256MB的物理内存，但硬盘容量大于4GB。
+ 部分交换：与交换技术相比较，<u>虚拟存储的调入和调出是对部分虚拟地址空间进行的</u>。
+ 不连续性：物理内存分配的不连续，虚拟地址空间使用的不连续。

*<small>（进程中某些虚拟地址连续的部分由于不需要被执行，被暂时移出内存。这可能导致异常-缺页中断，倒是操作系统会自动处理异常，当被移出的页面又需要被使用时，会再重新加载到内存中。）</small>*

### 5.3.5 虚拟页式内存管理

> [内存管理--虚拟内存管理技术](https://www.cnblogs.com/Leo_wl/p/12781450.html)
>
> [虚拟内存设置多少比较合适？](https://www.zhihu.com/question/295194595?sort=created)
>
> 在物理内存+虚拟内存总大小不足50GB的情况下，为什么一次性申请50G不行，分批就可以呢？因为一次性申请50G，系统直接就能判定没有这么多。但是如果你分开使用，虽然我们每次都申请5G，但系统并没有真的给我们5G，只有在真的需要写入的时候，才会真的让这5G对应物理地址。<small>（也就是只申请但是没有进行写操作，就不会实际分配物理地址，挺多就是虚拟地址标记50GB空间--我估计也不会真的对应50GB空间的页表每页都标记，操作系统应该能更智能地标记这种只申请却不用的空间分配。）</small>
>
> 在物理内存和虚拟内存都将超额的情况下，继续申请并写入数据，不同的系统会有不同的应对方式。常见的应对方式即操作系统OS触发OOM killer，杀死系统认为大量浪费占用内存的进程，以维持系统的正常运行。

![内核空间用户空间全图](https://user-gold-cdn.xitu.io/2020/4/20/17195b7a9b3cf2c2?w=1019&h=414&f=png&s=56870)

+ 大部分虚拟存储系统都采用虚拟页式存储管理技术，即在页式存储管理的基础上，增加**请求调页**和**页面置换**功能。
+ 基本思路：
  + 当一个用户程序要调入内存运行时，不是将程序的所有页面都装入内存，而是只装入部分的页面，就可以启动程序运行。
  + 在运行的过程中，如果发现要运行的程序或要访问数据不在内存，则向系统发出**缺页中断**请求，系统在处理这个中断时，将外存中相应的页面调入内存，使得该程序能够继续运行。

页表表项：

```none
[逻辑页号i][访问位][修改位][保护位][驻留位][物理页帧号]
```

+ 驻留位：表示该页是在内存还是在外存。如果该位等于1，表示该页位于内存当中，即该页表项是有效的，可以使用；如果该位等于0，表示该页当前还在外存当中，如果访问该页表项，将导致缺页中断；
+ 保护位：表示允许对该页做何种类型的访问，如只读、可读写、可执行等；
+ 修改位：表示此页在内存中是否被修改过。当系统回收该物理页面时，根据此位来决定是否把它的内存写回内存；
+ 访问位：如果该页面被访问过（包括读操作或写操作），则设置此位。用于页面置换算法。

<small>*（保护位，假如设置成不可执行，但是程序要求在该页进行执行操作，就会抛异常）*</small>

*<small>（修改位，如果这个页在内存中被修改过就是1，这样下次如果要置换该页的内存，就需要把这个页数据覆写到外存。如果0，那么直接释放，不需要把也数据覆写到外存。很明显如果修改位1的页被置换出来，费时又费力。）</small>*

*<small>（访问位，虚拟内存页式管理，在物理内存不够用时，尽量挑选没被访问过的页置换出去，猜测之前没用到的数据之后也不需要。）</small>*

![img](https://img-blog.csdn.net/20180410124345656?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0FsYXRlYmxvb21lcg==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)

![img](https://img-blog.csdn.net/20180410123314860?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0FsYXRlYmxvb21lcg==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)

> [五、虚拟内存](https://blog.csdn.net/Alatebloomer/article/details/79876498)
>
> [内存空间和用户空间](https://blog.csdn.net/baidu_38310096/article/details/78225020)

### 5.3.6 缺页中断

> [缺页中断]([https://baike.baidu.com/item/%E7%BC%BA%E9%A1%B5%E4%B8%AD%E6%96%AD/5029040?fr=aladdin](https://baike.baidu.com/item/缺页中断/5029040?fr=aladdin))
>
> 缺页中断就是要访问的页不在主存，需要操作系统将其调入主存后再进行访问。在这个时候，被内存[映射](https://baike.baidu.com/item/映射)的文件实际上成了一个分页交换文件。
>
> **页缺失**（英语：Page fault，又名**硬错误**、**硬中断**、**分页错误**、**寻页缺失**、**缺页中断**、**页故障**等）指的是当软件试图访问已映射在[虚拟](https://baike.baidu.com/item/虚拟)[地址空间](https://baike.baidu.com/item/地址空间)中，但是并未被加载在[物理内存](https://baike.baidu.com/item/物理内存)中的一个[分页](https://baike.baidu.com/item/分页)时，由[中央处理器](https://baike.baidu.com/item/中央处理器)的内存管理单元所发出的[中断](https://baike.baidu.com/item/中断)。
>
> 通常情况下，用于处理此中断的程序是[操作系统](https://baike.baidu.com/item/操作系统)的一部分。如果操作系统判断此次访问是有效的，那么操作系统会尝试将相关的分页从硬盘上的[虚拟内存](https://baike.baidu.com/item/虚拟内存)文件中调入内存。而如果访问是不被允许的，那么操作系统通常会结束相关的[进程](https://baike.baidu.com/item/进程)。

缺页中断分类：

1. 软性

   ​	**软性页缺失**指页缺失发生时，相关的页已经被加载进内存，但是没有向MMU注册的情况。操作系统只需要在MMU中注册相关页对应的物理地址即可。

   ​	发生这种情况的可能性之一，是一块物理内存被两个或多个程序[共享](https://baike.baidu.com/item/共享)，操作系统已经为其中的一个装载并注册了相应的页，但是没有为另一个程序注册。

   ​	可能性之二，是该页已被从CPU的[工作集](https://baike.baidu.com/item/工作集)中移除，但是尚未被交换到[磁盘](https://baike.baidu.com/item/磁盘)上。比如[OpenVMS](https://baike.baidu.com/item/OpenVMS)这样的使用次级页缓存的系统，就有可能会在工作集过大的情况下，将某页从工作集中去除，但是不写入硬盘也不擦除（比如说这一页被读出硬盘后没被修改过），只是放入空闲页表。除非有其他程序需要，导致这一页被分配出去了，不然这一页的内容不会被修改。当原程序再次需要该页内的数据时，如果这一页确实没有被分配出去，那么系统只需要重新为该页在MMU内注册映射即可。

2. 硬性

   ​	与软性页缺失相反，**硬性页缺失**是指相关的页在页缺失发生时未被加载进内存的情况。这时操作系统需要：

   1. 寻找到一个空闲的页。或者把另外一个使用中的页写到磁盘上（如果其在最后一次写入后发生了变化的话），并注销在MMU内的记录
   2. 将数据读入被选定的页
   3. 向MMU注册该页

   ​	硬性页缺失导致的性能损失是很大的。以一块7200[rpm](https://baike.baidu.com/item/rpm)的主流[机械硬盘](https://baike.baidu.com/item/机械硬盘)为例，其平均寻道时间为8.5毫秒，读入内存需要0.05毫秒。相对的，[DDR3内存](https://baike.baidu.com/item/DDR3内存)的访问延迟通常在数十到100纳秒之间，性能差距可能会达到8万到22万倍。

   ​	另外，有些操作系统会将程序的一部分延迟到需要使用的时候再加载入内存执行，以此来提升性能。这一特性也是通过捕获硬性页缺失达到的。

   当硬性页缺失过于频繁的发生时，称发生系统颠簸。

3. 无效

   ​	当程序访问的虚拟地址是不存在于虚拟地址空间内的时候，则发生**无效页缺失**。一般来说这是个软件问题，但是也不排除硬件可能，比如因为内存故障而损坏了一个正确的[指针](https://baike.baidu.com/item/指针)。

   ​	具体动作与所使用的操作系统有关，比如Windows会使用[异常](https://baike.baidu.com/item/异常)机制向程序报告，而[类Unix系统](https://baike.baidu.com/item/类Unix系统)则会使用[信号](https://baike.baidu.com/item/信号)机制。如果程序未处理相关问题，那么操作系统会执行默认处理方式，通常是转储内存、终止相关的程序，然后向用户报告。

---

缺页中断发生时的事件顺序如下：

1) 硬件陷入内核，在内核[堆栈](https://baike.baidu.com/item/堆栈)中保存[程序计数器](https://baike.baidu.com/item/程序计数器)。大多数机器将当前指令的各种状态信息保存在特殊的CPU[寄存器](https://baike.baidu.com/item/寄存器)中。

2) 启动一个汇编代码例程保存[通用寄存器](https://baike.baidu.com/item/通用寄存器)和其他易失的信息，以免被操作系统破坏。这个例程将操作系统作为一个函数来调用。

3) 当操作系统发现一个缺页中断时，尝试发现需要哪个虚拟页面。通常一个硬件寄存器包含了这一信息，如果没有的话，操作系统必须检索程序计数器，取出这条指令，用软件分析这条指令，看看它在缺页中断时正在做什么。

4) 一旦知道了发生缺页中断的[虚拟地址](https://baike.baidu.com/item/虚拟地址)，操作系统检查这个地址是否有效，并检查存取与保护是否一致。如果不一致，向进程发出一个信号或杀掉该进程。如果地址有效且没有保护错误发生，系统则检查是否有空闲[页框](https://baike.baidu.com/item/页框)。如果没有空闲页框，执行[页面置换算法](https://baike.baidu.com/item/页面置换算法)寻找一个页面来淘汰。

5) 如果选择的页框“脏”了，安排该页写回磁盘，并发生一次[上下文切换](https://baike.baidu.com/item/上下文切换)，挂起产生缺页中断的进程，让其他进程运行直至磁盘传输结束。无论如何，该页框被标记为忙，以免因为其他原因而被其他进程占用。

6) 一旦页框“干净”后（无论是立刻还是在写回磁盘后），操作系统查找所需页面在磁盘上的地址，通过磁盘操作将其装入。该页面被装入后，产生缺页中断的进程仍然被挂起，并且如果有其他可运行的用户进程，则选择另一个用户进程运行。

7) 当磁盘中断发生时，表明该页已经被装入，[页表](https://baike.baidu.com/item/页表)已经更新可以反映它的位置，[页框](https://baike.baidu.com/item/页框)也被标记为正常状态。

8) 恢复发生缺页[中断指令](https://baike.baidu.com/item/中断指令)以前的状态，[程序计数器](https://baike.baidu.com/item/程序计数器)重新指向这条指令。

9) 调度引发缺页中断的进程，操作系统返回调用它的汇编语言例程。

10) 该例程恢复寄存器和其他状态信息

![img](https://img-blog.csdn.net/20180410125645119?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0FsYXRlYmxvb21lcg==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)

![img](https://img-blog.csdn.net/2018041013120418?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0FsYXRlYmxvb21lcg==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)

> [五、虚拟内存](https://blog.csdn.net/Alatebloomer/article/details/79876498)

### 5.3.7 后备存储

在何处保存未被映射的页？

+ 能够简单地识别在二级存储器中的页
+ 交换空间（磁盘或者文件）：特殊格式，用于存储未被映射的页面

概念：后备存储backing store

+ 一个虚拟地址空间的页面可以被映射到一个文件(在二级存储）中的某个位置*（windows保存到文件[pagefile.sys](https://baike.baidu.com/item/PageFile.Sys) 中）*
+ 代码段：映射到可执行二进制文件
+ 动态加载的共享库程序段：映射到动态调用的库文件
+ 其他段：可能被映射到交换文件（swap file）

### 5.3.8 虚拟内存性能

为了方便理解分页的开销，使用有效存储器访问时间effective memory access time（EAT）。
$$
EAT=访问时间*页表命中几率+page fault处理时间*page fault几率
$$
举例：

访问时间：10ns；	磁盘访问时间：5ms；

参数p = page fault几率；	参数q = dirty page几率；

EAT = 10 \* （1-p) + 5000000p(1+q)

*(这里q，是置换页面的时候，被置换的页面可能修改位是1，那么置换出去还需要覆写外存，消耗额外的时间)*

*（明显，p越小，也就是页表命中率越高的话，缺页中断出现越少。而一般程序也经常是一个页4KB的数据访问几百次，然后才会触发一次缺页中断。减少缺页中断损耗的时间，程序执行的效率也就间接提高了。）*

![img](https://img-blog.csdn.net/20180410132934394?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0FsYXRlYmxvb21lcg==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)

> [五、虚拟内存](https://blog.csdn.net/Alatebloomer/article/details/79876498)

# 6. 页面置换算法

## 6.0 概述

### 6.0.1 功能与目标

功能：当缺页中断发生，需要调入新的页面而内存已满时，根据特定算法选择内存当中某个物理页面置换。

目标：**尽可能地减少**页面地换进换出次数（即缺页中断的次数）。具体来说，把未来不再使用的或短期内较少使用的页面换出，通常只能在局部性原理指导下依据过去的统计数据来进行预测。

**页面锁定**（frame locking）：用于描述必须常驻内存的操作系统的关键部分或时间关键（time-critical）的应用进程。实现的方法是：在页表中添加锁定标志位（lock bit）。

*（有些特殊的代码段，比如操作系统的代码段，需要一直存在于内存中，才能正常运行操作系统。这时候，就可以给操作系统使用的代码段所在的页面添加标志位lock bit，这样页表含有lock bit的页就不会参与页面置换算法，直接被忽略。）*

*由于访问同一页必定不会产生缺页中断，所以考虑缺页中断和页面置换的情况时，无需关心页内偏移量。只考虑页号。*

> [操作系统课本里面说页表项占用一个字节是怎么来的？](https://www.v2ex.com/t/309437)
>
> 在 32 位 Linux 中，一个页是 4K（ 12 位地址空间），剩下的 20 位地址空间被二级页表索引，每一级负责 10 位。
>
> 但是页表项除了地址还要存其他很多信息，比如页的访问权限之类的。所以每一个页表项依旧需要 4 个字节。
>
> 那么问题就明朗了，一级页表 1024 项（ 10 位地址空间嘛），每个项目 4 字节正好 4K （一页）。二级页表项也是 1024 项，每个项目依旧 4 字节正好 4K 。最后形成的两级页表体系一共能表示 20 位地址空间，补上 12 位页内地址空间正好 32 位。
>
> 需要说明的是二级页表只对 32 位系统有效， 64 位就是其他的做法了。

### 6.0.2 常见算法

**局部页面置换算法**：

+ 最优页面置换算法（OPT，optimal）
+ 先进先出算法（FIFO）
+ 最近最久未使用算法（LRU，Least Recently Used）
+ 时钟页面置换算法（Clock）
+ 二次机会法（或者enhanced clock）
+ 最不常用算法（LFU，Least Frequently Used）

**全局页面置换算法**

+ 工作集页置换算法
+ 缺页率置换算法

局部页面置换算法，目的是减小**单进程**的缺页中断次数。

全局页面置换算法，目的是减少**操作系统系统整体**出现的缺页中断次数。

## 6.1 局部页面置换算法

### 6.1.1 最优页面置换算法(OPT，optimal)

基本思路：

​	当一个缺页中断发生时，对于保存在内存当中的每一个<u>逻辑页面</u>，计算在它下一次访问之前，还需等待多长时间，从中选择等待时间最长的那个，作为被置换的页面。

这只是一种理想情况，在实际系统中是无法实现的，因为操作系统无法知道每一个页面要等待多长时间以后才会再次被访问。

最优页面置换算法，可用作其他算法的性能评价的依据<small>（在一个模拟器上运行某个程序，并记录每一次的页面访问情况，在第二遍运行时即可使用最优算法）</small>。

![img](https://img-blog.csdn.net/20180417162011992?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0FsYXRlYmxvb21lcg==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)

> [六、页面置换算法](https://blog.csdn.net/Alatebloomer/article/details/79976068)

### 6.1.2 先进先出算法（FIFO)

基本思路：

​	选择在内存中驻留时间最长的页面并淘汰。具体来说：操作系统维护着一个链表，其记录了所有内存当中的<u>逻辑页面</u>。从链表的排序顺序来看，链首页面的驻留时间最长，链尾页面的驻留时间最短。当发生一个页面中断时，把链首页面淘汰出局，把新的页面添加到链表的末尾。 

**性能较差**，调出的页面可能是经常要访问的页面，并且有**Belady现象**，FIFO算法很少单独使用。 

*（理论上，给的物理页总量越多，那么产生的缺页中断次数应该越少。而采用FIFO的页置换算法时，同种访问页表项的顺序下，加大物理页总量，有时反而出现缺页中断次数增多的现象。这就是Belady现象，只有FIFO算法会有该现象。）*

![img](https://img-blog.csdn.net/2018041716313855?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0FsYXRlYmxvb21lcg==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)

> [六、页面置换算法](https://blog.csdn.net/Alatebloomer/article/details/79976068)

### 6.1.3 最近最久未使用算法（LRU，Least Recently Used）

基本思路：

​	当一个缺页中断发生时，选择最久未使用的那个页面，并淘汰。

它是最优置换算法的近似，其根据程序的局部性原理，即在最近一小段时间（最近几条指令）内，如果某些页面被频繁访问，那么在将来的一小段时间内，它们还可能会再被频繁访问。反过来说，过去某些页面长时间未被访问，那么它们将来还可能长时间地得不到访问。

*（最优置换算法OPT根据未来推测未来，所以肯定最优，但是不现实。而LRU算法，根据过去推测未来，假定过去不常被访问的页面将来也可能不会再被访问。在OS长期工作下，LRU效益可近似OPT。）*

![img](https://img-blog.csdn.net/20180417165641987?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0FsYXRlYmxvb21lcg==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)

<u>LRU算法需要记录各个页面使用时间的先后顺序，开销比较大</u>。两种常见的实现方法是：

- 系统维护一个页面链表。

  ​	最近刚使用过的页面作为首结点，最久未使用的页面作为尾结点，每一次访问内存时，找到相应的页面，把它从链表中摘下来，再移动到链表之首。每次缺页中断发生时，淘汰链表末尾的页面。

- 设置一个活动页面堆栈。

  ​	当访问某页时，将此页号入栈顶，然后，考察栈内是否有与此页面相同的页号，若有则抽出。当需要淘汰一个页面时，总是选择栈底的页面，它就是最久未使用的。

![img](https://img-blog.csdn.net/20180417165714990?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0FsYXRlYmxvb21lcg==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)

> [六、页面置换算法](https://blog.csdn.net/Alatebloomer/article/details/79976068)

### 6.1.4 时钟页面置换算法(Clock)

 Clock 页面置换算法，LRU的近似，对FIFO的一种改进： 

基本思路：

+ 需要用到页表项当中的访问位，当一个页面被装入内存时，把该位初始化为0。然后如果这个页被访问（读/写）时，则把它置为1。（硬件完成访问位置1的操作,无需软件实现）

- 把各个页面组织成环形链表（类似钟表面），把指针指向最老的页面（最先进来）。
- 当发生一个缺页中断，考察指针所指向的最老的页面，若它的访问为为0，则立即淘汰；若访问位为1，则把该位置为0，然后指针往下移动一格。如此下去，直到找到被淘汰的页面，然后把指针移动到它的下一格。

*（Clock算法比起LRU而言，判断最老页面不够精确，因为系统也会周期性把所有页的访问位置0，以避免死锁。而且CLock算法的环形队列判断访问位0再置换，并不一定置换的就是最老页面。相对LRU更粗糙，但是不需要维护LRU需求的复杂数据结构和计算，实现较简单。）*

![img](https://img-blog.csdn.net/20180417172110816?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0FsYXRlYmxvb21lcg==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)

Clock算法，维持一个环形页面链表保存在内存中

+ 用一个时钟（或者使用/引用）位来标记一个页面是否经常被访问
+ 当一个页面被引用时，这个位被设置（为1）

时钟头扫描页面寻找一个带有`used bit=0`，将其替换出来。（替换在一个周期内没有被引用过的页面）

```pascal
func Clock_Replacement
begin
while (victim page not found) do
	if (used bit for current page = 0) do
		replace current page(& set used bit to 1)
	else
		reset used bit(to 0)
	end if 
	advance clock pointer
end while
end Clock_Replacement
```

![img](https://img-blog.csdn.net/2018041717255663?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0FsYXRlYmxvb21lcg==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)

*<small>（虽然这个模拟场景比前面LRU算法多了一次缺页中断，但是实际应用中，Clock算法效益接近LRU。而且Clock算法只用一个`used bit`，相对LRU需要维护链表or栈要更简单且开销小。）</small>*

> [六、页面置换算法](https://blog.csdn.net/Alatebloomer/article/details/79976068)

### 6.1.5 二次机会法（或者enhanced clock）

​	由于Clock算法，只考虑页是否被访问，没考虑页是否被修改，这可能导致被换出的页经常是被"写"过的，这样内存置换该页时，还需要覆写一遍数据到外存（硬盘）中，需要额外消耗时间。

​	于是出现了在Clock算法基础上改进的，二次机会法。其同时用**脏位**`dirty bit`和**使用位**`used bit`来指导置换，允许脏页总是在一次时钟头扫描中保留下来。
*(尽量优先置换未被访问且未被执行写操作的页。脏页需要时钟头扫描两次才可能被置换，所以该算法被称作二次机会法)*

![img](https://img-blog.csdn.net/20180417201216650?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0FsYXRlYmxvb21lcg==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)

+ `dirty bit`，标记1说明该页需进行写操作，0说明没有进行写操作。和`used bit`一样都是硬件修改数值，无需操作系统干预。
+ `used bit`只能看出来该页是否被访问过，不管页是用来读还是写，都是1。

这里指针每次转动时，会把`used bit`和`dirty bit`同时值为1以外的都置为双0，只有双1变成0 1。也就是访问过且被修改过的脏页，会在内存中多一次存活的机会。

*毕竟修改过的页，要置换出去之前，还需要先覆写内存数据到硬盘原本的物理位置，这需要消耗大量时间，所以二次机会法力求尽量置换没用过或者只读的内存页面。*

![img](https://img-blog.csdn.net/20180417203910279?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0FsYXRlYmxvb21lcg==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)

*w表示访问内存页，且需要进行写操作，即`dirty bit = 1`*。

这个二次机会法的样例里，和前面LRU一样是3次缺页中断。和Clock算法一样，二次机会法在实际应用中效果近似LRU算法。并且二次机会法区分读/写情况，尽量置换只读页，减少内存覆写硬盘的开销。

> [六、页面置换算法](https://blog.csdn.net/Alatebloomer/article/details/79976068)

### 6.1.6 最不常用算法(LFU，Least Frequently Used)

- 基本思路：当一个缺页中断发生时，选择访问次数最少的那个页面，并淘汰。
- 实现方法：对每个页面设置一个访问计数器，每当一个页面被访问时，该页面的访问计数器加1。在发生缺页中断时，淘汰计数值最小的那个页面。
- LRU/LFU区别：LRU考察的是多久未访问，时间越短越好；而LFU考察的是访问次数或频度，访问次数越多越好。 

LFU算法可能存在的某类问题：一个页面在进程开始时使用的次数很多，但是以后就再也不用了，如果计数器保持数值不变，那么这个页可能直到进程结束前都不会被释放。（解决方法：定期把次数寄存器右移以一位。）

*（LRU是最近没被使用，LFU是最近使用最少次，两者实现的开销都相对较大。LRU需要借助链表or栈等数据结构；而LFU需要给每个页表另外配置访问计数器，那么硬件上可能需要额外的寄存器等，开销大。）*

*（所以如果要综合考虑成本和效率去使用LFU，不能简单考虑硬件层面添加寄存器等设备。最好能找到类似Clock这类折中且效率近似LRU的实现方式。）*

### 6.1.7 Belady现象

> [belady现象--百度百科]([https://baike.baidu.com/item/belady%E7%8E%B0%E8%B1%A1/7635569?fr=aladdin](https://baike.baidu.com/item/belady现象/7635569?fr=aladdin))
>
> 所谓Belady现象是指：在分页式虚拟存储器管理中，发生缺页时的置换算法采用FIFO（[先进先出](https://baike.baidu.com/item/先进先出/9629304)）算法时，如果对一个进程未分配它所要求的全部页面，有时就会出现分配的页面数增多但缺页率反而提高的异常现象。

- Belady现象：在采用FIFO算法时，有时会出现分配的物理页面数增加，缺页率反而提高的异常现象。
- 出现Belady现象的原因：FIFO算法的置换特征与进程访问内存的动态特征是矛盾的，与置换算法的目标是不一致的（即替换较少使用的页面），因此，被它置换出去的页面不一定是进程不会访问的。

### 6.1.8 LRU、FIFO和Clock的比较

- LRU算法和FIFO本质都是先进先出的思路，只不过LRU是针对**页面的最近访问时间**来进行排序，所以需要在每一次页面访问的时候动态地调整各个页面之间的先后顺序（每一个页面的最近访问时间变了）；而FIFO是针对**页面进入内存的时间**来进行排序，这个时间是固定不变的，所以各页面之间的先后顺序是固定的。如果一个页面在进入内存后没有被访问，那么它的最近访问时间就是它进入内存的时间。换句话说，如果内存当中的所有页面都未曾被访问过，那么LRU算法就会退化为FIFO算法。

  例如：给进程分配3个物理页面，逻辑页面的访问顺序为1、2、3、4、5、6、1、2、3......

- LRU算法性能较好，但系统开销较大；FIFO算法的系统的开销较小，但可能发生Belady现象。因此，折中的办法就是Clock算法，在每一次页面访问时，它不必去动态调整页面在链表中的顺序，而仅仅是做一个标记，然后等到发生缺页中断的时候，再把它移动到链表的末尾。对于内存当中那些未被访问的页面，Clock算法的表现与LRU算法一样好；而对于那些曾经访问过的页面，它不能像LRU那样，记住它们的准确位置（访问顺序）。

*（Clock和基于Clock改进的二次机会法，本身都是通过1~2bit标志位，来近似模拟LRU算法。如果我们的程序本身不符合局部性原则，那么LRU算法实际的效果会退化成FIFO。同理，近似Clock和二次机会法也有可能退化成FIFO。所以我们程序开发本身需要考虑局部性原则，这样才能充分发挥页面置换算法的作用。）*

## 6.2 全局页面置换算法

### 6.2.0 概述

全局页面置换算法置换页面的选择范围是所有可换出的物理页面

全局置换需要解决的问题：

- 进程在不同阶段的内存需求是变化的
- 分配给进程的内存也需要在不同阶段有所变化
- 全局置换算法需要确定分配给进程的物理页面数

> [六、页面置换算法](https://blog.csdn.net/Alatebloomer/article/details/79976068)

### 6.2.1 工作集和常驻集

1. **工作集**

   前面介绍的各种页面置换算法都是基于一个前提，即程序的局部性原理。但是此原理是否成立？

   + 如果局部性原理不成立，那么各种页面算法都没有什么区别，也就没什么意义。例如：假设进程对逻辑页面的访问顺序是1、2、3、4、5···，即单调递增，那么在**物理页面**数有限的前提下，不管采用何种页面置换算法，每次的页面访问都必然导致缺页中断。 

   + 如果局部性原理是成立的，那么如何证明它的存在，如何对它进行定量分析？这就是工作集模型！

   工作集：一个进程当前使用的**逻辑页面集合** ，可以用一个二元函数`W(t,Δ)`表示。

   + `t`是当前的执行时刻；

   + `Δ`称为工作集窗口 （working-set window），即一个定长的页面访问的时间窗口；

   + `W(t,Δ)`表示在当前时刻t之前的Δ时间窗口当中的所有访问页面组成的集合（随着t的变化，该集合也在不断地变化）；

   + `| W(t,Δ)|`指工作集的大小，即页面数目。

   ![img](https://img-blog.csdn.net/20180418094046133?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0FsYXRlYmxvb21lcg==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)

   工作集大小的变化：进程开始执行后，随着访问新页面逐步建立较稳定的工作集。当内存访问的局部性区域的位置大致稳定时，工作集大小也大致稳定；局部性区域的位置改变时，工作集快速扩张和收缩过渡到下一个稳定值。

   ![img](https://img-blog.csdn.net/20180418094502895?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0FsYXRlYmxvb21lcg==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)

2. **常驻集**

   常驻集是指在当前时刻，**进程实际驻留在内存当中的页面集合**。

   - 工作集是进程在运行过程中固有性质。而常驻集取决于系统分配给进程的物理页面数目，以及所采用的页面置换算法；
   - 如果一个进程的整个工作集在内存当中，即工作集⊆（=）常驻集,那么进程将很顺利地运行,而不会造成太多的缺页中断(直到工作集发生剧烈变动，从而过渡到另一状态)；
   - 当进程常驻集的大小达到某个数目之后，再分配更多的物理页面，缺页率也不会明显下降。

常驻集和工作集很好理解。比如某程序占用硬盘页A~Z，但是操作系统只先把程序A~G部分放入内存(因为还有其他程序要用内存,所以不能给这个程序一下子分配A~Z那么多页的物理内存空间)，这个A~G大小就是常驻集，常驻集就是{A~G}；而工作集，和程序执行有关，是实时变动的，比如A~Z内，某时间段内工作集{A~C}，也就是只有A~C的代码被执行了，D-G还没有被执行，但是也在内存中待命就是了。

这种常驻集没有和程序等大的，肯定会有缺页中断发生。比如A~G先加载到页表，A-G每个页面都要至少中断一次(从硬盘加载到内存）。之后A~G代码可能还要调用H~Z范围的代码，那就又需要把H~Z的从物理硬盘加载某几页到内存，内存需要置换出来一些（置换方式,看具体什么算法。）

> [六、页面置换算法](https://blog.csdn.net/Alatebloomer/article/details/79976068)

### 6.2.2 工作集页置换算法

- 思路：置换出不在工作集中的页面
- 当前时刻前`τ`个内存访问的页引用是工作集，`τ`被称为窗口大小
- 实现方法：维护窗口内的访存页面链表，访存时，换出不在工作集的页面；更新访存链表。缺页时，换入页面；更新访存链表。

![img](https://img-blog.csdn.net/20180418101003790?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0FsYXRlYmxvb21lcg==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)

这个样例中，假设工作集窗口大小为4，然后程序分配到的物理帧共5页（内存中加载 常驻集a~e）

这里置换判断依据是"页面是否在工作集窗口范围内"，所以当`Time=2`时，虽然Page e没有引发缺页中断，我们也主动把它置换出去。（内存给我们这个进程5个物理页大小的空间，正好只加载程序的a~e这5页的内容，剩余的还没加载）

工作集算法，从整个系统层面思考缺页中断处理，对于每个进程，都采取尽早把无用内容（不在工作集窗口内的页面）置换出去的策略，使得别的程序更能有机会获取到足够大的物理空间。其避免产生某些程序占用很多无用页帧而引起其他程序被迫频繁缺页中断的现象。

*<small>（有点TCP中的流量控制和拥塞避免的感觉了，流量控制针对通信双方而言，而拥塞避免针对所有TCP通信用户。）</small>*

> [六、页面置换算法](https://blog.csdn.net/Alatebloomer/article/details/79976068)

### 6.2.3 缺页率置换算法

**可变分配策略**：常驻集大小可变 。例如：每个进程在刚开始运行的时候，根据程序的大小给它分配一定数目的物理页面，然后在进程运行过程中，再动态地调整常驻集的大小。

- 可采用**全局页面置换**的方式，当发生一个缺页中断时，被置换的页面可以是在其他进程当中，各个并发进程竞争地使用物理页面。
- 优缺点：性能较好，但增加了系统开销。
- 具体实现：可以使用**缺页率置换算法**（PFF, page fault frequency）来动态调整常驻集的大小。

**缺页率**

缺页率表示"缺页次数/ 内存访问次数"（比率）或"缺页的平均时间间隔的倒数"。

影响缺页率的因素：

- 页面置换算法<small>（每个页面置换算法在不同页面使用情况下缺页率不尽相同）</small>
- 分配给进程的物理页面数目<small>（除了FIFO可能产生Belady异常现象，其他页面置换算法，一般程序分配到的物理页面越多，缺页率越小。）</small>
- 页面本身的大小<small>（比如一个页大小从4KB改成4MB，那么一般缺页率也会更小。毕竟一页能够加载原本的1024倍大小的内容）</small>
- 编程方法<small>（一般而言，程序编写越符合时间和空间局部性，那么出现缺页的概率越低。）</small>

若运行的程序缺页率过高，则通过增加工作集来驻集来分配更多的物理页面；

若运行的程序缺页率过低，则通过减少工作集以减少它的物理页面数。

力图使每个运行中的程序的缺页率保持在一个合理的范围内。

![img](https://img-blog.csdn.net/20180418102536991?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0FsYXRlYmxvb21lcg==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)

实现：

- 访存时，设置引用位标志
- 缺页时，计算从上次缺页时间tlast到现在tcurrent的时间间隔

1. 如果tcurrent– tlast>T,则置换所有在[tlast, tcurrent]时间内没有被引用的页
2. 如果tcurrent – tlast≤ T, 则增加缺失页到工作集中

![img](https://img-blog.csdn.net/20180418103854147?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0FsYXRlYmxvb21lcg==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)

​	工作集置换算法每次Time变动都要检查窗口内的页是否需要置换，而缺页率置换算法只在发生缺页中断时才进行页置换操作。

​	相对局部页面置换算法（FIFO、LRU、LFU、Clock、二次机会法）等，工作集置换算法和缺页率置换算法，这两种全局置换算法针对的是整个操作系统的程序缺页中断的控制，而不是针对某一个进程的缺页中断处理。对于多道程序运行的操作系统来说，全局置换算法是必备品。<small>（操作系统的实际实现肯定没有像这几个描述的那么简单，但是大致的思想都有提到）</small>

> [六、页面置换算法](https://blog.csdn.net/Alatebloomer/article/details/79976068)

### 6.2.4 抖动问题

- 如果**分配给一个进程的物理页面太少**，不能包含整个工作集，即常驻集∈工作集，进程会造成很多的缺页中断，需要频繁地在内存和外存之间替换页面，从而使进程的运行速度变慢，我们把这种状态称为”抖动”。 
- 产生抖动的原因：随着驻留内存的进程数目增加，分配给每个进程的物理页面数不断减少，缺页率不断上升。所以OS要选择一个适当的进程数目和进程需要的帧数，以便在并发水平和缺页率之间达到一个平衡。 

抖动问题可能会被本地的页面置换改善。

更好的规则为加载控制：调整并发进程数（MPL）来进行系统负载控制

- ∑WSi=内存的大小
- 平均缺页间隔时间mean time between page faults(MTBF) = 缺页异常处理时间 page fault service time(PFST)

![img](https://img-blog.csdn.net/20180418110731341?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0FsYXRlYmxvb21lcg==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)

MTBF/PFST的比值越大，说明CPU用于处理缺页中断的时间越少，当比值为1时，CPU达到能承载的程序的最大数量，如果超过这个进程并发数，CPU利用率会大大降低。<small>（因为缺页中断需要内存和硬盘频繁IO交互，CPU没啥事干，等待中断处理完毕后，获取到数据对应的物理地址后，才能继续工作。当然CPU也不是真没啥事干，X进程缺页中断，那就保存X进程PCB状态，先去执行别的进程比如Y进程。）</small>

> [六、页面置换算法](https://blog.csdn.net/Alatebloomer/article/details/79976068)

# 7. 进程和线程、上下文切换

## 7.1 进程

### 7.1.1 进程概述

1. 进程定义

   一个具有一定独立功能的程序在一个数据集合上的一次动态执行过程。俗话说，运行的程序=进程。

2. 进程的组成

   - <u>程序的代码</u>； 
   - <u>程序处理的数据</u>； 
   - 程序计数器中的值，指示下一条将运行的指令； 
   - 一组通用寄存器的当前值，堆，栈； 
   -  一组系统资源（如打开的文件）；

   总之，进程包含了正在运行的一个程序的所有状态信息。

3. 进程与程序的联系
   - 程序是产生进程的基础<small>（一个程序可以产生多个进程，例如通过`fork`系统调用产生子进程）</small>
   - 程序的每次构成不同的进程<small>（就算程序代码一样，没有使用什么随机数，但是产生的PID不同。分配到的虚拟地址也不一定每次都一样）</small>
   - 进程是程序功能的体现<small>(很好理解，你代码不运行那写干嘛)</small>
   - 通过多次执行，一个程序可对应多个进程；调用关系，一个进程可包括多个程序 。<small>（一个程序对应多个进程，比如调用`fork`产生新进程；一个进程包括多个程序，比如进程中调用`exec`执行别的程序）</small>
4. 进程与程序的区别
   - 进程是动态的，程序是静态的：程序是有序代码的集合；进程是程序的执行，进程有核心态/用户态<small>(程序代码中文件操作等，需要向OS申请系统调用，用户进程本身用户态运行，系统调用/中断/异常等核心态事件处理需要OS去执行，然后再将结果返回给用户态的进程。)</small>
   - 进程是暂时的，程序是永久的：进程是一个状态变化的过程，程序可永久保存
   - 进程与程序的组成不同：进程的组成包括程序、数据和进程控制块（PCB，Processing Control Block）。

### 7.1.2 进程的特点

- 动态性：可动态的创建、结束进程；
- 并发性：进程可以倍独立调度并占用处理机运行；<small>（并发，多个进程抢占一个CPU核；并行，多个CPU核可同时被多个进程占用）</small>
- 独立性：不同进程的工作不相互影响 ；<small>(操作系统管理页表，给每个进程分配不同的物理帧空间)</small>
- 制约性：因访问共享数据/资源或进程间同步而产生制约。<small>（后续探讨的同步、互斥问题）</small>

![img](https://img-blog.csdn.net/20180418173728904?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0FsYXRlYmxvb21lcg==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)

程序=算法+数据结构

描述进程的数据结构：进程控制块（Process Control Block，PCB）

操作系统为每个进程都维护了一个PCB，用来保存与该进程有关的各种状态信息。

> [七、进程和线程](https://blog.csdn.net/Alatebloomer/article/details/79993225)

### 7.1.3 进程控制块PCB

​	进程控制块：操作系统管理控制进程运行所用的信息集合。操作系统用PCB来描述进程的基本情况以及运行变化的过程。**PCB是进程存在的唯一标识**。<small>（通常，PCB是存放在内核态的内存空间里的，与用户态的进程分开物理帧区域来存放）</small>

- 进程的创建：为该进程生成一个PCB
- 进程的终止：回收它的PCB
- 进程的组织管理：通过对PCB的组织管理来实现

---

PCB含有以下三大类信息： 

1. **进程标识信息**

   ​	如本进程的标识，本进程的产生者标识（父进程标识）；用户标识。<small>（通常进程标识称为PID，而父进程标识即PPID，parent PID）</small>

2. **处理机状态信息保存区**<small>（保存进程的运行现场信息）</small>

   + 用户可见寄存器。用户程序可以使用的数据，地址等寄存器。

   + 控制和状态寄存器。如程序计数器（PC），程序状态字（PSW）。

   + 栈指针。过程调用/系统调用/中断处理和返回时需要用到它。

3. **进程控制信息**

   + 调度和状态信息。用于操作系统调度进程并占用处理机使用。

   + 进程间通信信息。为支持进程间的与通信相关各种标识、信号、信件等，这些信息存在接收方的进程控制块中。

   + 存储管理信息。包含有指向本进程映像存储空间的数据结构。

   + 进程所用资源。说明由进程打开、使用的系统资源，如打开的文件等。

   + 有关数据结构连接信息。进程可以连接到一个进程队列中，或连接到相关的其他进程的PCB。

---

PCB的组织方式

- 链表：同一状态的进程其PCB成一链表，多个状态对应多个不同的链表

  ​	<small>(各状态的进程形成不同的链表：就绪链表、阻塞链表、等待链表、运行链表...）</small> 

- 索引表：同一状态的进程归入一个index表（由index指向PCB），多个状态对应多个不同的index表

  ​	<small>(各状态的进行形成不同的index表：就绪索引表、阻塞索引表、等待索引表、运行索引表...)</small>

![img](https://img-blog.csdn.net/20180418193850816?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0FsYXRlYmxvb21lcg==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)

> [七、进程和线程](https://blog.csdn.net/Alatebloomer/article/details/79993225) <= 图片来源。

### 7.1.4 进程状态

> [init进程--百度百科]([https://baike.baidu.com/item/init%E8%BF%9B%E7%A8%8B/3042859?fr=aladdin](https://baike.baidu.com/item/init进程/3042859?fr=aladdin))
>
> 描述init进程，它是内核启动的第一个**用户级进程**。init有许多很重要的任务，比如像启动getty（用于用户登录）、实现运行级别、以及处理孤立进程。
>
> 如果一个进程没有父进程，那么其父进程会被设置成init进程。（原本有父进程，但是父进程比子进程提早终止的话，这些失去父进程而被迫修改PPID为1的子进程，就成了僵尸进程。）

进程的生命周期：创建，运行，等待，唤醒，结束。

1. 进程创建

   引起进程的3个主要事件：

   + 系统的初始化<small>（操作系统初始化时，创建的第一个进程，**INIT进程**，用户级进程）</small>

   - 用户请求创建一个新进程
   - 正在运行的进程执行了创建进程的**系统调用**

   创建进程时，操作系统会在内核态初始化对应的PCB。进程创建后，进入就绪态。<small>（一般会被OS移动到就绪队列中，如果存在就绪队列这类数据结构的话）</small>

2. 进程运行

   内核选择一个就绪的进程，让它占用处理机并执行

   <small>(OS根据特定的调度算法，从就绪队列挑一个，使其进入运行态，其PCB放入运行队列，然后对应的用户进程占用CPU执行代码)</small>

3. 进程等待

   在以下情况下，进程等待（阻塞）：

   - 请求并等待系统服务，无法马上完成
   - 启动某种操作，无法马上完成
   - 需要的数据没有到达 

   进程只能自己阻塞自己，因为只有进程自身才能知道何时需要等待某种事件的发生。

   <small>（比如进程请求系统调用，请求磁盘IO读写操作；该进程等待/阻塞，CPU核会先去执行别的进程，等待OS处理完系统调用获取了文件描述符后，将其返回给进程，并将其改成就绪态）</small>

4. 进程唤醒

   唤醒进程的原因：

   - 被阻塞进程需要的资源可被满足
   - 被阻塞进程等待的事件到达
   - 将该进程的PCB插入到**就绪队列**

   **进程只能被别的进程或操作系统唤醒**。

5. 进程结束

   在以下四种情形下，进程结束：

   - 正常退出（自愿的）
   - 错误退出（自愿的）
   - 致命错误（强制性的）<small>(比如试图访问不属于分配给自己的物理帧空间，这是不被允许的)</small>
   - 被其他进程所杀（强制性的）

### 7.1.5 进程状态变化模型

进程的三种基本状态：

​	进程在生命结束前处于且仅处于三种基本状态之一

不同系统设置的进程状态数目不同。

- 运行状态(Running)：当一个进程**正在处理机上运行**时。
- 就绪状态(Ready )：一个进程**获得了除处理机之外的一切资源**，一旦得到处理器就可以运行。
- 等待状态(又称阻塞状态Blocked)：一个进程正在等待某一时间而暂停运行时。如等待某资源，等待输入/输出完成。

进程其他的基本状态：

+ 创建状态(New)：一个进程正在被创建，还没被传到就绪状态之前的状态。
+ 结束状态(Exit)：一个进程正在从系统中消失时的状态，这是因为进程结束或由于其他原因所导致。

---

状态变化图

![img](https://img-blog.csdn.net/20180419110120704?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0FsYXRlYmxvb21lcg==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)

可能的状态变化如下：

- NULL→New:一个新进程被产生出来执行一个程序。
- New→Ready：当进程被创建完成并初始化后，一切就绪准备运行时，变为就绪状态。<small>（该过程通常很快，操作系统完成PCB创建和初始化，并放入就绪队列）</small>
- Ready→Running：处于就绪态的进程被进程调度程序选中后，就分配到处理机上来运行。
- Running→Exit：当进程表示它已经完成或者因出错，当前运行进程会由操作系统作结束处理。
- Running→Ready:处于运行状态的进程在其运行过程中，由于分配给它的处理时间片用完而让出处理机。<small>（操作系统管理，维护一个管理时钟，一旦发现某个进程时间片已经到达期限，就会试图把进程切换回就绪态，并指示CPU执行其他轮到时间片的就绪态进程）</small>
- Running→Blocked：当进程请求某样东西必须等待时。<small>（比如请求操作文件）</small>
- Blocked→Ready：当进程要等待某事件到来时，它从阻塞状态变到就绪状态。<small>（比如操作系统获取到文件描述符，并返回给进程）</small>

> [七、进程和线程](https://blog.csdn.net/Alatebloomer/article/details/79993225) <= 图片来源

### 7.1.6 进程挂起

​	**进程在挂起状态时，意味着进程没有占用内存空间。处在挂起状态的进程映像在磁盘上。**<small>（阻塞态的进程还是在内存中）</small>

挂起状态：

- 阻塞挂起状态（Blocked-suspend ）：进程在外存并等待某事件的出现；

  <small>（进程阻塞状态的时候被挂起）</small>

- 就绪挂起状态（Ready-suspend）：进程在外存，但只要进入内存，就可运行；

  <small>（进程就绪状态的时候被挂起）</small>

---

与挂起相关的状态转换

挂起（Suspend）：把一个进程从内存转到外存，可能有以下几种情况：

- 阻塞到阻塞挂起： 没有进程处于就绪状态或就绪进程要求更多内存资源时，会进行这种转换，以提交新进程或运行就绪进程 ；
- 就绪到就绪挂起：**当有高优先级阻塞（系统认为会很快就绪的）进程和低优先就绪进程时，系统会选择挂起低优先级就绪进程；**
- 运行到就绪挂起： **对抢先式分时系统，当有高优先级阻塞挂起进程因事件出现而进入就绪状态挂起时，系统可能会把运行进程转到就绪挂起状态；**

在外存时的状态转换

- **阻塞挂起到就绪挂起**：当有阻塞挂起进程因相关事件出现时，系统会把阻塞挂起进程转换为就绪挂起进程。

  <small>（比如进程原本要求操作文件，那么操作系统在其挂起后才完成系统调用并返回文件描述符给挂起的进程。此时进程本身还在硬盘中，返回的资源信息也是加载到硬盘，所以进入就绪挂起，而不是就绪态。）</small>

解挂/激活（Activate）：把一个进程从外存转到内存，可能有以下几种情况

- 就绪挂起到就绪：没有就绪进程或就绪挂起进程优先级高于就绪进程时，会进行这种转换；
- 阻塞挂起到阻塞：当一个进程释放足够内存时，系统会把一个高优先级阻塞挂起（系统会认为很快出现所等待的事件）进程转换为阻塞进程；

### 7.1.7 状态队列

OS怎么通过PCB和定义的进程来管理PCB，帮助完成进程的调度？

用进程的观点来看待OS：用户进程、磁盘管理进程、终端进程.......

![img](https://img-blog.csdn.net/20180419161259880?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0FsYXRlYmxvb21lcg==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)

以进程为基本结构的OS：最底层为CPU调度程序（包括中断处理等）；上面一层为一组各式各样的进程；

---

**状态队列**

- 由操作系统来维护一组队列，用来表示系统当中所有进程的当前状态； 
- 不同的状态分别用不同的队列来表示（就绪队列、各种类型的阻塞队列）；
- 每个进程的PCB都根据它的状态加入到相应的队列中，当一个进程的状态发生变化时，它的PCB从一个状态队列中脱离出来，加入到另外一个队列。

![img](https://img-blog.csdn.net/20180419162151776?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0FsYXRlYmxvb21lcg==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)

<u>如果阻塞队列等待的事件类别不同，阻塞队列也可能分成具体的好几个</u>。

*<small>(这里等待事件1得到满足，如果阻塞队列1所有事件都等这个，那么就都从阻塞态变成就绪态；如果这个事件1只满足其中某几个或某一个进程的需求，那么就那几个或那一个转变为就绪态。)</small>*

> [七、进程和线程](https://blog.csdn.net/Alatebloomer/article/details/79993225) <= 一如既往的图片来源。

## 7.2 线程(Thread)管理

### 7.2.0 引言

自从19世纪60年代提出进程概念以来，在操作系统中一直都是以进程作为独立运行的基本单位，直到80年代中期，人们又提出了更小的能独立运行的基本单位——线程。

为什么使用线程？

【案例】编写一个MP3播放软件。

核心功能模块有三个：

（1）从MP3音频文件当中读取数据；

（2）对数据进行解压缩；

（3）把解压缩后的音频数据播放出来；

![img](https://img-blog.csdn.net/20180419164322121?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0FsYXRlYmxvb21lcg==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)

单进程实现，可能导致音频播放不连贯。因为Read()触发系统调用IO，进程需要等待操作系统返回数据，那么进程阻塞或挂起，如果播放器没有事先缓存足够的数据量再允许用户播放，那么用户就会感受到明显的卡顿。

![img](https://img-blog.csdn.net/20180419164453298?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0FsYXRlYmxvb21lcg==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)

多进程实现：如果多核CPU，确实能保证进程并行。

但是有另外的问题，即进程之间如何通信、如何共享数据？

另外，维护进程的系统开销较大。创建进程时，分配资源、创建PCB；撤销进程时，回收资源、撤销PCB；进程切换时，保存当前进程的状态信息。

为此，需要提出一种新的实体，需要满足一下特性：

1. 实体之间可以**并发**地执行；
2. 实体之间共享相同的地址空间；

*（之前进程访问的物理地址空间是不同进程之间分隔开的，而同一进程内的所有线程访问相同的地址空间，即访问同一个进程的地址空间。）*

> [七、进程和线程](https://blog.csdn.net/Alatebloomer/article/details/79993225) <= 图片来源

### 7.2.1 线程概述

> [线程--百度百科]([https://baike.baidu.com/item/%E7%BA%BF%E7%A8%8B/103101?fr=aladdin](https://baike.baidu.com/item/线程/103101?fr=aladdin))
>
> [**线程崩溃必会使进程崩溃吗**](https://bbs.csdn.net/topics/330102295)
>
> 线程崩溃的本质就是内存出错。而内存出错有时不会引起其他线程出错的，因为崩溃的线程，也就是出错的内存有时侯没有被其他线程访问，也就不会产生问题，但有时候会打乱其他线程的内存。

Thread：**进程当中的一条执行流程**。

从两个方面来重新理解进程 

- 从资源组合的角度

  ​	进程把一组相关的资源组合起来，构成了一个资源平台（环境），包括地址空间（代码段、数据段）、打开的文件等各种资源；

- 从运行的角度

  ​	代码在这个资源平台上的一条执行流程（线程）。

![img](https://img-blog.csdn.net/20180419170245966?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0FsYXRlYmxvb21lcg==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)

线程=进程-共享资源

---

线程的优点：

- 一个进程中可以同时存在多个线程；
- 各个线程之间可以并发地执行；
- 各个线程之间可以共享地址空间和文件等资源。 

线程的缺点：

+ 一个线程写崩溃，会导致其所属进程的所有线程都崩溃。

*<small>（早期浏览器实现，每个页面就是一个线程，那么一个页面崩溃，其他页面由于访问到发生崩溃的区域，于是也跟着都崩溃了。所以后来浏览器一般一个页面对应一个进程，这样一个页面崩溃也不会导致其他的页面崩溃。相对性能，先考虑安全性。当然不同场景需要考虑用进程or线程，不是绝对的。）</small>*

*<small>（线程崩溃时，所属的进程也不是一定会奔溃。只是大多数形况下，线程使用的内存区域引发崩溃，这个内存也是主线程会访问到的，进而导致整个进程奔溃。如果崩溃的线程本身访问的资源与其他线程不重合，那就算该线程崩溃，也不会引起整个进程崩溃。）</small>*

---

![img](https://img-blog.csdn.net/20180419171218322?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0FsYXRlYmxvb21lcg==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)

![img](https://img-blog.csdn.net/20180419171425901?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0FsYXRlYmxvb21lcg==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)

*<small>如果每个线程共享PC，那么你没法分开执行程序（毕竟PC就是指向代码要执行的位置）。所以很明显每个线程需要独立的寄存器、栈等，确保每个线程控制流是相对独立的。</small>

> [七、进程和线程](https://blog.csdn.net/Alatebloomer/article/details/79993225) <= 图片来源

### 7.2.2 线程与进程的比较

+ 进程是资源分配单位，线程是CPU调度单位；

- 进程拥有一个完整的资源平台，而线程只独享必不可少的资源，如寄存器和栈；
- 线程同样具有就绪、阻塞和运行三种基本状态，同样具有状态之间的转换关系；

+ 线程能减少并发执行的时间和空间开销

  + 线程的创建时间比进程短
  + 线程的终止时间比进程短
  + 同一进程内的线程切换时间比进程短

  + 由于同一进程的各线程间共享内存和文件资源，可直接进行不通过内核的通信

*<small>（线程各方面"较快"有很大一部分原因是，同一进程下的所有线程使用同一个进程的物理页空间。在进程占有CPU使用时，线程的创建、切换等操作，不会频繁出现缺页中断、TLB表缓存失效等问题。）</small>*

*<small>（其实，线程之前的切换等，也不一定就不会经过内核，因为操作系统对线程的实现不同，对其TCB的实现也不同。一般而言，相对线程也会有近似进程控制块PCB的概念，衍生出TCB线程控制块。至于TCB由内核态还是用户态管理，得看OS实现。一般而言，TCB和PCB都是内核态管理的。）</small>*

### 7.2.3 线程的实现

主要有三种线程的实现方式 ：

- 用户线程：在用户空间实现；<small>(用户线程用户态，操作系统看不到，由应用程序使用的用户线程库去实现和管理)</small>

  <small>**POSIX Pthreads**, Mach C-threads ，Solaris threads</small>

- 内核线程：在内核中实现；<small>（由操作系统创建和管理）</small>

  <small>Windows ，Solaris， Linux</small>

- **轻量级进程**（LightWeight Process）：内核中实现，支持用户线程

  <small>Solaris</small>

*<small>（具体线程到底是哪种类型的，需要看操作系统支持和实现的是哪种，不是你想要啥就一定能用的。）</small>*

**用户线程**与**内核线程**的对应关系：

- 多对一：N个用户线程对应一个内核线程
- 一对一：一个用户线程对应一个内核线程
- 多对多：N个用户线程对应M个内核线程

---

用户线程

*(内核Kernel只能看到进程PCB，而线程的管理由用户线程库实现和管理，操作系统内核无感知用户线程的存在。)*

![img](https://img-blog.csdn.net/20180419192041164?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0FsYXRlYmxvb21lcg==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)

​	在**用户空间**实现的线程机制，它不依赖于操作系统的内核，由一组用户级的线程库函数来完成线程的管理，包括线程创建、终止、同步和调度等。

用户线程的优点：

- 由于用户线程的维护由相应的进程来完成（通过线程库函数），**不需要OS系统内核了解用户线程的存在，可用于不支持线程技术的多进程操作系统**<small>(也就是用户线程，对于内核来说，就是普通的进程代码段，没什么特别的。 )</small>
- 每个进程都需要它自己私有的**线程控制块（TCB）**列表，用来记录它的各个线程的状态信息（PC、栈指针、寄存器），TCB由线程库函数来维护
- 用户线程的切换也是由库函数来完成，无需用户态/核心态切换，所以速度特别快
- 允许每个进程有自定义的线程调度算法

用户线程的缺点：

+ 如果一个线程发起系统调用而阻塞，则整个进程都在等待

  <small>（毕竟系统调用，需要内核态和用户态之间的切换，操作系统对用户线程无感知，会将出现系统调用请求的进程变为阻塞态，那么该进程中所有线程都无法继续执行。）</small>

+ 一个线程开始运行后，除非主动交出CPU使用权，否则所在进程中的其他线程都无法运行

  <small>（因为用户态的线程库没有能够主动打断线程执行的特权，所以只能等待线程主动让出CPU使用权，其他线程才能执行。不像进程有操作系统处理时钟中断，会时不时让其他进程得到执行，也就是操作系统内核态会主动让CPU使用权在各个进程PCB之间切换，而用户态的线程库本身没有这种能力。即用户线程如果代码中没有明确停止运行的操作，那么每次CPU切换进程回该线程时，都是这个线程继续执行，因为它没有写明让其他哪个线程执行，而是继续执行自己本线程的操作。操作系统无感知用户线程，在它看来不过是在正常不过的进程代码在运行而已。）</small>

+ 由于时间片分配给进程，固与其他进程相比，在多线程执行时，每个线程得到的时间片较少，执行会很慢

  <small>(因为你进程就算创建了100个用户线程，但是操作系统无感知用户线程，就给你这个操作系统像往常一样分配不怎么大的时间片。那么你这进程执行时，本来分到时间片就不多了，还得100个线程抢着分用时间片，并且切换线程也要保存一些线程状态，效率不高反而是降低了。所以现在常说的下载器多开线程下载，肯定不是这种用户线程，因为用户线程根本不会加快运行效率，毕竟用的时间片还是所属进程分配到的，又不会比单线程时的进程分配的还多一点之类的。)</small>

---

内核线程

![img](https://img-blog.csdn.net/20180419195056234?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0FsYXRlYmxvb21lcg==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)

<small>*（内核线程，由操作系统管理。Windows使用的即内核线程，所以操作系统能看到TCB线程控制块，即TCB也是在Kernel内核中实现的。）*</small>

内核线程是指在操作系统的**内核**当中实现的一种线程机制，由操作系统的内核完成线程的创建、终止和管理。

- 在支持内核线程的操作系统中，由内核来维护进程和线程的上下文信息（PCB和TCB）；
- 线程的创建，终止和切换都是通过系统调用/内核函数的方式来进行，由内核完成，因此系统开销较大；
- **在一个进程中，如果某个内核线程发起系统调用而被阻塞，并不会影响其他内核线程的运行**；
- **时间片分配给线程，多线程的进程自动获得更多CPU时间**；
- Windows NT和Windows 2000/XP支持内核线程。

*(支持内核线程的操作系统，CPU调度的单位就是TCB线程，而不再是进程PCB了。内核线程的创建、中止、管理都由操作系统完成，这时候进程PCB主要就负责资源的管理。)*

*(同一进程的所有线程TCB由该进程PCB统一管理。)*

*(由于内核线程是内核创建的，所以线程之间的切换也是需要经过内核态和用户态之间的切换。用户线程切换只需要保存当时的线程状态即可，内核级线程间切换相对而言开销更大一点。）*

*(现在Windows程序一般是内核线程，好处是粒度更小，缺点就是线程管理需要用户态和内核态之间的切换，开销大一点）*

> [七、进程和线程](https://blog.csdn.net/Alatebloomer/article/details/79993225) <= 依旧图片来源担当。
>
> [Linux 用户级线程和内核级线程](https://blog.csdn.net/itakyubi/article/details/100024284)

---

轻量级进程（LightWeight Process）

它是**内核支持的用户线程**。一个进程有一个或多个轻量级进程，**每个轻量级进程由一个单独的内核线程**来支持。（Solaris/Linux）

*（Solaris系统已经基本没什么人用了←指新项目基本都考虑放Linux上，其实只考虑Linux的实现即可）*

![img](https://img-blog.csdn.net/20180419200132492?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0FsYXRlYmxvb21lcg==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)

Linux采用一个用户线程对应一个轻量级进程的方式，而每个轻量级进程由一个单独的内核线程支持。

Solaris既可一个用户线程对应一个轻量级进程，也可以N个用户线程对应M个轻量级进程，每个轻量级进程同样还是由一个独立的内核线程支持。<small>（Solaris理论上更加灵活，但是这样线程出错要跟踪找源头等其实更繁琐）</small>

> [七、进程和线程](https://blog.csdn.net/Alatebloomer/article/details/79993225) <= 依旧图片来源担当

### 7.2.4 Linux线程和Java线程(优质网文转载)

#### 对下面转载的几篇文章的概要

根据下述几篇文章描述，Linux线程其实就是进程，Linux中线程和进程都是用的数据结构`task_struct`，即TCB包含于PCB，不过是线程在`task_struct`中标识使用和主线程（进程本身本处的线程）相同的虚拟地址空间（逻辑地址）。

Linux的同一个进程的所有线程的`Tgid`（线程组id）都相同，为该进程的`pid`，而每个线程的`pid`是不同的，毕竟Linux线程本质还是进程。不过Linux创建进程和线程的成本都相对早期的UNIX要低很多，因为创建子进程采用COW策略（Copy-On-Write），只有被创建的子进程和父进程谁对父进程原本的数据段、代码段等进程资源进行写操作触发异常<small>（进程中创建子进程，会先标记该进程原本的虚拟空间为只读，如果该进程or其子进程对这片虚拟区域进行写操作，触发异常）</small>，然后操作系统才会真正给子进程分配独立的虚拟空间。（当然如果进程中新建的是子线程，而不是子进程的话，子线程本身就没必要分配独立虚拟空间，和父进程共用虚拟空间，所以创建子进程时就没必要考虑COW策略了）。

Linux创建子进程，采用COW策略，相对成本低；而创建子线程，实际创建的是和当前进程共用虚拟空间的进程，连COW策略都省了，所以成本也低。

Linux进程的`task_struct`中表示使用的虚拟地址空间的数据结构是`mm_struct`，`mm_struct`不会在用户进程退出后马上被释放，只有没有用户线程指针指向该进程原本所占用的`mm`，且没有内核线程的`active_mm`指向该用户进程的`mm`，即既没有用户进程使用这个地址空间，也没有内核线程引用这个地址空间时，那么该退出的进程的`mm_struct`才会被释放。

Linux内核态线程，`mm`为NULL，而用户态线程`mm`指向其进程占用的虚拟空间；当内核态线程出于某些原因需要访问内核空间时，会系统使`task_struct`中`mm_struct`的`active_mm`指向OS记录的CPU最近一次调用的用户进程的`mm`，因为所有用户进程/线程的内核空间都是一样的，所以通过随便一个用户进程/进程的`mm`就能访问页表查找到内核空间。

*<small>(这小字部分下面的内容是下面转载的[Linux下的进程类别（内核线程、轻量级进程和用户进程）以及其创建方式--Linux进程的管理与调度（四）](https://blog.csdn.net/gatieme/article/details/51482122)的最后几段文字)</small>*

Linux使用`task_struct`来描述进程和线程

1. 一个进程由于其运行空间的不同, 从而有**内核线程**和**用户进程**的区分, 内核线程运行在内核空间, 之所以称之为线程是因为它没有虚拟地址空间, 只能访问内核的代码和数据, 而用户进程则运行在用户空间, 不能直接访问内核的数据但是可以通过中断, 系统调用等方式从用户态陷入内核态，但是内核态只是进程的一种状态, 与内核线程有本质区别
2. 用户进程运行在用户空间上, 而一些通过共享资源实现的一组进程我们称之为线程组, Linux下内核其实本质上没有线程的概念, Linux下线程其实上是与其他进程共享某些资源的进程而已。但是我们习惯上还是称他们为**线程**或者**轻量级进程**

因此, Linux上进程分3种，内核线程（或者叫核心进程）、用户进程、用户线程, 当然如果更严谨的，你也可以认为用户进程和用户线程都是用户进程。

**内核scheduler在进程context switching的时候，会根据tsk->mm判断即将调度的进程是用户进程还是内核线程**

**内核线程拥有 进程描述符、PID、进程正文段、核心堆栈**

**用户进程拥有 进程描述符、PID、进程正文段、核心堆栈 、用户空间的数据段和堆栈**

**用户线程拥有 进程描述符、PID、进程正文段、核心堆栈，同父进程共享用户空间的数据段和堆栈**

*用户线程也可以通过exec函数族拥有自己的用户空间的数据段和堆栈，成为用户进程。*

----



> [[linux内核——进程，轻量级进程，线程，线程组](https://www.cnblogs.com/vinozly/p/5617970.html) <= 下方文字+内容 转自该文章

![img](https://images0.cnblogs.com/i/615801/201403/221504410064366.jpg)

一、进程、轻量级进程、线程、线程组之间的关系

借助上图说明：

进程P0有四条执行流，即线程，

主线程t0是它的第一个线程，且与进程P0相关联，

之后衍生出t1、t2、t3三个线程，这三个线程与轻量级进程P1、P2、P3一一关联，

所有的进程、轻量级进程、线程组成了线程组。

轻量级进程也是进程，只不过它与某进程的某特定线程相关联。

二、它们的标识相关说明

**`pid`是进程标识符，`tgid`是线程组标识符**

每个进程都有自己的`pid`，如图中：进程pid（P0）= a，轻量级进程pid（P1）= b / pid（P1）= c / pid（P1）= d。

同属于一个线程组的所有进程、轻量级进程有同样的线程组标识符，且其为第一个线程所关联的进程标识符，

例如：图中第一个线程为t0，它所关联的进程为P0，pid（P0）= a，所以tgid（P1）= a / tgid（P1）= a / tgid（P1）= a / tgid（P1）= a。

**当我们使用函数getpid（current_p）时，返回值不是current_p的pid，而是它的tgid。（current_p为当前进程）**。

这一点，可以从系统调用getpid和gettid中看出来（位于kernel/timer.c）。

```c
asmlinkage long sys_getpid(void)
  {
    return current->tgid;
  }
 asmlinkage long sys_gettid(void)
 {
   return current->pid;
 }
```

---

> [java线程是用户级线程还是内核级线程?](https://blog.csdn.net/gdhgr/article/details/81945993?utm_source=blogxgwz4) <= 下方内容来自该文章的底部评论区

1、证明java线程不是纯粹用户级线程：java中有个fork join框架，这个框架是利用多处理技术进行maprudce的工作，也就证明了内核是可以感知到用户线程的存在，因此才会将多个线程调度到多个处理器中。还有，java应用程序中的某个线程阻塞，是不会引起整个进程的阻塞，从这两点看，java线程绝不是纯粹的用户级线程。 2、再来，证明java线程不是纯粹内核级线程：这点比较直观，如果使用纯粹的内核级线程，那么有关线程的所有管理工作都是内核完成的，用户程序中没有管理线程的代码。显然，java线程库提供了大量的线程管理机制，因此java线程绝不是纯粹的内核级线程。 综上，**java线程是混合型的线程模型，一般而言是通过lwp将用户级线程映射到内核线程中**。

在目前的JDK版本中，操作系统支持怎样的线程模型，在很大程度上决定了Java虚拟机的线程是怎样映射的，这点在不同的平台上没达成一致，Java虚拟机规范中也并未限定Java线程需要使用哪种线程模型来实现。对于Sun JDK来说，它的windows版与Linux版都是使用一对一的线程模型实现的，一条Java线程就映射到一条轻量级进程之中，因为Windows和Linux系统提供的线程模型就是一对一的。而在Solaris平台中，由于操作系统的线程特性可以同时支持一对一（通过Bound Threads或Alternate Libthread实现）及多对多（通过LWP/Thread Based Synchronization实现）的线程模型，因此在Solaris版的JDK也对应提供了两个平台专有的虚拟机参数：`-XX:+UseLWPSynchronization`(默认值)和`-XX:+UseBoundThreads`来明确指定虚拟机使用哪种线程模型。

---

> [内核线程、轻量级进程、用户线程和LinuxThreads库](https://blog.csdn.net/jack05/article/details/5281079) <= 下方内容转自该博文

**内核线程**

内核线程只运行在内核态，不受用户态上下文的拖累。

+ 处理器竞争：可以在全系统范围内竞争处理器资源；
+ 使用资源：唯一使用的资源是内核栈和上下文切换时保持寄存器的空间
+ 调度：调度的开销可能和进程自身差不多昂贵

+ 同步效率：资源的同步和数据共享比整个进程的数据同步和共享要低一些。

**轻量级进程**

**轻量级进程*(LWP)*是建立在内核之上并由内核支持的用户线程，它是内核线程的高度抽象，每一个轻量级进程都与一个特定的内核线程关联**。内核线程只能由内核管理并像普通进程一样被调度。

轻量级进程由`clone()`系统调用创建，参数是`CLONE_VM`，即**与父进程是共享进程地址空间和系统资源**。

与普通进程区别：LWP只有一个最小的执行上下文和调度程序所需的统计信息。

+ 处理器竞争：因与特定内核线程关联，因此可以在全系统范围内竞争处理器资源

+ 使用资源：与父进程共享进程地址空间

+ 调度：**像普通进程一样调度**

**用户线程**

用户线程是完全建立在用户空间的线程库，**用户线程的创建、调度、同步和销毁全由库函数在用户空间完成，不需要内核的帮助**。因此这种线程是极其低消耗和高效的。

+ 处理器竞争：单纯的用户线程是建立在用户空间，其对内核是透明的，因此<u>其所属进程单独参与处理器的竞争，而进程的所有线程参与竞争该进程的资源</u>。

+ 使用资源：与所属进程共享进程地址空间和系统资源。

+ 调度：由在用户空间实现的线程库，在所属进程内进行调度

**Linux使用的线程库**

`LinuxThreads`是用户空间的线程库，所采用的是线程-进程1对1模型(即一个用户线程对应一个轻量级进程，而一个轻量级进程对应一个特定的内核线程)，**将线程的调度等同于进程的调度**，<u>**调度交由内核完成**，而线程的创建、同步、销毁由核外线程库完成</u>（`LinuxThtreads`已绑定到GLIBC中发行）。

在`LinuxThreads`中，由专门的一个管理线程处理所有的线程管理工作。当进程第一次调用`pthread_create()`创建线程时就会先创建*(clone())*并启动管理线程。后续进程`pthread_create()`创建线程时，都是管理线程作为`pthread_create()`的调用者的子线程，通过调用`clone()`来创建用户线程，并记录轻量级进程号和线程*id*的映射关系，因此，**用户线程其实是管理线程的子线程**。

`LinuxThreads`只支持调度范围为`PTHREAD_SCOPE_SYSTEM`的调度，默认的调度策略是`SCHED_OTHER`。

用户线程调度策略也可修改成`SCHED_FIFO`或`SCHED_RR`方式，这两种方式支持优先级为*0-99,*而`SCHED_OTHER`只支持*0*。

+ `SCHED_OTHER`分时调度策略

+ `SCHED_FIFO`实时调度策略，先到先服务

+ `SCHED_RR`实时调度策略，时间片轮转

`SCHED_OTHER`是普通进程的，后两个是实时进程的（**一般的进程都是普通进程，系统中出现实时进程的机会很少**）。`SCHED_FIFO`、`SCHED_RR`优先级高于所有`SCHED_OTHER`的进程，所以只要他们能够运行，在他们运行完之前，所有`SCHED_OTHER`的进程的都没有得到执行的机会。

---

> [Linux下的进程类别（内核线程、轻量级进程和用户进程）以及其创建方式--Linux进程的管理与调度（四）](https://blog.csdn.net/gatieme/article/details/51482122)
>
> 以下内容来自上述文章（只摘取部分）

#### 进程和线程

虽然我们在区分Linux进程类别, 但是我还是想说Linux下只有一种类型的进程，那就是`task_struct`，当然我也想说**linux其实也没有线程的概念, 只是将那些与其他进程共享资源的进程称之为线程。**

1. 一个进程由于其运行空间的不同, 从而有**内核线程**和**用户进程**的区分, 内核线程运行在内核空间, 之所以称之为线程是因为它没有虚拟地址空间, 只能访问内核的代码和数据, 而用户进程则运行在用户空间, 但是可以通过中断, 系统调用等方式从用户态陷入内核态。
2. 用户进程运行在用户空间上, 而一些通过共享资源实现的一组进程我们称之为线程组, Linux下内核其实本质上没有线程的概念, Linux下线程其实上是与其他进程共享某些资源的进程而已。但是我们习惯上还是称他们为**线程**或者**轻量级进程**

因此, Linux上进程分3种，内核线程（或者叫核心进程）、用户进程、用户线程, 当然如果更严谨的，你也可以认为用户进程和用户线程都是用户进程。

进程是具有一定独立功能的程序关于某个数据集合上的一次运行活动,进程是系统进行资源分配和调度的一个独立单位。线程是进程的一个实体,是CPU调度和分派的基本单位,它是比进程更小的能独立运行的基本单位。<u>线程自己基本上不拥有系统资源,只拥有一点在运行中必不可少的资源(如程序计数器,一组寄存器和栈),但是它可与同属一个进程的其他的线程共享进程所拥有的全部资源。</u>

线程与进程的区别归纳：

- 地址空间和其它资源：进程间相互独立，同一进程的各线程间共享。某进程内的线程在其它进程不可见。
- 通信：进程间通信IPC，线程间可以直接读写进程数据段（如全局变量）来进行通信——需要进程同步和互斥手段的辅助，以保证数据的一致性。
- 调度和切换：线程上下文切换比进程上下文切换要快得多。
- 在多线程OS中，进程不是一个可执行的实体。

#### Linux的线程实现机制

Linux实现线程的机制非常独特。从内核的角度来说, 他并没有线程这个概念。Linux把所有的进程都当做进程来实现。内核中并没有准备特别的调度算法或者定义特别的数据结构来表示线程。相反, 线程仅仅被视为一个与其他进程共享某些资源的进程。每个线程都拥有唯一隶属于自己的task_struct, 所以在内核看来, 它看起来就像式一个普通的进程(只是线程和同组的其他进程共享某些资源)

在之前[Linux进程描述符task_struct结构体详解–Linux进程的管理与调度（一）](http://blog.csdn.net/gatieme/article/details/51383272)和[Linux进程ID号–Linux进程的管理与调度（三）](http://blog.csdn.net/gatieme/article/details/51383377)中讲解进程的pid号的时候我们就提到了, 进程task_struct中**pid存储的是内核对该进程的唯一标示**, 即对进程则标示进程号, 对线程来说就是其线程号, 那么**对于线程来说一个线程组所有线程与领头线程具有相同的进程号，存入tgid字段**

因此**getpid()返回当前进程的进程号，返回的应该是tgid值而不是pid的值, 对于用户空间来说同组的线程拥有相同进程号即tpid, 而对于内核来说, 某种成都上来说不存在线程的概念, 那么pid就是内核唯一区分每个进程的标示。**

>正是linux下组管理, 写时复制等这些巧妙的实现方式
>
>- linux下进程或者线程的创建开销很小
>- 既然不管是线程或者进程内核都是不加区分的，一组共享地址空间或者资源的线程可以组成一个线程组, 那么其他进程即使不共享资源也可以组成进程组, 甚至来说一组进程组也可以组成会话组, 进程组可以简化向所有组内进程发送信号的操作, 一组会话也更能适应多道程序环境

#### Linux内核线程

Linux内核可以看作一个服务进程(管理软硬件资源，响应用户进程的种种合理以及不合理的请求)。内核需要多个执行流并行，为了防止可能的阻塞，多线程化是必要的。

内核线程就是内核的分身，一个分身可以处理一件特定事情。Linux内核使用内核线程来将内核分成几个功能模块，像kswapd、kflushd等，这在处理异步事件如异步IO时特别有用。**内核线程的使用是廉价的，唯一使用的资源就是内核栈和上下文切换时保存寄存器的空间。**支持多线程的内核叫做多线程内核(Multi-Threads kernel )。内核线程的调度由内核负责，一个内核线程处于阻塞状态时不影响其他的内核线程，因为其是调度的基本单位。这与用户线程是不一样的。

- 处理器竞争：可以在全系统范围内竞争处理器资源；
- 使用资源：唯一使用的资源是内核栈和上下文切换时保持寄存器的空间
- 调度：调度的开销可能和进程自身差不多昂贵
- 同步效率：资源的同步和数据共享比整个进程的数据同步和共享要低一些。

#### 内核线程与普通进程的异同

1. 跟普通进程一样，内核线程也有优先级和被调度。
   当和用户进程拥有相同的`static_prio`时，内核线程有机会得到更多的cpu资源
2. 内核线程的bug直接影响内核，很容易搞死整个系统, 但是用户进程处在内核的管理下，其bug最严重的情况也只会把自己整崩溃
3. **内核线程没有自己的地址空间，所以它们的”current->mm”都是空的**；
4. 内核线程只能在内核空间操作，不能与用户空间交互；

内核线程不需要访问用户空间内存，这是再好不过了。所以内核线程的`task_struct`的mm域为空

但是刚才说过，内核线程还有核心堆栈，没有mm怎么访问它的核心堆栈呢？这个核心堆栈跟`task_struct`的`thread_info`共享8k的空间，所以不用mm描述。

但是内核线程总要访问内核空间的其他内核啊，没有mm域毕竟是不行的。
所以内核线程被调用时, 内核会将其`task_strcut`的`active_mm`指向前一个被调度出的进程的mm域, 在需要的时候，内核线程可以使用前一个进程的内存描述符。

<u>因为内核线程不访问用户空间，只操作内核空间内存，而所有进程的内核空间都是一样的。这样就省下了一个mm域的内存</u>。

> [内核线程的进程描述符task_struct中的mm和active_mm](https://blog.csdn.net/weixin_30561177/article/details/99628467)
>
> 对于普通用户进程来说，mm指向虚拟地址空间的用户空间部分，而对于内核线程，mm为NULL。
>
> 这位优化提供了一些余地, 可遵循所谓的惰性TLB处理(lazy TLB handing)。active_mm主要用于优化，由于内核线程不与任何特定的用户层进程相关，内核并不需要倒换虚拟地址空间的用户层部分，保留旧设置即可。由于内核线程之前可能是任何用户层进程在执行，故用户空间部分的内容本质上是随机的，内核线程决不能修改其内容，故将mm设置为NULL，同时如果切换出去的是用户进程，内核将原来进程的mm存放在新内核线程的active_mm中，因为某些时候内核必须知道用户空间当前包含了什么。

#### 不同操作系统的线程实现机制的区别

总而言之, Linux中线程与专门线程支持系统是完全不同的

Unix System V和Sun Solaris将用户线程称作为轻量级进程(LWP-Light-weight process), 相比较重量级进程, 线程被抽象成一种耗费较少资源, 运行迅速的执行单元。

而对于linux来说, 用户线程只是一种进程间共享资源的手段, 相比较其他系统的进程来说, linux系统的进程本身已经很轻量级了

举个例子来说, 假如我们有一个包括了四个线程的进程,

在提供专门线程支持的系统中, 通常会有一个包含只想四个不同线程的指针的进程描述符。该描述符复制描述像地址空间, 打开的文件这样的共享资源。线程本身再去描述它独占的资源。

相反, Linux仅仅创建了四个进程, 并分配四个普通的task_struct结构, 然后建立这四个进程时制定他们共享某些资源。

#### Linux内核线程的创建

在内核中，有两种方法可以生成内核线程，一种是使用`kernel_thread()`接口，另一种是用`kthread_create()`接口

##### kernel_thread

先说`kernel_thread`接口，使用该接口创建的线程，必须在该线程中调用`daemonize()`函数，这是因为只有当线程的父进程指向”`Kthreadd`”时，该线程才算是内核线程，而恰好`daemonize()`函数主要工作便是将该线程的父进程改成“`kthreadd`”内核线程；默认情况下，调用`deamonize()`后，会阻塞所有信号，如果想操作某个信号可以调用`allow_signal()`函数。

```c
int kernel_thread(int (*fn)(void *), void *arg, unsigned long flags); 
            // fn为线程函数，arg为线程函数参数，flags为标记
void daemonize(const char * name,...); // name为内核线程的名称
```

##### kthread_create

而`kthread_create`接口，则是标准的内核线程创建接口，只须调用该接口便可创建内核线程；默认创建的线程是存于不可运行的状态，所以需要在父进程中通过调用`wake_up_process()`函数来启动该线程。

```c
struct task_struct *kthread_create(int (*threadfn)(void *data),void *data,
                                  const char namefmt[], ...);
 //threadfn为线程函数;data为线程函数参数;namefmt为线程名称，可被格式化的, 类似printk一样传入某种格式的线程名
```

线程创建后，不会马上运行，而是需要将`kthread_create()` 返回的`task_struct`指针传给`wake_up_process()`，然后通过此函数运行线程。

##### kthread_run

当然，还有一个创建并启动线程的函数：`kthread_run`

```c
struct task_struct *kthread_run(int (*threadfn)(void *data),
                                    void *data,
                                    const char *namefmt, ...);
```

线程一旦启动起来后，会一直运行，除非该线程主动调用do_exit函数，或者其他的进程调用kthread_stop函数，结束线程的运行。

```c
int kthread_stop(struct task_struct *thread);

kthread_stop() 通过发送信号给线程。
如果线程函数正在处理一个非常重要的任务，它不会被中断的。当然如果线程函数永远不返回并且不检查信号，它将永远都不会停止。

​```c
int wake_up_process(struct task_struct *p); //唤醒线程
struct task_struct *kthread_run(int (*threadfn)(void *data),void *data,
                                const char namefmt[], ...);//是以上两个函数的功能的总和
```

> 因为线程也是进程，所以其结构体也是使用进程的结构体”`struct task_struct`”。
>
> **内核线程的退出**
>
> 当线程执行到函数末尾时会自动调用内核中`do_exit()`函数来退出或其他线程调用`kthread_stop()`来指定线程退出。

#### 总结

Linux使用`task_struct`来描述进程和线程

1. 一个进程由于其运行空间的不同, 从而有**内核线程**和**用户进程**的区分, 内核线程运行在内核空间, 之所以称之为线程是因为它没有虚拟地址空间, 只能访问内核的代码和数据, 而用户进程则运行在用户空间, 不能直接访问内核的数据但是可以通过中断, 系统调用等方式从用户态陷入内核态，但是内核态只是进程的一种状态, 与内核线程有本质区别
2. 用户进程运行在用户空间上, 而<u>一些通过共享资源实现的一组进程我们称之为线程组, Linux下内核其实本质上没有线程的概念, Linux下线程其实上是与其他进程共享某些资源的进程而已</u>。但是我们习惯上还是称他们为**线程**或者**轻量级进程**

因此, Linux上进程分3种，内核线程（或者叫核心进程）、用户进程、用户线程, 当然如果更严谨的，你也可以认为用户进程和用户线程都是用户进程。

**内核线程拥有 进程描述符、PID、进程正文段、核心堆栈**

**用户进程拥有 进程描述符、PID、进程正文段、核心堆栈 、用户空间的数据段和堆栈**

**用户线程拥有 进程描述符、PID、进程正文段、核心堆栈，同父进程共享用户空间的数据段和堆栈**

> 用户线程也可以通过exec函数族拥有自己的用户空间的数据段和堆栈，成为用户进程

----

> [内核线程的进程描述符task_struct中的mm和active_mm](https://www.cnblogs.com/linhaostudy/p/9904846.html) <= 下述内容出至该文章

`task_struct`进程描述符中包含两个跟进程地址空间相关的字段`mm`,` active_mm`，

```
struct task_struct
{
    // ...
    struct mm_struct *mm;
    struct mm_struct *avtive_mm;
    //...
};
```

大多数计算机上系统的全部<u>虚拟地址空间</u>分为两个部分: **供用户态程序访问的虚拟地址空间**和**供内核访问的内核空间**。每当内核执行上下文切换时, 虚拟地址空间的用户层部分都会切换, 以便当前运行的进程匹配, 而内核空间不会发生切换。

对于普通用户进程来说，`mm`指向虚拟地址空间的用户空间部分，而对于内核线程，`mm`为NULL。

这位优化提供了一些余地, 可遵循所谓的惰性TLB处理(lazy TLB handing)。`active_mm`主要用于优化，由于内核线程不与任何特定的用户层进程相关，内核并不需要倒换虚拟地址空间的用户层部分，保留旧设置即可。由于内核线程之前可能是任何用户层进程在执行，故用户空间部分的内容本质上是随机的，内核线程决不能修改其内容，故将`mm`设置为NULL，同时如果切换出去的是用户进程，内核将原来进程的`mm`存放在新内核线程的`active_mm`中，因为某些时候内核必须知道用户空间当前包含了什么。

> 为什么没有mm指针的进程称为惰性TLB进程?
>
> 假如内核线程之后运行的进程与之前是同一个, 在这种情况下, 内核并不需要修改用户空间地址表。地址转换后备缓冲器(即TLB)中的信息仍然有效。只有在内核线程之后, 执行的进程是与此前不同的用户层进程时, 才需要切换(并对应清除TLB数据)。

内核线程和普通的进程间的区别在于内核线程没有独立的地址空间，`mm`指针被设置为NULL；它只在内核空间运行，从来不切换到用户空间去；并且和普通进程一样，可以被调度，也可以被抢占。

----

> [Linux进程地址管理之mm_struct](https://blog.csdn.net/dyllove98/article/details/8917197) <= 下面内容来自该文，只摘取部分

Linux对于内存的管理涉及到非常多的方面，这篇文章首先从对进程**虚拟地址空间**的管理说起。(所依据的代码是2.6.32.60）

<u>无论是内核线程还是用户进程，对于内核来说，无非都是`task_struct`这个数据结构的一个实例而已</u>，`task_struct`被称为进程描述符（process descriptor),因为它记录了这个进程所有的context。其中有一个被称为'内存描述符‘（memory descriptor)的数据结构 `mm_struct`，抽象并描述了Linux视角下管理进程地址空间的所有信息。

`mm_struct`定义在`include/linux/mm_types.h`中，其中的域抽象了进程的地址空间，如下图所示：

![img](http://images.cnitblog.com/blog/516769/201304/13214813-ece9b1c2abbd4ea8b1de1dd266849b73.png)

每个进程都有自己独立的`mm_struct`，使得每个进程都有一个抽象的平坦的独立的32或64位地址空间<u>，各个进程都在各自的地址空间中相同的地址内存存放不同的数据而且互不干扰</u>。如果进程之间共享相同的地址空间，则被称为**线程**。

*（文章包含详细的代码分析，比较长，建议直接去原文查看）*

无论我们在调用`fork`,`vfork`,`clone`的时候最终会调用`do_fork`函数，区别在于`vfork`和`clone`会给`copy_mm`传入一个`CLONE_VM`的flag，这个标识表示父子进程都运行在同样一个‘虚拟地址空间’上面（在Linux称之为**lightweight process**或者**线程**），当然也就共享同样的物理地址空间（Page Frames)。

`copy_mm`函数中，如果创建线程中有`CLONE_VM`标识，则表示父子进程共享地址空间和同一个内存描述符，并且只需要将`mm_users`值+1，**也就是说mm_users表示正在引用该地址空间的thread数目，是一个thread level的counter。**

`mm_count`呢？`mm_count`的理解有点复杂。

对Linux来说，用户进程和内核线程（kernel thread)都是task_struct的实例，唯一的区别是kernel thread是没有进程地址空间的，内核线程也没有mm描述符的，所以内核线程的tsk->mm域是空（NULL）。**内核scheduler在进程context switching的时候，会根据tsk->mm判断即将调度的进程是用户进程还是内核线程**。但是虽然thread thread不用访问用户进程地址空间，但是仍然需要page table来访问kernel自己的空间。但是幸运的是，**对于任何用户进程来说，他们的内核空间都是100%相同的**，所以内核可以’borrow'上一个被调用的用户进程的`mm`中的页表来访问内核地址，这个`mm`就记录在`active_mm`。

简而言之就是，对于kernel thread,tsk->mm == NULL表示自己内核线程的身份，而tsk->active_mm是借用上一个用户进程的mm，用mm的page table来访问内核空间。对于用户进程，tsk->mm == tsk->active_mm。

为了支持这个特别，`mm_struct`里面引入了另外一个counter，`mm_count`。<u>刚才说过`mm_users`表示这个进程地址空间被多少线程共享或者引用，而`mm_count`则表示这个地址空间被内核线程引用的次数+1。</u>

比如一个进程A有3个线程，那么这个A的`mm_struct`的`mm_users`值为3，但是`mm_count`为1，所以`mm_count`是**process level**的counter。维护2个counter有何用处呢？考虑这样的scenario，内核调度完A以后，切换到内核内核线程B，B ’borrow' A的`mm`描述符以访问内核空间，这时mm_count变成了2，同时另外一个cpu core调度了A并且进程A exit，这个时候`mm_users`变为了0，`mm_count`变为了1，但是内核不会因为`mm_users==0`而销毁这个`mm_struct`，内核只会当`mm_count==0`的时候才会释放`mm_struct`，<u>因为这个时候既没有用户进程使用这个地址空间，也没有内核线程引用这个地址空间。</u>

page table的拷贝,page table负责logic address到physical address的转换。

**拷贝的结果就是父子进程有独立的page table，但是page table里面的每个entries值都是相同的，也就是说父子进程独立地址空间中相同logical address都对应于相同的physical address，这样也就是实现了父子进程的COW(copy on write)语义。**

**事实上，vfork和fork相比，最大的开销节省就是对page table的拷贝。**

**而在内核2.6中，由于page table的拷贝，fork在性能上是有所损耗的，所以内核社区里面讨论过shared page table的实现（**http://lwn.net/Articles/149888/）。

---

> [Linux 内核101：进程数据结构](https://zhuanlan.zhihu.com/p/63979739) <= 以下内容出至该文章

##### 基本概念清单

- Linux 里面，进程和线程到了内核，统一都叫做任务(Task)。
- 每个 task 都有一个数据接口 `task_struct`，用来保存task 状态。

##### 任务列表

Linux内核中有一个包含所有task的链表，把所有的`task_struct`连起来。

如图所示：



![img](https://pic4.zhimg.com/80/v2-55996c49556d91f589eeb97529fa52f1_720w.jpg)

##### task_struct

struct 定义：

```text
struct list_head        tasks;
```

看一下每个 `task_struct`包含了哪些重要的字段。

##### 任务 ID

和任务 ID 相关的字段有下面这些：

```text
pid_t pid;
pid_t tgid;
struct task_struct *group_leader; 
```

这三个字段的具体含义为：

- `pid` : 每个 task 都有一个 `pid`，是唯一的，不管是进程还是线程。
- `tgid`: 指向主线程的`pid`
- `group_leader`: 指向进程的主线程

任何一个进程，如果只有主线程，那`pid`是自己，`tgid`是自己，`group_leader`指向的还是自己。

但是，如果一个进程创建了其他线程，那就会有所变化了。线程有自己的`pid`，`tgid`就是进程的主线程的`pid`，`group_leader`指向的就是进程的主线程。

有了`tgid`之后，我们就可以判断一个task是线程还是进程了。

那么区分是进程还是线程有什么用呢？考虑下面几个场景：

- `ps`命令

`ps`默认展示的是所有进程的列表，而不是把所有的线程都列出来，那会显得很乱没有重点。

- 给线程发送 `kill-9`信号？

假如说我们给某个进程中的一个线程发送了退出信号(比如 `kill-9`)，那么我们不应该只退出这个线程，而是退出整个进程（至于为什么请看下文）。所以就需要某种方式，能够获取该线程所在进程中所有线程的 pid。

**从一个进程中杀死某一个线程是非常危险的操作。** <u>比如说某个 thread正在进行分配内存的工作，这时候它会hold内存分配器的lock。如果你把它强制杀死了，这个锁就永远不会释放，那么其他的 thread 也会停止。所以需要主进程的协助，来优雅地退出所有的线程</u>。



![img](https://pic1.zhimg.com/80/v2-0524aa29f1d1e3a2c0dfa3de6d3e9e12_720w.png)





![img](https://pic1.zhimg.com/80/v2-cd4fc2e061161ca6f348593fb3ed5e71_720w.png)



> 上图来源于 [这个so 上的问答](https://link.zhihu.com/?target=https%3A//unix.stackexchange.com/questions/403988/how-to-kill-an-individual-thread-under-a-process-in-linux)。

不信的话，我们可以来做一个实验。

下图显示的是 `htop`工具，**白色的表示进程**，**绿色的表示线程**。可以看到每个线程确实都有一个唯一的 PID。

![img](https://pic4.zhimg.com/80/v2-8fc867216f7b77c6f032e1dadd1d145a_720w.jpg)

现在让我们来给图中标记的PID 为 `21656`的 `code-server`线程发送 `kill-9`信号，然后发现，整个进程都退出了：

![img](https://pic1.zhimg.com/80/v2-eb7f711e34726f3af0a5aeab76767ce6_720w.png)



上图中， `code-server`这个 docker 容器进程在一分钟前退出了。

##### 信号处理

> 源代码地址：[https://github.com/torvalds/linux/blob/master/include/linux/sched.h#L863](https://link.zhihu.com/?target=https%3A//github.com/torvalds/linux/blob/master/include/linux/sched.h%23L863)

```c
/* Signal handlers: */
struct signal_struct        *signal;
struct sighand_struct        *sighand;
sigset_t            blocked;
sigset_t            real_blocked;
sigset_t            saved_sigmask;
struct sigpending        pending;
unsigned long            sas_ss_sp;
size_t                sas_ss_size;
unsigned int            sas_ss_flags;
```

- blocked : 被阻塞暂不处理
- pending : 等待处理
- sighand : 哪个信号正在被处理

注意这里的 `struct signal_struct *signal;`指向了一个 `signal` struct。这个struct 中还有一个 `struct sigpending pending;`。前面提到过需要区分线程和进程，这里也可以看出一点端倪。第一个是线程组共享的，一个是本任务的。

##### 任务状态

一个 task 的任务状态一共可以取下面的这些值：

```c
/* Used in tsk->state: */
#define TASK_RUNNING                    0
#define TASK_INTERRUPTIBLE              1
#define TASK_UNINTERRUPTIBLE            2
#define __TASK_STOPPED                  4
#define __TASK_TRACED                   8
/* Used in tsk->exit_state: */
#define EXIT_DEAD                       16
#define EXIT_ZOMBIE                     32
#define EXIT_TRACE                      (EXIT_ZOMBIE | EXIT_DEAD)
/* Used in tsk->state again: */
#define TASK_DEAD                       64
#define TASK_WAKEKILL                   128
#define TASK_WAKING                     256
#define TASK_PARKED                     512
#define TASK_NOLOAD                     1024
#define TASK_NEW                        2048
#define TASK_STATE_MAX                  4096
```

![img](https://pic2.zhimg.com/80/v2-61ccf3113f2fa36c42f59ba9e2bfddaf_720w.jpg)

##### 总结一下

![img](https://picb.zhimg.com/80/v2-e6fe59f772ea7cb5eeeee751b03b65de_720w.jpg)

----

> [linux下进程和线程](https://blog.csdn.net/ptgood/article/details/107694118) <= 推荐，算一个概要，有个大致理解
>
> [Linux线程的实现 & LinuxThread vs. NPTL & 用户级内核级线程 & 线程与信号处理](https://www.cnblogs.com/charlesblc/p/6242518.html) <= 推荐阅读
> [Java线程与Linux内核线程的映射关系](https://blog.csdn.net/u011955252/article/details/53350265) <= 建议阅读
>
> [POSIX多线程——基本线程管理函数介绍](https://blog.51cto.com/keren/176759) <= 建议阅读
>
> **如果线程有50%的时间被阻塞，线程的数量就应该是内核数量的2倍。**如果更少的比例被阻塞，那么它们就是计算密集型的，则需要开辟较少的线程。如果有更多的时间被阻塞，那么就是IO密集型的程序，则可以开辟更多的线程。于是我们可以得到下面的线程数量计算公式：
>
> `线程数量=内核数量 / （1 - 阻塞率）`
>
> [为什么说linux是轻量级进程？](https://www.zhihu.com/question/55920420)
>
> 因为linux并没有为线程准备特定的数据结构。在内核看来，只有进程而没有线程，在调度时也是当做进程来调度。linux所谓的线程其实是**与其他进程共享资源**的进程。
> 为什么说是轻量级？在于**它只有一个最小的执行上下文和调度程序所需的统计信息**。他是进程的执行部分，只带有执行相关的信息。
>
> [linux ps命令介绍](https://www.cnblogs.com/allen8807/archive/2010/11/10/1873843.html) <= 进程相关的一些属性（其中，PID进程号，PPID父进程号；RSS-常驻内存集，内存分配到的内存大小，可以理解为分配到的物理帧总大小；VSZ进程分配到的虚拟内存，可以理解为分配到的页总大小）
>
> [linux 交换分区是什么？](https://zhidao.baidu.com/question/38814789.html)
>
> Linux下可以创建两种类型的交换空间，一种是swap分区，一种是swap文件。前者适合有空闲的分区可以使用，后者适合于没有空的硬盘分区，硬盘的空间都已经分配完毕。
>
> [Linux交换空间（swap space）](https://segmentfault.com/a/1190000008125116)
>
> Linux下有两种类型的swap空间，swap分区和swap文件，他们有各自的特点：
>
> - **swap分区上面由于没有文件系统，所以相当于内核直接访问连续的磁盘空间，效率相对要高点**，但由于swap分区一般安装系统时就分配好了了，后期要缩减空间和扩容都很不方便。
> - swap文件放在指定分区的文件系统里面，所以有可能受文件系统性能的影响，但据说2.6版本以后的内核可以直接访问swap文件对应的物理磁盘地址，相当于跳过了文件系统直接访问磁盘，不过如果swap文件在磁盘上的物理位置不连续时，还是会对性能产生不利影响，但其优点就是灵活，随时可以增加和移除swap文件。
>
> [Linux 内存管理中的 RSS 和 VSZ 是什么意思？](https://www.jianshu.com/p/9bf36aa82f90)
>
> RSS 是常驻内存集（Resident Set Size），表示该进程分配的内存大小，不包括进入交换分区的内存，包括共享库占用的内存（只要共享库在内存中），包括所有分配的栈内存和堆内存。
>
> VSZ 表示进程分配的虚拟内存，包括进程可以访问的所有内存，包括进入交换分区的内容，以及共享库占用的内存。
>
> 如果一个进程，程序的大小有 500K，链接的共享库大小有 2500K，堆栈内存共有 200K，其中 100K 进入了交换分区（也就是前面学过的内存不足时，用到的虚拟技术，把部分用不着时可替换出去的内存放到交换分区，即硬盘中。通常在内存不够用时，才会让某些进程的部分内容置换到交换分区中）。
>
> 进程实际加载了共享库中的 1000K 的内容，以及自己程序的中的 400K 的内容。请问 RSS 和 VSZ 应是多少？
>
> RSS: 400K + 1000K + 100K = 1500K
> VSZ: 500K + 2500K + 200K = 3200K
>
> RSS 中有一部分来自共享库，而共享库可能被许多进程使用，所以如果把所有进程的 RSS 加起来，可能比系统内存还要大。
>
> 有一个较新的参数 PSS  (proportional set size)，它对于共享内存的计算与 RSS 不同。参考前面的例子，如果有两个进程使用同一个共享库，那么：
>
> PSS: 400K + (1000K/2) + 100K = 400K + 500K + 100K = 1000K
>
> **线程共享同一个地址空间，所以一个进程内部的所有线程有相同的RSS，VSZ和 PSS**。可使用`ps`或者`top`命令观察这些信息。

## 7.3 进程上下文切换(context switch）

### 7.3.1 进程上下文切换概述

停止当前进程（从运行状态改变成其他状态）并调度其他进程（转变成运行状态）。 

- 必须在切换前存储许多部分的进程上下文
- 必须能够在之后恢复它们，所以进程不能显示它曾经被暂停过
- 必须快速（上下文切换非常频繁）

需要存储那些上下文？

- 寄存器（PC,SP,…）,CPU状态...
- 一些时候可能会费时，所以我们应该尽可能避免

*进程执行中要关注寄存器状态，如PC（程序计数器，进程执行到了什么地方），栈指针（调用关系，相应的局部变量位置）等。 上下文切换时，这些信息要被保存到PCB中，运行另一个进程时需要把这个进程的PCB中的上下文恢复到寄存器中去，使接下来进程可以继续在CPU上执行。*

![img](https://img-blog.csdn.net/20180419205638787?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0FsYXRlYmxvb21lcg==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)

保存上下文信息，上下文信息中有不少和硬件相关，所以这部分代码都是汇编语言实现的。

- 操作系统为活跃进程准备了进程控制块（PCB）

- 操作系统将进程控制块（PCB）放置在一个合适的队列里

  + 就绪队列

  + 等待I/O队列（每个设备的队列）

  +  僵尸队列

  *<small>操作系统会根据各个进程所处的不同状态（就绪、运行、阻塞、挂起、结束等），把不同的进程PCB放到不同的队列中管理。</small>*

> [七、进程和线程](https://blog.csdn.net/Alatebloomer/article/details/79993225) <= 同样图片都来自这，这次有些内容也是转自这，因为本来课程内容就没什么区别。

## 7.4 创建进程、加载程序、进程等待和终止

### 7.4.1 创建进程fork

创建进程：

- windows进程创建API：CreateProcess（filename）
- Unix进程创建系统调用：fork/exec

1. fork()把一个进程复制成二个进程，parent（old PID），child(new PID)
2. exec()用新程序来重写当前进程，PID没有改变

![img](https://img-blog.csdn.net/20180419214511927?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0FsYXRlYmxvb21lcg==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)

fork() 创建一个继承的子进程

- 复制父进程的所有变量和内存
- 复制父进程的所有CPU寄存器(有一个寄存器childpid例外,下面讲到)

fork()的返回值

- 子进程的fork()返回0
- 父进程的fork()返回子进程标识符
- 如果出现错误，fork返回一个负值；
- fork() 返回值可方便后续使用，子进程可使用getpid()获取PID（Linux中系统调用的getpid其实获得的是tgid，也就是线程组id，等同于线程组所属的进程的pid，而gettid()才是返回每个线程各自的pid。Linux的线程本质还是进程。）

fork()的地址空间复制

- fork()执行过程对于子进程而言，是在调用时对父进程地址空间的一次复制
- 对于父进程fork()返回child PID, 对于子进程childPID返回值为0

![img](https://img-blog.csdn.net/20180420110923441?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0FsYXRlYmxvb21lcg==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)

fork()的使用示例

![img](https://img-blog.csdn.net/2018042011163143?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0FsYXRlYmxvb21lcg==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)

![img](https://img-blog.csdn.net/20180420111644199?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0FsYXRlYmxvb21lcg==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)

注意fork()的执行顺序

> [七、进程和线程 ](https://blog.csdn.net/Alatebloomer/article/details/79993225) <=这个部分，该文章比我自己OneNote详细不少，大部分内容摘自该文章

### 7.4.2 进程中加载程序exec

**exec：系统调用exec( )加载新程序取代当前运行进程**

在调用执行fork之后父进程与子进程一样（只有pid不一样），在系统调用exec()之后，会加载新程序calc，PCB信息会发生变化，用户态内存空间变化（代码变化）。

![img](https://img-blog.csdn.net/2018042011413056?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0FsYXRlYmxvb21lcg==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)

​	这里`fork()`创建子进程可能失败，所以需要用`else`语句来处理创建子进程失败的情况<small>（执行创建子进程操作的该父进程中，如果创建失败，`fork()`返回负数）</small>。

​	当父进程`pid=fork()`的返回值为正数，说明子进程创建成功。而`child_status=wait(pid)`，即该父进程等待子进程运行结束后，父进程才继续执行。

​	`if(pid==0)`包含的`exec(...)`语句，由于子进程从`int pid = fork`这句继续往下执行，而子进程获取的`fork()`返回值为0，所以子进程会进入`if`语句。子进程执行`exec("calc",...)`，这会加载指定的程序`calc`到当前子进程的虚拟空间中，取代当前进程，只保留PID不变。<small>（执行exec，新程序代替原本进程的程序，代码段、堆栈等也都变了，不再是原本进程的那些资源。位于不变的就是PID进程号了。）</small>

![img](https://img-blog.csdn.net/20180420115241651?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0FsYXRlYmxvb21lcg==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)

上图，为执行`fork()`后，创建出新的子进程，系统内核态也创建对应的PCB。父进程`pid=fork()`返回值为128，而子进程获取的`fork()`返回值为0。父进程接下去准备执行`child_status=wait(pid)`，而子进程接下去准备执行`exec_status=exec("calc",...)`。

执行`fork()`后，内存中子进程的数据段等，一开始还是使用和父进程相同的虚拟空间。只有当`fork()`出来的子进程或原本父进程对自己的虚拟空间进行写操作后，才会触发异常，然后操作系统真正给子进程分配虚拟空间，并复制原本的数据段、代码段、堆、程序计数器、栈。

![img](https://img-blog.csdn.net/2018042011533285?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0FsYXRlYmxvb21lcg==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)

​	子进程执行`exec_status=exec("calc",...)`后，可以看到子进程PCB的`open_files`发生改变，栈、堆、代码段、数据段也相继发生改变，也就是操作系统真正给该子进程分配了新的独立虚拟空间。

---

+ `exec()`允许一个进程"加载"一个不同段程序并在main开始执行（事实上 _start）

- 它允许一个进程指定参数的数量（argc）和字符串参数数组（argv）
- 如果调用成功，它是相同进程，但是运行了一个不同的程序
- 代码、stack（栈），heap（堆）要重写

`fork()`的简单实现： 

- 对子进程分配内存
- 复制父进程的内存和CPU寄存器到子进程中
- **开销昂贵**

*(Linux等操作系统基本都采用COW写时复制技术优化`fork()`。)*

99%的情况下，我们调用`fork(）`之后调用`exec()`

- 在`fork()`操作中内存复制是没有作用的
- 子进程将可能关闭打开的文件和连接
- 开销大
- 为什么不能结合它们在一个调用中（OS/2，windows）？

*（早期UNIX的`fork()`简单粗暴，子进程复制父进程的代码段、调用栈、数据段、堆等信息，之后执行`exex()`又替换掉原本复制过来的代码段、调用栈、数据段等。很显然，早期的`fork()`直接复制父进程资源，大多数情况下是白白浪费时间和浪费内存空间。因为一般执行`fork()`后又会执行`exec()`）*

`vfork()`

- 一个创建进程的系统调用，不用创建一个同样的内存映像
- 一些时候称为轻量级`fork()`
- 子进程应该几乎立即调用`exec()`
- 现在不再使用，如果操作系统应对`fork()`时采用了copy on write（COW） 技术。（通过OS的虚存管理，只复制了父进程meta元数据即页表，指向的是同一地址空间，当父进程or子进程对这一地址空间中某个地址单元进行写操作时会触发异常，这时候操作系统才会给子进程分配真正的虚拟空间和执行父进程资源的复制操作。）

> [七、进程和线程 ](https://blog.csdn.net/Alatebloomer/article/details/79993225) <=部分文字摘自该文，主要还是用该文章里面的图片，因为我没图床，额

### 7.4.3 进程等待和终止

父进程使用`wait(子进程pid)`来等待子进程终止。

- 一个子进程向父进程返回一个值，所以父进程必须接受这个值并处理。 

- `wait()`系统调用担任这个要求

  + 它使父进程睡眠来等待子进程的结果

  + 当一个子进程调用`exit()`时，操作系统解锁父进程，并且将通过`exit()`传递得到的返回值作为`wait()`调用的一个结果（连同子进程的pid一起）。如果这里没有子进程存活，`wait()`立刻返回

  + 当然，如果这里有子进程的僵死等待，`wait()`立即返回其中一个值（并且解除僵死状态）

- 进程结束执行之后，它调用`exit()`

- `exit()`系统调用：

  + 将这个程序的"结果"作为一个参数
  + 关闭所有打开的文件、连接等等
  + 释放内存
  + 释放大部分支持进程的操作系统结构
  + 检查是否父进程是存活的：
    + 如果父进程存活，它保留结果的值直到父进程需要它；在这种情况下，进程没有真实死亡，而是进入了僵死（zombie/defunct）状态。
    + 如果父进程已死，它释放所有的数据结构，该进程死亡
  + 清理所有等待的僵死进程

- 进程终止是最终的垃圾收集（资源回收）

*(INIT进程会定期扫描进程控制块，kill僵尸进程。僵尸进程的PPID都是1，因为僵尸进程已经与父进程分离或者父进程已死，但是操作系统不允许用户进程无父进程，所以将这些无父进程的进程都设置PPID为1。INIT进程是操作系统初始化后创建的第一个用户进程，PID=1。)*

*(为啥没事父进程要调用`wait()`等待子进程执行结束？原因之一，子进程`exit()`退出后，确实打开的文件等用户态资源基本都能被释放。但是子进程的内核态资源`PCB`，在Linux中具体的数据结构对应的是`task_struct`，`PCB`并不是子进程结束后就能立马释放的。比如子进程·`task_struct`中`mm_struct`类型的`mm`，其主要表示用户进程占用的虚拟空间资源信息，只有所有该进程的线程没有再使用`mm`表示的地址空间，且没有内核进程/线程引用该`mm`表示的地址空间时，`mm_struct`才能被释放。如果父进程`wait()`子进程结束后，就可以通过其他系统调用方法，请求操作系统尽快释放子进程的`PCB`资源，这是子进程自身做不到的，需要父进程这个还未结束的进程替它擦屁股。）*

![img](https://img-blog.csdn.net/20180420165219366?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0FsYXRlYmxvb21lcg==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)

​	上图想表述的是，子进程被`fork()`创建出来后，执行`exec()`到最后结束，子进程可能经历多个状态。

​	首先初始化子进程完毕，等待被执行（New→Ready）；如果子进程被CPU调度，准备执行`exec()`（Ready→Running）；如果还没执行就用完时间了（Running→Ready）；子进程执行`exec()`时，需要内核态与用户态之间的转换（Running→Blocked）；内核态完成`exec()`系统调用，并返回结果给子进程（Blocked→Ready）；子进程执行完，最后退出，隐式调用`exit()`，如果父进程执行`wait()`要求返回值，那么子进程等待父进程获取到返回值之前，成为僵尸进程，**如果该子进程其实是个非主线程的线程，那么直接释放资源**（Running→Zombie）；父进程`wait()`获取到子进程`exit()`的返回值，子进程真正结束，释放资源。(Zombie→Exit)。

> [七、进程和线程](https://blog.csdn.net/Alatebloomer/article/details/79993225) <= 依旧是图床担当。

### 7.3.5 Linux僵尸进程(优质网文转载)

#### 根据以下推荐文章-大致总结

僵尸进程：

​	执行`exit()`后，进程进入zombie状态。用户态资源，在`exit()`后已经释放，但是内存态PCB等内核态资源，该进程本身无法释放。由该进程的父进程执行`wait()`或`waitpid()`后，内部包含的系统调用等逻辑将在内核态下运行，真正释放子进程PCB等内核资源。

​	某进程创建多个线程，如果主线程执行了`pthread_exit()`，但仍存在其他线程未结束运行，那么该主线程所在的进程仍zombie。因为主线程所在的PCB中的共享资源部分，进程下创建的其余线程也可访问，所以主线程结束，其PCB也不可能释放。只有当所有线程执行结束，主线程的PCB才能得到释放。

（注意，子线程结束，子线程自己的PCB直接释放，因为其共享资源相关的访问指针指向主线程的PCB共享资源。换言之，子线程自己的资源其它线程不需要，而共享资源不归自己管，所以子线程`pthread_exit()`能够直接释放自己的PCB，但主线程必须等待所有子线程结束才能释放自己的PCB）

**如果不需要获取子进程状态，那避免子进程zombie的方法，最好就是`signal(SIGCHLD,SIG_IGN);`**。**Linux下，显式声明`SIG_IGN`之后，子进程exit不会进入zombie状态，直接释放子进程资源**（亲测如此，fork出2500子进程exit后直接消失，没有zombie状态的转变。）。

如果父进程用wait，还需要父进程参与处理子进程exit后的资源释放，在无需获取子进程状态值的情况下，多此一举=>亲测如果fork上千子进程，父进程wait释放资源挺慢的。（我Linux单核CPU，2GB内存。测试样例里父进程死循环调用wait，fork出2500个子进程，看到几秒消失5-10个zombie进程）。

如果需要获取子进程状态，可以自定义handler，再调用`signal(SIGCHLD,自定义handler)`=>需要在自定义handler中再调用wait释放子进程资源（handler中不调用wait，会导致子进程仍zombie，亲测）。

---



> [父进程是init一定不会变僵尸进程吗？](http://blog.chinaunix.net/uid-28541347-id-5750193.html) <= 以下内容出自该文章

​	工作中有次操作线上环境时发现有一个“Z”（僵尸）进程，当时首先想到的方法就是把这个进程的父进程干掉。但是`ps`下发现该进程的父进程已经是`init`了。这个情况让我很迷惑，因为通常来说一旦`init`进程接管了Z状态的进程就会调用`wait`将其回收。而且这是避免僵尸进程的一种重要手段，怎么会出现这种情况呢？

​	后来仔细回想了下进程终止时的过程，忽然找到问题所在了。我们知道**任何进程结束最终都会调用内核函数`do_exit`**。而这个函数有两个重要的事要完成：

（1）**（作为父进程）**给自己的子进程（如果存在）找一个新的父进程；

（2）**（作为子进程）**通知自己的父进程为自己“收尸”；

​	第一个事情比较简单，如果要退出的进程时多线程进程，则可以将子进程托付给自己的兄弟线程，如果没有这样的线程，则托付给`init`进程；

​	第二件事情对于单线程的进程比较简单，但多线程的进程就会复杂些。因为**只有线程组的主线程才有资格通知父进程，线程组的其他线程终止时，并不通知父进程，也没必要保留资源进入僵尸状态，直接调用`release_task`函数释放所有的资源就好**。

​	由于父进程只认子进程的主线程，所以在线程组中，如果主线程终止了，但是线程组还有其他线程，那么就不会通知父进程来收尸，直到线程组中最后一个线程退出时。换句话说：在用户层面，可以调用`pthread_exit`让主线程先"死"，但在内核态中，主线程的`task_struct`一定要保住，哪怕变为僵尸。

所以**当主线程退出但是其他线程还在运行，这时主线程就会变为Z状态，纵使`init`接管了主线程**。

**测试代码**：

```c
#include <stdio.h>
#include <unistd.h>
#include <stdlib.h>
#include <pthread.h>
void thread_fun(void)
{
    printf("begin thread...\n");
    sleep(100);
    printf("end thread...\n");
}
int main(int argc, char *argv[])
{
    pthread_t tid;
    int ret;
    daemon(0, 0);//调用deamon是为了让init接管主线程
    printf("process begin...\n");
    ret=pthread_create(&tid,NULL,(void *)thread_fun,NULL);
    if(ret !=0 )
    {
        printf("pthread error\n");
        exit(1);
    }
    printf("main thread is end...\n");
    pthread_exit(0);
    printf("never exce\n"); 
}
```

我们创建线程执行sleep，然后主线程退出，当我们使用ps查看主线程状态时结果如下：

`18099   1 Zsl  a.out 0.0  0.0`

可以看到，虽然`init`接管了主线程，但是主线程依然是Z状态。要想让这个Z进程消失，有两个办法，一个是杀死整个进程，也就是kill 18099，在一个就是`ps -eLf`查找其他线程，然后kill掉。

----

> [linux 如何清理僵尸进程](https://www.cnblogs.com/yuxc/archive/2012/11/04/2753391.html) <===以下内容出至该博客文章

今天在维护服务器的时候，发现有5个nova-novncproxy的僵尸进程。

```shell
26327 ?        S      0:05  \_ /usr/bin/python /usr/bin/nova-novncproxy --config-file=/etc/nova/nova.conf
 4765 ?        Z      0:00      \_ [nova-novncproxy] <defunct>
 4766 ?        Z      0:00      \_ [nova-novncproxy] <defunct>
 4767 ?        Z      0:00      \_ [nova-novncproxy] <defunct>
 4768 ?        Z      0:00      \_ [nova-novncproxy] <defunct>
 4769 ?        Z      0:00      \_ [nova-novncproxy] <defunct>
```

之前对于僵尸进程的了解并不深，赶紧找了[篇相关文章](http://blog.51osos.com/linux/linux-how-to-kill-zombie-process/)来学习一下，该如何处理。

#### 定义

In UNIX System terminology, a process that has terminated,but whose parent has not yet waited for it, is called a zombie.

   **在UNIX 系统中,一个进程结束了,但是他的父进程没有等待(调用wait / waitpid)他, 那么他将变成一个僵尸进程. 在fork()/execve()过程中，假设子进程结束时父进程仍存在，而父进程fork()之前既没安装SIGCHLD信号处理函数调用waitpid()等待子进程结束，又没有显式忽略该信号，则子进程成为僵尸进程。**

如何查看linux系统上的僵尸进程，如何统计有多少僵尸进程？

`#ps -ef | grep defunct`

或者查找状态为Z的进程，Z就是代表zombie process,僵尸进程的意思。

另外使用top命令查看时有一栏为S,如果状态为Z说明它就是僵尸进程。

Tasks: 95 total,  1 running, 94 sleeping,  0 stopped,  **0 zombie**

top命令中也统计了僵尸进程。或者使用下面的命令：

`ps -ef | grep defunct | grep -v grep | wc -l`

#### 如何杀死僵尸进程呢？

一般僵尸进程很难直接kill掉，不过您可以kill僵尸爸爸。父进程死后，僵尸进程成为”孤儿进程”，过继给1号进程`init`，`init`始终会负责清理僵尸进程．它产生的所有僵尸进程也跟着消失。

`ps -e -o ppid,stat | grep Z | cut -d” ” -f2 | xargs kill -9`

或

kill -HUP \`ps -A -ostat,ppid | grep -e ’^[Zz]‘ | awk ’{print $2}’\`

当然您可以自己编写更好的shell脚本，欢迎与大家分享。

我将nova-novncproxy stop后再start，僵尸进程即消失，问题解决。

另外子进程死后，会发送`SIGCHLD`信号给父进程，父进程收到此信号后，执行`waitpid()`函数为子进程收尸。就是基于这样的原理：就算父进程没有调用wait，内核也会向它发送`SIGCHLD`消息，而此时，尽管对它的默认处理是忽略，如果想响应这个消息，可以设置一个处理函数。

#### 如何避免僵尸进程呢？

处理`SIGCHLD信号`并不是必须的。但对于某些进程，特别是服务器进程往往在请求到来时生成子进程处理请求。如果父进程不等待子进程结束，子进程将成为僵尸进程（zombie）从而占用系统资源。如果父进程等待子进程结束，将增加父进程的负担，影响服务器进程的并发性能。在Linux下 可以简单地将`SIGCHLD`信号的操作设为`SIG_IGN`。
`signal(SIGCHLD,SIG_IGN);`
这样，**内核在子进程结束时不会产生僵尸进程**（Linux亲测有效）。这一点与BSD4不同，**BSD4下必须显式等待子进程结束才能释放僵尸进程**

或者

用两次`fork()`，而且使紧跟的子进程直接退出，使得孙子进程成为孤儿进程，从而`init`进程将负责清除这个孤儿进程。

---

> [Zombie Processes and their Prevention](https://www.geeksforgeeks.org/zombie-processes-prevention/) <== 以下内容出至该文章

#### **Zombie state**

When a process is created in UNIX using  fork() system call, the address space of the Parent process is  replicated. If the parent process calls wait() system call, then the  execution of parent is suspended until the child is terminated. At the  termination of the child, a ‘SIGCHLD’ signal is generated which is  delivered to the parent by the kernel. Parent, on receipt of ‘SIGCHLD’  reaps the status of the child from the process table. Even though, **the  child is terminated, there is an entry in the process table  corresponding to the child where the status is stored.** When parent  collects the status, this entry is deleted. Thus, all the traces of the  child process are removed from the system. If the parent decides not to  wait for the child’s termination and it executes its subsequent task,  then at the termination of the child, the exit status is not read.  Hence, there remains an entry in the process table even after the  termination of the child. This state of the child process is known as  the Zombie state. 



#### **Why do we need to prevent the creation of Zombie process?**
 There is one process table per system. The size of the process table is  finite. If too many zombie processes are generated, then the process  table will be full. That is, the system will not be able to generate any new process, then the system will come to a standstill. Hence, we need  to prevent the creation of zombie processes.



#### Different ways in which the creation of Zombie can be Prevented

1. **Using wait() system call :** When the parent process  calls wait(), after the creation of a child, it indicates that, it will  wait for the child to complete and it will reap the exit status of the  child. The parent process is suspended(waits in a waiting queue) until  the child is terminated. It must be understood that during this period,  the parent process does nothing just waits.

2. **By ignoring the SIGCHLD signal :** When a child is  terminated, a corresponding SIGCHLD signal is delivered to the parent,  if we call the `signal(SIGCHLD,SIG_IGN)`, then the SIGCHLD signal is  ignored by the system, and the child process entry is deleted from the  process table. **Thus, no zombie is created**. However, in this case, the  parent cannot know about the exit status of the child.

3. **By using a signal handler :** The parent process installs a signal handler for the SIGCHLD signal. The signal handler calls  wait() system call within it. In this senario, when the child  terminated, the SIGCHLD is delivered to the parent. On receipt of  SIGCHLD, the corresponding handler is activated, which in turn calls the wait() system call. Hence, the parent collects the exit status almost  immediately and the child entry in the process table is cleared. Thus no zombie is created.

   ```c
   // A C program to demonstrate handling of 
   // SIGCHLD signal to prevent Zombie processes. 
   #include<stdio.h> 
   #include<unistd.h> 
   #include<sys/wait.h> 
   #include<sys/types.h> 
   
   void func(int signum) 
   { 
   	wait(NULL); 
   } 
   
   int main() 
   { 
   	int i; 
   	int pid = fork(); 
   	if (pid == 0) 
   		for (i=0; i<20; i++) 
   			printf("I am Child\n"); 
   	else
   	{ 
   		signal(SIGCHLD, func); 
   		printf("I am Parent\n"); 
   		while(1); 
   	} 
   } 
   ```

---

#### linux下的僵尸进程处理SIGCHLD信号

>[linux下的僵尸进程处理SIGCHLD信号](https://www.cnblogs.com/wuchanming/p/4020463.html)	<=	以下内容出自该文章，下面只截取代码的部分，其他的概念相关的，建议直接阅读原文

1. 如以下代码会创建100个子进程，但是父进程并未等待它们结束，所以在父进程退出前会有100个僵尸进程。

   ```c
   #include <stdio.h>  
   #include <unistd.h>  
   
   int main() {  
   
     int i;  
     pid_t pid;  
   
     for(i=0; i<100; i++) {  
       pid = fork();  
       if(pid == 0)  
         break;  
     }  
   
     if(pid>0) {  
       printf("press Enter to exit...");  
       getchar();  
     }  
   
     return 0;  
   }
   ```

   ​	其中一个解决方法即是编写一个SIGCHLD信号处理程序来调用wait/waitpid来等待子进程返回。

   ​	但是通过运行程序发现还是会有僵尸进程，而且每次僵尸进程的数量都不定。这是为什么呢？其实主要是因为**Linux的信号机制是不排队的**，假如在某一时间段多个子进程退出后都会发出SIGCHLD信号，但父进程来不及一个一个地响应，所以最后父进程实际上只执行了一次信号处理函数。但执行一次信号处理函数只等待一个子进程退出，所以最后会有一些子进程依然是僵尸进程。

   ​	虽然这样但是有一点是明了的，就是收到SIGCHLD必然有子进程退出，而我们可以在信号处理函数里循环调用waitpid函数来等待所有的退出的子进程。至于为什么不用wait，主要原因是在wait在清理完所有僵尸进程后再次等待会阻塞。

   > 我自己改成1000个fork，在单核2G服务器上试了下，最后剩下77个zombie进程。

2. 最佳方案如下：

   ```c
   #include <stdio.h>  
   #include <unistd.h>  
   #include <signal.h>  
   #include <errno.h>  
   #include <sys/types.h>  
   #include <sys/wait.h>  
   
   void wait4children(int signo) {  
     int status;  
     while(waitpid(-1, &status, WNOHANG) > 0);  
   }  
   
   int main() {  
   
     int i;  
     pid_t pid;  
   
     signal(SIGCHLD, wait4children);  
   
     for(i=0; i<100; i++) {  
       pid = fork();  
       if(pid == 0)  
         break;  
     }  
   
     if(pid>0) {  
       printf("press Enter to exit...");  
       getchar();  
     }  
   
     return 0;  
   }
   ```

   ​	这里使用waitpid而不是使用wait的原因在于：我们在一个循环内调用waitpid，以获取所有已终止子进程的状态。我们必须指定WNOHANG选项，它告诉waitpid在有尚未终止的子进程在运行时不要阻塞。我们不能在循环内调用wait，因为没有办法防止wait在正运行的子进程尚有未终止时阻塞。

> 以下文章和上述的类似，但是也都是精品文，推荐阅读
>
> [linux下的僵尸进程处理SIGCHLD信号](https://www.cnblogs.com/wuchanming/p/4020463.html)	<= 	精品文章，推荐阅读
>
> [宋宝华: 僵尸进程的成因以及僵尸可以被“杀死”吗？](https://cloud.tencent.com/developer/article/1536277) <== 具体的代码+调试过程，截图。
>
> [Linux 僵尸进程](https://www.cnblogs.com/sparkdev/p/8275221.html) 
>
> Linux 允许进程查询内核以获得其父进程的 PID，或者其任何子进程的执行状态。例如，进程可以创建一个子进程来执行特定的任务，然后调用诸如 wait() 这样的一些库函数检查子进程是否终止。如果子进程已经终止，那么，它的终止代号将告诉父进程这个任务是否已成功地完成。
> 为了遵循这些设计原则，不允许 Linux 内核在进程一终止后就丢弃包含在进程描述符字段中的数据。只有父进程发出了与被终止的进程相关的 wait() 类系统调用之后，才允许这样做。这就是引入僵死状态的原因：尽管从技术上来说进程已死，但必须保存它的描述符，直到父进程得到通知。**如果一个进程已经终止，但是它的父进程尚未调用 wait() 或 waitpid() 对它进行清理，这时的进程状态称为僵死状态，处于僵死状态的进程称为僵尸进程(zombie process)。**任何进程在刚终止时都是僵尸进程，正常情况下，僵尸进程都立刻被父进程清理了。
>
> [linux僵死进程的产生与避免](http://blog.chinaunix.net/uid-23089249-id-210808.html) <== 精简，且介绍4种常见防止长时间zombie进程的方式，有代码。
>
> [Do I need to do anything with a SIGCHLD handler if I am just using wait() to wait for 1 child to finish at a time?](https://stackoverflow.com/questions/18437779/do-i-need-to-do-anything-with-a-sigchld-handler-if-i-am-just-using-wait-to-wai)
>
> [POSIX](http://pubs.opengroup.org/onlinepubs/9699919799/functions/V2_chap02.html#tag_15_04) says about `SIG_IGN` (strictly under the XSI extension note):
>
> > If the action for the SIGCHLD signal is set to SIG_IGN, child  processes of the calling processes shall not be transformed into zombie  processes when they terminate. If the calling process subsequently waits for its children, and the process has no unwaited-for children that  were transformed into zombie processes, it shall block until all of its  children terminate, and wait(), waitid(), and waitpid() shall fail and  set errno to [ECHILD].
>
> And in the description of [`<signal.h>`](http://pubs.opengroup.org/onlinepubs/9699919799/basedefs/signal.h.html) says that the default signal disposition for `SIGCHLD` is 'ignore' (code 'I').  However, it is the `SIG_DFL` behaviour is to 'ignore' the signal; the signal is never generated.  This is different from the `SIG_IGN` behaviour.
>
> [Be careful to pthread_exit() in main()](https://www.bo-yang.net/2014/11/20/pthread_exit-in-main) <== 外国人博客文章，需翻墙。带图和分析。
>
> However, you must be carefull when using `pthread_exit()` in the main thread. **Because after calling `pthread_exit()` and before the process really terminate, the process becomes “zombie” - it still exists even though it is “dead”, just like a Unix/Linux process that’s terminated but hasn’t yet been “reaped” by a `wait` operation.** The zombie process may retain most or all of the system resources that it used when running, so it is not a good idea to leave threads in this state for longer than necessary. And obviously, zombie process cannot save you resources! So also don’t try `pthread_exit()` for saving CPU and memory.
>
> I also noticed a undocumented problem caused by `pthread_exit()` - it may lead to failure of open procfs(`/proc/`) files! If one of your threads would open `/proc/mounts`(*currently I only find this file will go wrong, and other procfs files like `/proc/cpuinfo` or `/proc/uptime` can be successfully opened*) during its life, and `pthread_exit()` is called after creating these threads in the main thread, you will meet the “Invalid argument” error because of functions like `open("/proc/mounts",'r')`.
>
> [CS330 Intro to Threads](https://www.cs.uregina.ca/Links/class-info/330-bkup/Threads/threads.html)
>
> ##### Managing Threads
>
> **If you do not detach or join a thread, then when it exits it will become a zombie thread and consume some system resources.** This can really cause trouble, so make sure you take care of the zombies before they take care of you. They will eat your computer's brains.
>
> All threads return a void \* when they are done. This can happen at the end of the start routine, or when `pthread_exit()` is called. The value returned can be a pointer to anything in process memory, which makes the a thread's returned value much more powerful than that of a process. This value, along with the thread's state, will be either accepted and freed by another thread that is waiting on it with `pthread_join()`, discarded if the thread is detached, or be freed when the process quits if neither of the other two conditions is met.

### 7.3.6 Linux的stopped进程(优质网文转载)

*根据以下推荐的文章，大致总结如下。*

#### stopped进程的产生原因

1. Linux手抖按了`ctrl+z`挂起进程
2. 守护进程(Daemon)中要求终端输入`stdin`

​	一般会产生stopped进程，是Linux执行了某些需要I/O操作等，需要与用户交互才能继续的进程之后，不小心`ctrl+z`把进程挂起了。导致进程`stopped`。或者是代码运行守护进程（Daemon）/后台进程（PPID=1），然后在程序中要求读取终端输入`stdin`，即需要用户交互才能继续运行，这样也会使进程`stopped`。

​	在《Unix 环境高级编程》第9.8节作业控制中讲到，**“如果后台程序试图读取终端，这并不是一个错误，但是终端驱动程序将检测这种情况，并向后台作业发送一个特定信号SIGTTIN，该信号会停止此后台程序，并向用户发送通知”**。

#### stopped进程的处理方法

1. 强制结束stopped进程。`kill -9 %任务号`，其中任务号可通过`jobs`指令查看。

   ```shell
   # 比如 top指令查看 任务调度、CPU等情况时，手抖退出ctrl+c按错，按成了ctrl+z
   # ctrl+z 导致进程挂起 stopped
   jobs # 看到[1]+ Stopped top，即top进程被挂起
   kill -9 %1 # 一般jobs查看的任务号是递增的，第一个是1
   jobs #这时候查看就为空
   ```

2. 使`Stopped`进程重新就绪并运行。`fg 任务号`，其中任务号可通过`jobs`指令查看

   ```shell
   # 比如 top指令查看 任务调度、CPU等情况时，手抖退出ctrl+c按错，按成了ctrl+z
   # ctrl+z 导致进程挂起 stopped
   jobs # 看到[1]+ Stopped top，即top进程被挂起
   fg 1 # 画面重现来到top进程，且top页面第二行显示 0 stopped，说明被挂起的进程任务1被重新执行
   ```

> [Linux终端下后台运行程序被Stopped的原因以及解决](https://blog.csdn.net/a2796749/article/details/53192940)
>
> [如何让stopped的状态的命令在linux后台执行](https://zhidao.baidu.com/question/1797125669132525467.html)
>
> [Linux中ctrl+z 、ctrl+c、 ctrl+d区别](https://www.cnblogs.com/kelamoyujuzhen/p/10269818.html)
>
> Ctrl + C 是强制中断程序的执行,进程已经终止。 Ctrl + C 发送 SIGINT信号 参考：[linux信号](https://www.cnblogs.com/kelamoyujuzhen/p/9387739.html)
>
> Ctrl + Z 的是将任务中止（暂停的意思），但是此任务并没有结束,他仍然在进程中他只是维持挂起的状态,用户可以使用fg/bg操作继续前台或后台的任务,`fg`命令重新启动**前台**被中断的任务，`bg`命令把被中断的任务放在**后台**执行. 参考：[bash工作管理](https://www.cnblogs.com/kelamoyujuzhen/p/10087378.html)

### 7.3.7 Linux的守护进程和后台进程(优质网文转载)

#### 根据推荐文章的个人大致总结

后台进程继承父进程的文件描述符、SID等。**后台进程不能获得终端输入`stdin`**，但是可以使用`stdout`打印内容到终端屏幕。

+ 如果后台进程代码中需求`stdin`，则该进程任务job进入stopped状态，进程任务被暂停。
+ 如果后台进程不依赖`stdin`，只用到`stdout`，其仍然后台运行，且屏幕照样输出程序中的`printf`等函数的输出内容。



守护进程，**守护进程一定是后台进程，但后台进程不一定是守护进程**。

守护进程关键和后台进程最大差别在于，守护进程daemon的SID独立，与创建自己的父进程无瓜葛。

+ PPID=1

  由INIT接管。

+ SID=PGID=PID （值为`setsid`设置的一个随机SID号）

  自成会话组、进程组，且PID=SID，PID=SID，表示该daemon进程为会话组组长、进程组组长。

  自成会话组，则即使原会话SID结束<small>（比如bash关闭，创建该子进程的终端Terminal结束等）</small>，该进程也不会被结束。（普通后台进程，原SID和创建后台进程的父进程SID相同，只要父进程结束，则也被迫结束）



后台进程的常见创建方式=> `某可执行程序 &`

```shell
[root@xxx /]# top &
[1] 6131
```

*由于`top`需要从终端输入`stdin`获取用户指令，所以进程被暂停stopped。再次通过top指令可以看到stopped进程数量多一个。而通过`ps -axj`可以看到`top &`运行的后台进程状态为T，处于暂停状态。*



守护进程的创建创建方式：

+ 单次`fork`，执行`setsid`自建会话组，SID=PGID=PID。（其余执行指令不细说，`setsid`是关键步骤）
+ 双`fork`，其余和单次fork基本无差异。双fork好在daemon不再是PID=SID，即非会话组组长，防止daemon打开一个新控制终端。
+ 使用`daemon(0,0);`将当前进程变成守护进程执行，其函数原型为`int daemon(int nochdir, int noclose);`



下面是较早之前，个人思考的错误结果。==>两次fork根本目的是为了防止daemon进程打开一个新的控制终端（只有会话组组长，即自身PID=SID的进程，能够打开一个新的控制终端。）

​	~~守护进程，即父进程为`INIT`的进程。<small>(需要记住一点，只有原父进程消亡=>执行exit，才会使得子进程的父进程被设置成INIT，PPID=1)</small>~~

~~**守护进程的创建，可以只fork一次，也可以fork两次**。~~

+ ~~只fork一次，因为需要子进程被INIT接管，所以需要直接结束原进程。~~

  ~~适用于父进程无需执行其余操作，只需要用来创建守护进程的场景。~~

+ ~~常见的创建守护进程的方式即`fork()`两次，让孙子进程称为守护进程。~~

  ~~适用于父进程还需要继续往下执行其他代码/操作的场景。<small>（子进程fork出孙子进程后，直接exit，而原进程也马上wait子进程，使得孙子进程能快速被INIT接管，且原进程也能短暂`wait`之后继续往下执行自己的逻辑。）</small>~~

​	~~*创建守护进程往往是希望其与当前的进程无关联，能够独立运行。听上去好像子进程就满足这点，但是实际上子进程`exit()`时，需要父进程`wait()`才能处理子进程消亡前发出的`SIGCHLD`信号(用于确认父进程是否需要返回值)，如果父进程由于某些原因比如网络编程，需要监听某端口，长时间不消亡且循环监听事件，而不执行`wait()`，将导致所有已执行`exit()`的子进程的`task_struct`都无法被释放，因为其父进程仍存在，不能被INIT接管，且父进程又不执行`wait()`这个阻塞操作，导致子进程变僵尸进程后长时间资源没法被回收。*~~

---

> [守护进程--百度百科]([https://baike.baidu.com/item/%E5%AE%88%E6%8A%A4%E8%BF%9B%E7%A8%8B/966835?fr=aladdin](https://baike.baidu.com/item/守护进程/966835?fr=aladdin))
>
> 守护进程(daemon)是一类在后台运行的特殊进程，用于执行特定的系统任务。很多守护进程在系统引导的时候启动，并且一直运行直到系统关闭。另一些只在需要的时候才启动，完成任务后就自动结束。

#### 分类

按照服务类型分为如下几个。

1. 系统守护进程：`syslogd`、`login`、`crond`、`at`等。
2. 网络守护进程：`sendmail`、`httpd`、`xinetd`、等。
3. 独立启动的守护进程：`httpd`、`named`、`xinetd`等。
4. 被动守护进程（由`xinetd`启动）：`telnet`、`finger`、`ktalk`等。

#### 特点

​	首先，**守护进程最重要的特性是后台运行**。其次，守护进程必须与其运行前的环境隔离开来。这些环境包括未关闭的文件描述符、[控制终端](https://baike.baidu.com/item/控制终端)、[会话](https://baike.baidu.com/item/会话)和[进程组](https://baike.baidu.com/item/进程组)、工作目录以及文件创建[掩码](https://baike.baidu.com/item/掩码)等。这些环境通常是守护进程从执行它的[父进程](https://baike.baidu.com/item/父进程)(特别是shell)继承下来的。最后，守护进程的启动方式有其特殊之处。它可以在[Linux](https://baike.baidu.com/item/Linux)系统启动时从启动脚本`/etc/rc.d`中启动，也可以由作业控制进程`crond`启动，还可以由用户终端(通常是shell)执行。

​	除了止电以外，守护进程与普通进程<u>基本上没有什么区别</u>。因此，编写守护进样实际上是把一个普通进程按照上述的守护进程的特性改造成为守护进程。

#### 运行方式

1. 独立运行的守护进程

   ​	独立运行的守护进程由`init`脚本负责管理，所有独立运行的守护进程的脚本在`/etc/rc.d`，`/init.d/`目录下。系统服务都是独立运行的守护进程包括`syslogd`和`cron`等。服务器监听在一个特点的端口上等待[客户端](https://baike.baidu.com/item/客户端)的连接。如果客户端产生一个连接请求，守护进程就创建一个子服务器响应这个连接，而主服务器继续监听。以保持多个子服务器池等待下一个客户端请求。

2. 由`xinetd`管理的守护进程

   ​	从守护进程的概念可以看出，系统所运行的每一种服务，都必须运行一个监听某个端口连接所发生的守护进程，这通常意味着资源浪费。为了解决这个问题，Linux引进了“网络守护进程服务程序”的概念。`CentOS 6.4`使用的网络守护进程是`xinted(eXtendedInterNET services daemon)`。

   ​	`xinetd`能够同时监听多个指定的[端口](https://baike.baidu.com/item/端口)，在接受用户请求时，它能够根据用户请求的端口不同，启动不同的网络服务进程来处理这些用户请求。可以把`xinetd`看作一个管理启动服务的管理服务器，它决定把一个客户请求交给那个程序处理，然后启动相应的守护程序。

#### 创建步骤

1. 创建子进程，终止父进程

   ​	由于**守护进程是脱离控制终端的**，因此首先创建子进程，终止父进程，使得程序在shell终端里造成一个已经运行完毕的假象。之后所有的工作都在子进程中完成，而用户在shell终端里则可以执行其他的命令，从而使得程序以僵尸进程形式<small>（父进程提前消亡，子进程变"孤儿进程"，被分配给INIT进程管理。失去原父进程后才不得不被INIT接管的，即僵尸进程。）</small>运行，在形式上做到了与控制终端的脱离。

2. 在子进程中创建新会话

   ​	这个步骤是创建守护进程中最重要的一步，在这里使用的是系统函数`setsid`。

   ​	`setsid`函数用于创建一个新的会话，并担任该会话组的组长。<u>调用`setsid`的三个作用：让进程摆脱**原会话**的控制、让进程摆脱**原进程组**的控制和让进程摆脱**原控制终端**的控制。</u>

   ​	在调用`fork`函数时，[子进程](https://baike.baidu.com/item/子进程)全盘拷贝父进程的<u>会话期(`session`，是一个或多个进程组的集合)、进程组、控制终端</u>等，虽然父进程退出了，但原先的会话期、进程组、控制终端等并没有改变，因此，那还不是真正意义上使两者独立开来。`setsid`函数能够使进程完全独立出来，从而脱离所有其他进程的控制。（COW，写时复制，所以单纯`fork`执行后使用的内存空间指针，还是指向原来的。）

3. 改变工作目录

   ​	<u>使用`fork`创建的子进程也继承了父进程的当前工作目录</u>。由于在进程运行过程中，当前目录所在的文件系统不能卸载，因此，把当前工作目录换成其他的路径，如“`/`”或“`/tmp`”等。改变工作目录的常见函数是`chdir`。

4. 重设文件创建掩码

   ​	文件创建掩码是指屏蔽掉文件创建时的对应位。由于使用`fork`函数新建的子进程继承了父进程的文件创建掩码，这就给该子进程使用文件带来了诸多的麻烦。因此，把文件创建掩码设置为0，可以大大增强该守护进程的灵活性。设置文件创建掩码的函数是`umask`，通常的使用方法为`umask(0)`。

5. 关闭文件描述符

   ​	**用`fork`新建的子进程会从父进程那里继承一些已经打开了的文件。这些被打开的文件可能永远不会被守护进程读或写，但它们一样消耗系统资源，可能导致所在的文件系统无法卸载**。

----

> [linux 守护进程的ppid的疑惑](https://zhidao.baidu.com/question/161961932.html)

```c
int main(int argc, char* argv[])
{
    logit("/tmp/test.log", 0, TRUE, "this is a test print 1 in log");
    daemon_init("/var/run/test.pid",TRUE,"/tmp/test.log");
    logit("/tmp/test.log", 1, FALSE, "this is a test print 2 in log");
    while(1){
        logit("/tmp/test.log",3,FALSE,"i am a daemon:%d->%d",getppid(),getpid());
        sleep(30);
    }
}
```

+ 问：

  ​	在守护进程里每隔半分钟`syslog`一下它的`pid`和`ppid`。但是，第一个输出：`ppid`为25672，`pid`为25673 以后的输出都是：`ppid`为1， `pid`为25673。为什么第一个输出不是1？

+ 答：

  ​	如果不是用`init`脚本启动而是由命令行启动的话，第一次启动的`ppid`是第一次`daemon_init`函数中第一次`fork`后的子进程的ID，`pid`是第二次`fork`后子进程的ID，第一次循环运行时，可能其父进程还未完全退出，或者系统清理过程还未执行，所以不是1，以后的循环中，由于第一次`fork`的子进程已经结束，第二次`fork`的子进程成为孤儿进程，被`init`接管，所以其`ppid`为1，而其自己的`pid`并未改变

---

> [后台进程不等于守护进程](https://www.cnblogs.com/virusolf/p/5420152.html) <= 只截取部分。文章偏实战场景。

**守护进程没有控制终端，而后台进程还有**。

通过这样的方式启动firefox, 
`#firefox &`
firefox现在在后台运行了，但是它不是守护进程，只是后台进程。
**因为它并没有脱离控制终端**，不信？你试着把启动firefox的终端`terminal`关掉，看看firefox会不会跟着一起关闭<==会被一同关闭。 

1. 后台的文件描述符也是继承于父进程，例如shell，所以它也可以在当前终端下显示输出数据。

   但是**daemon进程自己变成了进程组长**，其文件描述符号和控制终端没有关联，是控制台无关的。 

2. 基本上任何一个程序都可以后台运行，但守护进程是具有特殊要求的程序，比如要脱离自己的父进程，成为自己的会话组长等，这些要在代码中显式地写出来。
   换句话说，**守护进程肯定是后台进程，但反之不成立**。

   守护进程顾名思义，主要用于一些长期运行，守护着自己的职责（监听端口，监听服务等）。我们的系统下就有很多守护进程。

3. 守护进程成为了进程组长(或者会话组长)，和控制终端失去了联系（其文件描述符也是继承于父进程的，但是在变成守护进程的同时`stdin`,`stdout`,`stderr`和控制台失去联系了）

---

> [创建守护进程为什么要fork两次](https://blog.csdn.net/dream_1996/article/details/73467969)  以下内容出至该文章，下面不过对排版进行修改

**一、守护进程的基本概念：**
守护进程也叫精灵进程，它是大多数服务器的载体。守护进程是一种运行在后台的一种特殊的进程，它独立于控制终端并且周期性的执行某种任务或是等待处理某些发生的时间。如果想让某个进程不因为用户或中断或其他变化而影响，那么就必须把这个进程变成一个守护进程。

**举个例子：**在现实生活中， 许多大型的软件或服务器必须保证7\*24小时（一周7天，一天24小时）无障碍的运行，例如淘宝网、百度搜索引擎、支付宝等等，那么像这样一种要一直运行的程序怎么实现呢？究其本质其实就是我们的守护进程。

**二、守护进程的特点：**

1. **自成进程组**，**自成会话**，**与控制终端脱关联**；
2. 守护进程的**父进程是1号进程**；
3. 守护进程的命令一般以字符d结尾；
4. 守护进程的生命周期是7*24小时不掉线；
5. 一般的网络服务器都以守护进程的形式在后台运行，比如常见的http，ftp等等服务器都是以守护进程的形式在后台运行；

**三、创建守护进程**

1、查看系统中的进程

```
ps axj
```

+ 参数a表示不仅列当前用户的进程,也列出所有其他用户的进程,
+ 参数x表示不仅列有控制终端的进程,也列出所无控制终端的进程,
+ 参数j表示列出与作业控制相关的信息。



2、`setsid`函数
（1）**创建守护进程最关键的一步是调用`setsid`函数创建一个新的Session,并成为Session Leader**。

```
#include<unistd.h>

pid_t setsid(void);
```

返回值：该函数调用成功时返回新创建的Session的id(其实也就是当前进程的id),出错返回-1。

（2）需要注意的是，**调用这个函数之前,当前进程不允许是进程组的Leader,否则该函数返回-1**。 
解决办法：先fork再调用`setsid`，fork创建的子进程和父进程在同一个进程组中,进程组的Leader必然是该组的第一个进程,所以子进程不可能是该组的第一个进程,在子 进程中调用`setsid`就不会有问题了。（默认fork出来继承父进程的SID，会话号）

（3）成功调用该函数的结果是: 

1. **创建一个新的Session,当前进程成为Session Leader,当前进程的id就是Session的id。** 
2. **创建一个新的进程组,当前进程成为进程组的Leader,当前进程的id就是进程组的id。** 
3. **如果当前进程原本有一个控制终端,则它失去这个控制终端,成为一个没有控制终端的进程。**所谓失去控制终端是指,原来的控制终端仍然是打开的,仍然可以读写,但只是一个普通的打开文件而不是控制终端了。



3、创建守护进程的步骤 

方式一：

(1)调用`umask`将文件模式创建屏蔽字设置为0.

```
umask(0);//umask必须清0，否则创建文件受系统默认权限的影响
```

文件权限掩码是屏蔽掉文件权限中的对应位。由于使用`fork（）`函数新创建的子进程继承了父进程的文件权限掩码，这就给该子进程使用文件带了很多的麻烦（比如父进程中的文件没有执行文件的权限，然而在子进程中希望执行相应的文件这个时候就会出问题）。因此在子进程中要把文件的权限掩码设置成为0，即在此时有最大的权限，这样可以大大增强该守护进程的灵活性。

(2)调用fork，父进程退出（exit）。 
原因： 
1）如果该守护进程是作为一条简单的shell命令启动的，那么⽗进程终止使得shell认为该命令已经执行完毕。
2）保证子进程不是一个进程组的组长进程。



(3)**调用`setsid`创建一个新会话**。 
`setsid`会导致： 
1）调用进程成为新会话的首进程。 
2）调用进程成为一个进程组的组长进程 。 
3）调用进程没有控制终端。（再次fork一次，保证daemon进程，之后不会打开`tty`设备）

调用`setsid`的原因： 
<u>由于创建守护进程的第一步是调用`fork()`函数来创建子进程，再将父进程退出。由于在调用了`fork()`函数的时候，子进程拷贝了父进程的会话期、进程组、控制终端等资源、虽然父进程退出了，但是会话期、进程组、控制终端等并没有改变，因此，需要用`setsid()`函数来时该子进程完全独立出来，从而摆脱其他进程的控制。</u>



(4)**将当前工作目录更改为根目录**。 
防止当前目录有一个目录被删除，导致守护进程无效。 
使用fork()创建的子进程是继承了父进程的当前工作目录，**由于在进程运行中，当前目录所在的文件系统是不能卸载**的，这对以后使用会造成很多的麻烦。因此通常的做法是让“/”作为守护进程的当前目录，当然也可以指定其他的别的目录来作为守护进程的工作目录。



(5)**关闭不再需要的文件描述符**。 
同文件权限码一样，用`fork()`函数新建的子进程会从父进程那里继承一些已经打开了的文件。这些文件被打开的文件可能永远不会被守护进程读写，如果不进行关闭的话将会浪费系统的资源，造成进程所在的文件系统无法卸下以及引起预料的错误。

如：关闭标准输入流、标准输出流、标准错误流。

```
close(0);
close(1);
close(2);
```



(6)其他：忽略`SIGCHLD`信号 (不是必须的)

`signal(SIGCHLD,SIG_IGN);`

```c
#include<stdio.h>
#include<unistd.h>
#include<signal.h>
#include<stdlib.h>
int creat_daemon()
{  
    //(1)调用umask将文件模式创建屏蔽字设置为0.
    umask(0);    
    //(2)调用fork，父进程退出（exit）。  
    pid_t id = fork();
    if(id > 0)
    {
        exit(1);
    }
    else if(id == 0)
    {
        setsid();         //(3)创建一个新的会话
        if(chdir("/")<0)  //(4)将新建会话的工作目录改成根目录
        {
            perror("chdir");
            return;
        }
        //(5)关闭从父进程继承来的文件
        close(0);
        //close(1); 假如这里没有关闭会出现什么情况，看下面结果
        close(2);
        //(6)忽略SIGCHLD信号。
        signal(SIGCHLD, SIG_IGN);

    }
}
int main()
{
    creat_daemon();
    while(1);
    return 0;
}
```


![img](https://img-blog.csdn.net/20170619165207875?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvZHJlYW1fMTk5Ng==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center)

由上图可知通过运行编写的mydaemon在Linux后台创建了一个守护进程，它的父进程是1号进程，它自己是**自成进程组，自成会话的**。

在Linux的根目录下有一个目录`proc`，`/proc`目录中包含许多以数字命名的子目录，这些数字表示系统当前正在运行进程的进程号，里面包含对应进程相关的多个信息文件。例如 cd /proc/18225，进入到刚才创建的守护进程的id里面，并查看它进程id所对应的文件描述符fd的情况。

![img](https://img-blog.csdn.net/20170619165223846?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvZHJlYW1fMTk5Ng==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center)

可以发现当1号文件描述符不关闭的时候，此时的1号文件描述符指向的是自己的特殊的设备文件/dev/pts/1。所以在创建守护进程的时候就是有一个步骤要关闭不必要的文件描述符。  





**方式二：**创建守护进程也可以调用`daemon`函数：`int daemon(int nochdir, int noclose);` 

一般调用daemon有两种方式：

1. `daemon(0,0); `

   **默认更改工作目录且更改文件描述符0，1，2为空，使其指向/dev/null。这个/dev/null就类似黑洞，不管哪个文件描述符对其进行写操作，它都会直接将数据丢弃；**

2. `daemon(1,0); `

   不更改工作目录也不更改文件描述符；



**守护进程要fork两次的原因：**

**第一次fork：**这里第一次fork的作用就是让shell认为这条命令已经终止，不用挂在终端输入上;再一个是为了后面的`setsid`服务，因为调用`setsid`函数的进程不能是进程组组长，如果不fork子进程，那么此时的父进程是进程组组长，无法调用`setsid`。所以到这里子进程便成为了一个新会话组的组长。



**第二次fork：**第2次fork不是必须的。也看到很多开源服务没有fork第二次。**fork第二次主要目的是，防止进程再次打开一个控制终端**。因为**打开一个控制终端的前台条件是该进程必须是会话组长**。再fork一次，子进程ID != sid（sid是进程父进程的sid）。所以也无法打开新的控制终端。

daemon目的就是防止终端产生的一些信号让进程退出。上面函数并没有直接调用signal函数去处理它。而是间接通过fork和setsid函数使用更少代码优雅处理。而被有些人误以为是防止父进程没有正常退出而导致的僵死进程的原因需要这样处理。

> [What is the reason for performing a double fork when creating a daemon?](https://stackoverflow.com/questions/881388/what-is-the-reason-for-performing-a-double-fork-when-creating-a-daemon)
>
> + Fork a second child and exit immediately to prevent zombies. This causes the second child process to be orphaned, making the init process responsible for its cleanup. And, since the first child is a session leader without a controlling terminal, it's possible for it to acquire one by opening a terminal in the future (System V- based systems). This second fork guarantees that the child is no longer a session leader, preventing the daemon from ever acquiring a controlling terminal.
>
> + In Unix every process belongs to a group which in turn belongs to a session. Here is the hierarchy…
>
>   **Session (SID) → Process Group (PGID) → Process (PID)**
>
>   The first process in the process group becomes the process group leader and the first process in the session becomes the session leader. **Every session can have one TTY associated with it**. **Only a session leader can take control of a TTY**. For a process to be truly daemonized (ran in the background) we should ensure that the session leader is killed so that there is no possibility of the session ever taking control of the TTY.
>
> + Strictly speaking, the double-fork has nothing to do with re-parenting the daemon as a child of `init`. All that is necessary to **re-parent the child is that the parent must exit. This can be done with only a single fork**. Also, doing a double-fork by itself doesn't re-parent the daemon process to `init`; the daemon's parent *must* exit. In other words, the parent always exits when forking a proper daemon so that the daemon process is re-parented to `init`.
>
>   So why the double fork? [POSIX.1-2008](http://pubs.opengroup.org/onlinepubs/9699919799/toc.htm) Section 11.1.3, "[The Controlling Terminal](http://pubs.opengroup.org/onlinepubs/9699919799/basedefs/V1_chap11.html#tag_11_01_03)", has the answer (emphasis added):
>
>   > The controlling terminal for a session is **allocated by the session leader** in an implementation-defined manner. If a session leader has no controlling terminal, and opens a terminal device file that is not already associated with a session without using the `O_NOCTTY` option (see `open()`), it is implementation-defined whether the terminal becomes the controlling terminal of the session leader. If a process which is **not a session leader** opens a terminal file, or the `O_NOCTTY` option is used on `open()`, **then that terminal shall not become the controlling terminal of the calling process**.
>
>   This tells us that if a daemon process does something like this ...
>
>   ```py
>   int fd = open("/dev/console", O_RDWR);
>   ```
>
>   ... then the daemon process *might* acquire `/dev/console` as its controlling terminal, depending on whether the daemon process is a session leader, and depending on the system implementation. The program can *guarantee* that the above call will not acquire a controlling terminal if the program first ensures that it is not a session leader.
>
>   Normally, when launching a daemon, `setsid` is called (from the child process after calling `fork`) to dissociate the daemon from its controlling terminal. However, calling `setsid` also means that the calling process will be the session leader of the new session, which leaves open the possibility that the daemon could reacquire a controlling terminal. **The double-fork technique ensures that the daemon process is not the session leader**, which then guarantees that a call to `open`, as in the example above, will not result in the daemon process reacquiring a controlling terminal.
>
>   **The double-fork technique is a bit paranoid. It may not be necessary if you *know* that the daemon will never open a terminal device file**. Also, on some systems it may not be necessary even if the daemon does open a terminal device file, since that behavior is implementation-defined. However, one thing that is not implementation-defined is that only a session leader can allocate the controlling terminal. **If a process isn't a session leader, it can't allocate a controlling terminal.** Therefore, if you want to be paranoid and be certain that the daemon process cannot inadvertently acquire a controlling terminal, regardless of any implementation-defined specifics, then the double-fork technique is essential.
>
> [Why use double-fork to daemonize?--2014](https://thelinuxjedi.blogspot.com/2014/02/why-use-double-fork-to-daemonize.html)
>
> When you fork and kill the parent of the fork the new child will become a child of "init" (the main process of the system, given a PID of 1).  You may find others state on the interweb that the double-fork is needed for this to happen, but that isn't true, a single fork will do this.
>
> What we are actually doing is a kind of safety thing to make sure that the daemon is completely detached from the terminal.  The real steps behind the double-fork are as follows:
>
> 1. The parent forks the child
> 2. The parent exits
> 3. The child calls [setsid()](http://linux.die.net/man/2/setsid) to start a new session with no controlling terminals
> 4. The child forks a grandchild
> 5. The child exits
> 6. The grandchild is now the daemon
>
> The reason we do step 4 & 5 is it is possible for the child to regain control of the terminal, but once it has lost control the forked grandchild cannot do this.
>
> Put simply it is a roundabout way of completely detaching itself from the terminal that started the daemon.  It isn't a strict requirement to do this at all, many modern init systems can daemonize a process that will stay in the foreground quite easily.  But it is useful for systems that can't do this and for anything that at some point in time is expected to be run without an init script.

### 7.3.8 Linux的fork和exec（优质网文转载）

> [同一进程下的线程可以共享什么？](https://blog.csdn.net/Lemon_MY/article/details/100714655) 下述内容转自该文章

线程独占的资源：

1.线程ID
2.**寄存器组的值**
3.**线程的堆栈**
4.**错误返回码**
5.线程的信号屏蔽码

线程间共享的资源：

1.进程代码段
2.进程的公有数据(利用这些共享的数据，线程很容易的实现相互之间的通讯，堆数据)
3.进程打开的**文件描述符**
4.信号的处理器
5.进程的当前目录
6.进程用户ID与进程组ID

---

> [程序员必备知识——fork和exec函数详解](https://blog.csdn.net/bad_good_man/article/details/49364947) <= 下面内容出至该文（只摘取部分）

1. fork函数原型

   ```c
   pid_t fork(void);
   ```

   ​	如果之前从未接触过这个函数，那么**理解fork函数的最困难之处在于调用它一次，它却返回两次**。它在调用进程（成为父进程）中返回一次，返回值为新派生进程（成为子进程）的进程ID号；在子进程中又返回一次，返回值为0。因此，返回值本身告知当前进程是子进程还是父进程。

   ​	`fork`在子进程中返回0而不是父进程的ID的原因在于：**任何子进程只有一个父进程**，而且子进程总是可以通过调用`getppid`取得父进程的ID。相反，父进程可以有许多子进程，而且无法获得各个子进程的进程ID。如果父进程想要跟踪所有子进程的ID，那么它必须记录每次调用`fork`的返回值。

   ​	<u>父进程中调用`fork`之前打开所有的描述字在`fork`返回之后由子进程分享。我们将看到网络服务器便利用了这个特性：父进程调用`accept`之后调用`fork`。所接受的已连接的套接口随后就在父进程与子进程之间分享。通常情况下，子进程接着读和写这个套接口，父进程则关闭这个已连接套接口。</u>

2. fork用法

   fork有两个典型的用法：

   1. 一个进程创建一个自身的拷贝，这样每个拷贝都可以在另一个拷贝执行其他任务的同时处理各自的某个操作。这是<u>网络服务器的典型用法</u>。
   2. 一个进程想要执行另一个程序。既然创建新进程的唯一方法为调用fork，该进程于是首先调用fork创建一个自身的拷贝，然后其中一个拷贝（通常为子进程）调用exec把自身替换成新的程序。这是<u>诸如shell之类程序的典型用法</u>。

3. exec用法

   ​	存放在硬盘上的可执行文件能够被UNIX执行的唯一方法是：由一个现有进程调用六个exec函数中的某一个。exec把当前进程映像替换成新的进程文件，而且该新程序通常从main函数处开始执行。进程ID并不改变。我们称调用exec的进程为调用进程，称新执行的程序为新程序。

   ​    六个exec函数的区别在于：（a）待执行的程序文件是由文件名还是由路径名指定；（b）新程序的参数是一一列出还是由一个指针数组来引用；（c）把调用进程的环境传递给新程序还是给新程序指定新的环境。

   ```c
   #include<unistd.h>
   int execl(const char *pathname, const char *arg0,.../* (char *)0 */);
   int execv(const char *pathname, char *const argv[]);
   int execle(const char *pathname, const char *arg0,.../* (char *)0,char *const envp[] */);
   int execve(const char *pathname, char *const argv[], char *const envp[]);
   int execlp(const char *filename, const char *arg0,.../* (char *)0 */);
   int execvp(const char *filename, char *const argv[]);
   // 所有六个函数返回：-1——失败，无返回——成功
   ```

   这些函数<u>只在出错时才返回到调用者</u>。否则，控制将传递给新程序的起始点，通常就是main函数。
   这六个函数之间的关系如下图所示。一般来说，只有`execve`是内核中的系统调用，其他五个都是调用`execve`的库函数。

   ![img](https://img-blog.csdn.net/20151023185149377?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQv/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center)

    **注意这六个函数的下列区别**：

   1. 顶行三个函数把新程序的每个参数字符串指定成`exec`的一个独立参数，并以一个空指针结束可变数量的这些参数。底行三个函数都有一个作为`exec`参数的`argv`数组，其中含有指向新程序各个参数字符串的所有指针。既然没有指定参数字符串的数目，这个`argv`数组必须含有一个用于指定其末尾的空指针。
   2. 左列两个函数指定一个`filename`参数。`exec`将使用当前的PATH环境变量把该文件名参数转换为一个路径名。然而如果这两个函数的`filename`参数中不论何处含有一个斜杠（/），PATH变量就不再使用。右两列四个函数指定一个全限定的`pathname`参数。
   3. 左两列四个函数不显式指定一个环境指针。相反，他们使用外部变量`environ`的当前值来构造一个传递给新程序的环境清单。右列两个函数显式指定一个环境清单，其`envp`指针数组必须以一个空指针结束。

---

> [进程调用fork与文件描述符的共享(fork,dup)](https://blog.csdn.net/ordeder/article/details/21716639) <= dup而不是dump，这个博主搞错了

#### 背景

​	Linux的进程描述`task_struct{}`中有一个数组专门用于记录一打开的文件，其中文件描述符作为该数组的下标，数组元素为指向所打开的文件所创建的文件表项。如下图所示，文件表项是用于描述文件当前被某个进程打开后的状态信息，包括文件状态标志，记录当前文件读取的位移量（可以通过接口`lseek`设置），以及文件的`i`节点指针（`i`节点描述文件的具体信息，如：创建，修改时间，文件大小，文件存储的块信息）。

​    不同进程打开同一个文件后，进程表和文件表的关系如下图所示：

![img](https://img-blog.csdn.net/20140321144748953?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvb3JkZWRlcg==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)

#### 进程的fork与文件描述符的拷贝

进程的所打开文件和在fork后的结构图如下所示，子进程是共享父进程的文件表项；

![img](https://img-blog.csdn.net/20140321144041890?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvb3JkZWRlcg==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)

可以通过一个测试实例来证实以上的描述：

```cpp
#include "slp.h"
 
int main()
{
    int fd1,fd2,fd3,nr;
    char buff[20];
    pid_t pid;
    fd1 = open("data.in",O_RDWR);
    pid = fork();
    if(pid == 0)
    {   
        nr = read(fd1,buff,10);
        buff[nr]='\0';
        printf("pid#%d content#%s#\n",getpid(),buff);
        exit(0);
    }   
    nr = read(fd1,buff,10);
    buff[nr]='\0';
    printf("pid#%d content#%s#\n",getpid(),buff);
    return 0;
}
```

测试用例

```html
data.in
abcdefghijklmnopqrstuvwxyz1234567890
EOF
```


测试结果：

```none
pid#20029 content#abcdefghij#
pid#20030 content#klmnopqrst#
```

结果分析：

​	进程20029对文件的读取后的当前位置应该为data.in的k字符所在的位置，进程20030是由20029进程之后开始读取的，他读取文件内容不是从a开始，而是从k开始，说明20030共享了20029的文件表。

#### 进程dup()一个文件描述符

![img](https://img-blog.csdn.net/20140321143935687?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvb3JkZWRlcg==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)

#### 总结

+ **进程调用`fork`后，子进程和父进程的文件描述符所对应的文件表项是共享的，这意味着子进程对文件的读写直接影响父进程的文件位移量(反之同理)**。

+ 进程中调用`fd2 = dup(fd1) `产生的新的`fd2`所指向的文件表项和`fd1`指向的文件表项是**相同**的；

+ 进程中分别调用：`fd1 = open("data.in",O_RDWR); fd2 = open("data.in",O_RDWR); `那么`fd1`和`fd2`指向的文件表项是**不同**的。

> 下面推荐一些文章，因为越往下的篇幅越长，所以就不复制内容过来了，建议自己阅读。
>
> [操作系统 fork与exec](https://blog.csdn.net/qq_39747794/article/details/88534383) <= 推荐阅读，如果纯小白的话
>
> <small>在传统的Unix、linux环境下，有两个基本的操作用于创建和修改进程：函数fork( )用来创建一个新的进程，该进程几乎是当前进程的一个完全拷贝；函数族exec( )用来启动另外的进程以取代当前运行的进程。这些函数给程序猿的多进程编程提供了遍历，也增加了CPU的利用率。</small>
>
> [fork() 与exec()的区分](https://www.jianshu.com/p/e84945fc8b8c) <= 推荐阅读，简洁
>
> <small>fork和exec经常会放到一块去使用，来创建一个新的子进程，并且在这个子进程里去运行一个新的程序。fork用来创建子进程，处理的对象是进程；而exec()是用来处理程序，重新加载一个进程里的程序。</small>
>
> [Linux下Fork与Exec使用](https://www.cnblogs.com/wuchanming/p/3784862.html) <= 推荐阅读，介绍更细粒，且还包括进程间通信IPC的内容
>
> [linux下多线程编程](https://www.cnblogs.com/wuchanming/p/4411908.html) <= 编程实战，内容确实不错。建议有时间的话阅读下。
>
> [linux中fork（）函数详解（原创！！实例讲解）](https://blog.csdn.net/jason314/article/details/5640969) <= 各种fork详细代码样例，有时间阅读和思考是挺不错的。
>
> [Linux fork隐藏的开销-过时的fork(正传)](https://blog.csdn.net/dog250/article/details/100168430) <= **超建议阅读，实际Linux环境下的fork代码使用分析**

### 7.3.9 Linux进程PID、PPID、PGID、SID等

> [PID, PPID, PGID与SID](https://blog.csdn.net/Justdoit123_/article/details/101347971) <= 以下内容出自该文

Linux中，进程都拥有以下的ID

- Process ID(PID)
  Linux中标识进程的一个数字，它的值是不确定的，是由系统分配的（但是有一个例外，启动阶段,kernel运行的第一个进程是`init`，它的PID是1，是所有进程的最原始的父进程），每个进程都有唯一PID，当进程退出运行之后，PID就会回收，可能之后创建的进程会分配这个PID
- Parent Process ID(PPID)
  字面意思，父进程的PID
- Process Group ID(PGID)
  PGID就是进程所属的Group的Leader的PID，如果PGID=PID，那么该进程是Group Leader
- Session ID(SID)
  和PGID非常相似，SID就是进程所属的Session Leader的PID，如果SID==PID，那么该进程是session leader

Session和Group都是管理多个进程的方式，同一个Group的进程属于同一个Session，一个Session里可以包含多个Group
`ps j`或者`ps -j`都可以显示出以上四个ID，命令如下

```shell
ps axj
ps -efj
12
```

#### 意义

group和session虽然都是进程的集合，但是他们的意义不同。

fork出的子进程，会继承group和session（PGID和SID与父进程相同）。

**session与终端相关（Control terminal），同一个终端启动的进程默认会在一个session里**。例如图形界面的终端（比如GNOME按`ctrl+atl+T`呼出的命令行界面），都是虚拟终端（Virtual terminal），他们**实质上只有一个终端在真正起作用**，输入`w`命令，可以看到所有的`control terminal`。
group则是方便管理，比如发送信号，kill可以一次向一个group的进程发送同一个信号，`ctrl+z`进入后台、`bg`、`fg`都可以对一个group的进程起作用。比如`ctrl+z`可以将一个group的进程stop暂停运行，`fg`可以让一个group继续运行

```shell
top #在前台运行一个top
按下ctrl+z，top进入stopped状态
bg %1 #让top在后台运行，相当于运行top &，输出信息会显示在当前的shell上
fg %1 #让top在前台运行
1234
```

这里的top如果是多进程的程序，他们在共同的group中，也有相同的效果

> 其他文章推荐：
>
> [对于Linux内核tty设备的一点理解](http://blog.chinaunix.net/uid-20543672-id-3225777.html)
>
> 最初tty是指连接到Unix系统上的物理或者虚拟终端。终端是一种字符型设备，通常使用tty来统称各种类型的终端设备。随着时间的推移，当通过串行口能够建立起终端连接后，这个名字也用来指任何的串口设备。它还有多种类，例如串口（ttySn、ttySACn、ttyOn）、USB到串口的转换器(ttyUSBn)，还有需要特殊处理才能正常工作的调制解调器（比如传统的WinModem类设备）等。tty虚拟设备支持虚拟控制台，它能通过键盘及网络连接或者通过xterm会话登录到计算机上。
>
> [The Linux Process Principle, PID、PGID、PPID、SID、TID、TTY](https://www.cnblogs.com/bspp1314/p/9429170.html)
>
> *0x1: PID(Process ID 进程 ID号)*
>
> 0x2: TGID(Thread Group ID 线程组 ID号)
>
> 0x3: PGID(Process Group ID 进程组 ID号)
>
> 0x4: PPID( Parent process ID 父进程 ID号)
>
> 0x5: SID(Session ID 会话ID)
>
> [What do the identifiers PID, PPID, SID, PGID, UID, EUID mean?--stackoverflow](https://stackoverflow.com/questions/41498383/what-do-the-identifiers-pid-ppid-sid-pgid-uid-euid-mean)
>
> - UID - User ID
> - [EUID - Effective User ID](https://en.wikipedia.org/wiki/User_identifier#Effective_user_ID)
>
> [Understanding Linux processes, threads, Pid,lwp,tid,tgid](https://topic.alibabacloud.com/a/understanding-linux-processes-threads-pidlwptidtgid_1_16_30000800.html)
>
> Initial understanding of the various IDs. Basically, from high to low, IDs below the split line are less important.
>
> - **PID**: Process ID.
> - **LWP**: Thread ID. A common display in user-configured commands such as PS.
> - **tid**: Thread ID, equal to LWP. TID is more commonly used in system-provided interface functions, such as Syscall (Sys_gettid) and Syscall (__nr_gettid).
> - **tgid**: Thread group ID, which is the process ID of thread group leader, equals PID.
>
> ------Split Line------
>
> - **pgid**: Process group ID, which is the process ID of the process group leader.
> - **pthread ID**: The ID provided by the Pthread library, the effective range is not at the system level and can be ignored.
> - **SID**: Session ID for the session leader.
> - **tpgid**: TTY process group ID for the process group leader.
>
> [Linux: How to View Threads of a Process--需翻墙](https://stackpointer.io/unix/linux-view-threads-process/536/) <== 具体的Linux指令操作

# 8.  处理器(CPU)调度

## 8.0 背景

### 8.0.1 上下文切换和CPU调度

上下文切换

- 切换CPU的当前任务，从一个进程/线程到另一个
- 保存当前进程/线程在PCB/TCB中的执行上下文（CPU状态）
- 读取下一个进程/线程的上下文 

*（Linux中进程和线程的控制块，其实都是`task_struct`，线程本质不过是设置共享某个进程虚拟空间资源的进程罢了。）*

CPU调度

- 从就绪队列中挑选一个进程/线程作为CPU将要运行的下一个线程/进程 
- 调度程序：挑选进程/线程的内核函数（通过一些调度策略）
- 什么时候进行调度？在进程/线程的生命周期中的什么时候进行调度？

![img](https://img-blog.csdn.net/20180421095732998?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0FsYXRlYmxvb21lcg==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)

只要进程发生状态改变，那么就触发一次调度。

*（毕竟PCB在内核态，状态改变必然需要操作系统辅助，而CPU作为计算机的执行者，当然需要调度相关的内核线程/进程完成PCB的信息修改了。）*

内核运行调度程序的条件（满足一条即可）

- 一个进程从运行状态切换到等待状态
- 一个进程被终结了

调度一般可以分成"不可抢占"和"可抢占"两种：

1. 不可抢占

   + 调度程序必须等待事件结束

2. 可以抢占：

   + 调度程序在中断被响应后执行

   + 当前进程从运行切换到就绪，或者一个进程从等待切换到就绪

   + 当前运行的进程可以被换出

*(早期操作系统单进程单线程，所以一般采用非抢占式的调度策略。一个进程必须从头到尾执行完毕后，其他进程才能执行。)*

*(起初，抢占和非抢占式调度，只针对用户态进程而言。但现代操作系统Windows、Linux等，内核态进程同样可进行抢占式调度。)*

> [八、CPU调度](https://blog.csdn.net/Alatebloomer/article/details/80026519) <= 图片来源。<small>虽然已经确认了github发起的CSDN的img加载会失败，但是至少我本地还能看。</small>

### 8.0.2 DMA（直接存储器访问）

> [为何IO操作不需要CPU?](https://www.zhihu.com/question/31259327/answer/147547667	) <= "2.为什么会出现DMA"的内容来源
>
> [DMA （直接存储器访问）--百度百科](https://baike.baidu.com/item/DMA/2385376?fr=aladdin) <= 大部分内容来源
>
> [下列设备中，哪种适于通过DMA方式与主机进行信息交换( )。](https://tiku.baidu.com/web/singledetail/31619822bcd126fff7050beb) <= "1. 什么是DMA"的第二段
>
> 下面内容基于上述文章内容再进行整理

1. 什么是DMA

   ​	DMA(Direct Memory Access，直接存储器访问) 是**所有现代电脑的重要特色，它允许不同速度的硬件装置来沟通，而不需要依赖于 CPU 的大量中断负载**。否则，CPU 需要从来源把每一片段的资料复制到[暂存器](https://baike.baidu.com/item/暂存器/4308343)（寄存器），然后把它们再次写回到新的地方。在这个时间中，CPU 对于其他的工作来说就无法使用。

   ​	直接存储器存取方式DMA (Direct memory access)是一种在外设和内存之间进行的**以数据块为单位**的快速的信息传输方式，适合于磁盘和内存间的信息交换，而其他几种外设只能以串行方式与存储器沟通（键盘、鼠标、针式打印机等）。

2. 为什么会出现DMA

   I/O发展是经过以下一步步发展的：

   1. CPU直接控制外围设备(硬盘磁带等), 在简单微处理器中常用
   2. 增加I/O模块，将CPU与外围设备解耦，CPU只与I/O模块
      打交道。只需定义好接口，CPU厂商和外围设备厂商就可以相互根据接口开发，互不影响
   3. 增加中断方式，还是经过I/O模块，只不过I/O模块完成之后，只需通知CPU即可，CPU在等待阶段完全可以去做其他事情，提高CPU利用率。
   4. I/O模块增加**DMA控制器**。之前的阶段是每次只传输一个字，就通知CPU，就发起一次中断，CPU放到寄存器中，再放到内存中。这样CPU就会被连续的中断打断，不断切换进程，上下文，效率很低。
      **DMA控制器类似于一个小的CPU, 有自己的寄存器(记录主存地址和取到的字的count等).**
      CPU可以发起一个DMA请求，传入读写操作类型，相关I/O设备地址，内存的起始地址，要操作的字数。
      然后DMA就可以获取总线的控制权, 将一大块内存和外部I/O读入或写出。
      等操作完成后, 再通知CPU。释放总线控制权。

   缺点是:
   系统总线也是一种资源，DMA操作期间，当处理器需要访问总线时，执行速度会变慢。
   但是总得来说，DMA是一种高效传输方式。

3. DMA工作原理

   ​	DMA传输将数据从一个地址空间复制到另外一个地址空间。当CPU 初始化这个传输动作，传输动作本身是由DMA控制器来实行和完成。典型的例子就是移动一个外部内存的区块到芯片内部更快的内存区。像是这样的操作并没有让[处理器](https://baike.baidu.com/item/处理器)工作拖延，反而可以被重新排程去处理其他的工作。DMA传输对于高效能嵌入式系统算法和网络是很重要的。

   ​	在实现DMA传输时，是由<u>DMA控制器直接掌管总线</u>，因此，存在着一个总线控制权转移问题。即DMA传输前，CPU要把总线控制权交给DMA控制器，而在结束DMA传输后，DMA控制器应立即把总线控制权再交回给CPU。**一个完整的DMA传输过程必须经过DMA请求、DMA响应、DMA传输、DMA结束4个步骤**。

4. DMA传输4步骤

   1. 请求

      CPU对DMA控制器初始化，并向[I/O接口](https://baike.baidu.com/item/I%2FO接口)发出操作命令，I/O接口提出DMA请求。

   2. 响应

      DMA控制器对DMA请求判别优先级及屏蔽，向总线裁决逻辑提出总线请求。当CPU执行完当前总线周期即可释放总线控制权。此时，总线裁决逻辑输出总线应答，表示DMA已经响应，通过DMA控制器通知I/O接口开始DMA传输。

   3. 传输

      **DMA控制器获得总线控制权后，CPU即刻挂起或只执行内部操作，由DMA控制器输出读写命令，直接控制RAM与I/O接口进行DMA传输**。

      在DMA控制器的控制下，在存储器和外部设备之间直接进行数据传送，在传送过程中不需要中央处理器的参与。开始时需提供要传送的数据的起始位置和数据长度。

   4. 结束

      当完成规定的成批数据传送后，DMA控制器即释放总线控制权，并向I/O接口发出结束信号。当I/O接口收到结束信号后，一方面停止I/O设备的工作，另一方面向CPU提出中断请求，使CPU从不介入的状态解脱，并执行一段检查本次DMA传输操作正确性的代码。最后，带着本次操作结果及状态继续执行原来的程序。

   ​	由此可见，DMA传输方式无需CPU直接控制传输，也没有中断处理方式那样保留现场和恢复现场的过程，通过硬件为RAM与I/O设备开辟一条直接传送数据的通路，使CPU的效率大为提高。

5. DMA传送方式（主要考虑占用总线的策略）

   ​	DMA技术的出现，使得外围设备可以通过DMA控制器直接访问内存，与此同时，CPU可以继续执行程序。那么DMA控制器与CPU怎样分时使用内存呢（即什么时候谁可以占用总线）?通常采用以下三种方法：(1)停止CPU访问内存；(2)周期挪用；(3)DMA与CPU交替访问内存。

   1. 停止CPU访问内存

      ​	<u>当外围设备要求传送一批数据时，由DMA控制器发一个停止信号给CPU，要求CPU放弃对地址总线、数据总线和有关控制总线的使用权</u>。DMA控制器获得总线控制权以后，开始进行数据传送。在一批数据传送完毕后，DMA控制器通知CPU可以使用内存，并把总线控制权交还给CPU。图(a)是这种传送方式的时间图。很显然，在这种DMA传送过程中，CPU基本处于不工作状态或者说保持状态。

      ![img](https://bkimg.cdn.bcebos.com/pic/f636afc379310a55c5956fa9b74543a98226103d?x-bce-process=image/watermark,image_d2F0ZXIvYmFpa2U4MA==,g_7,xp_5,yp_5)

      + 优点

        ​	控制简单，它**适用于数据传输率很高的设备**进行成组传送。

      + 缺点

        ​	在DMA控制器访问内存阶段，**内存的效能没有充分发挥，相当一部分内存工作周期是空闲的**。这是因为，外围设备传送两个数据之间的间隔一般总是大于内存存储周期，即使高速I/O设备也是如此。例如，软盘读出一个8位二进制数大约需要32us，而半导体内存的存储周期小于0.5us，因此许多空闲的存储周期不能被CPU利用。

   2. 周期挪用

      ​	当I/O设备没有DMA请求时，CPU按程序要求访问内存；<u>一旦I/O设备有DMA请求，则由I/O设备挪用一个或几个内存周期。</u>

      这种传送方式的时间图如下图(b)：

      ![img](https://bkimg.cdn.bcebos.com/pic/4bed2e738bd4b31c098f25e687d6277f9e2ff838?x-bce-process=image/watermark,image_d2F0ZXIvYmFpa2U4MA==,g_7,xp_5,yp_5)

      I/O设备要求DMA传送时可能遇到两种情况：

      1. 此时CPU不需要访内，如CPU正在执行乘法指令。由于乘法指令执行时间较长，此时I/O访内与CPU访内没有冲突，即I/O设备挪用一二个内存周期对CPU执行程序没有任何影响。
      2. **I/O设备要求访内时CPU也要求访内，这就产生了访内冲突，在这种情况下I/O设备访内优先，因为I/O访内有时间要求，前一个I/O数据必须在下一个访问请求到来之前存取完毕**。显然，在这种情况下I/O 设备挪用一二个内存周期，意味着CPU延缓了对指令的执行，或者更明确地说，在CPU执行访内指令的过程中插入DMA请求，挪用了一二个内存周期。 与停止CPU访内的DMA方法比较，周期挪用的方法既实现了I/O传送，又较好地发挥了内存和CPU的效率，是一种广泛采用的方法。但是I/O设备每一次周期挪用都有申请总线控制权、建立线控制权和归还总线控制权的过程，所以传送一个字对内存来说要占用一个周期，但对DMA控制器来说一般要2—5个内存周期(视逻辑线路的延迟而定)。因此，**周期挪用的方法适用于I/O设备读写周期大于内存存储周期的情况**。

   3. DMA与CPU交替访问内存

      ​	**如果CPU的工作周期比内存存取周期长很多，此时采用交替访内的方法可以使DMA传送和CPU同时发挥最高的效率**。

      这种传送方式的时间图如下：

      ​	![img](https://bkimg.cdn.bcebos.com/pic/95eef01f3a292df5f35b1d48bc315c6035a873d6?x-bce-process=image/watermark,image_d2F0ZXIvYmFpa2U4MA==,g_7,xp_5,yp_5)

      ​	此图是DMA与CPU交替访内的详细时间图．假设CPU工作周期为1.2us，内存存取周期小于0.6us，那么一个CPU周期可分为C1和C2两个分周期，其中C1专供DMA控制器访内，C2专供CPU访内。

      ​	**这种方式不需要总线使用权的申请、建立和归还过程，总线使用权是通过C1和C2分时制的**。CPU和DMA控制器各自有自己的访内地址寄存器、数据寄存器和读/写信号等控制寄存器。在C1周期中，如果DMA控制器有访内请求，可将地址、数据等信号送到总线上。在C2周期中，如CPU有访内请求，同样传送地址、数据等信号。事实上，对于总线，这是用C1，C2控制的一个多路[转换器](https://baike.baidu.com/item/转换器)，这种总线控制权的转移几乎不需要什么时间，所以对DMA传送来讲效率是很高的。

      ​	<u>这种传送方式又称为"透明的DMA"方式，其来由是这种DMA传送对CPU来说，如同透明的玻璃一般，没有任何感觉或影响。在透明的DMA方式下工作，CPU既不停止主程序的运行，也不进入等待状态，是一种高效率的工作方式。当然，相应的硬件逻辑也就更加复杂</u>。

      ![img](https://bkimg.cdn.bcebos.com/pic/ca1349540923dd5448e81635d109b3de9d8248d7?x-bce-process=image/watermark,image_d2F0ZXIvYmFpa2U4MA==,g_7,xp_5,yp_5)

#### 8.0.2.1 DMA和零拷贝(优质网文推荐)

##### 零拷贝技术详细解读

> [零拷贝技术详细解读（Zero Copy）](https://blog.csdn.net/weixin_44038332/article/details/108980796)	<=	优秀网文，强烈建议阅读。这里就简单贴文章中的几张图。

![在这里插入图片描述](https://img-blog.csdnimg.cn/20201009161615375.jpg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NDAzODMzMg==,size_16,color_FFFFFF,t_70#pic_center)

![在这里插入图片描述](https://img-blog.csdnimg.cn/20201009161742144.jpg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NDAzODMzMg==,size_16,color_FFFFFF,t_70#pic_center)

![在这里插入图片描述](https://img-blog.csdnimg.cn/20201009162057988.jpg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NDAzODMzMg==,size_16,color_FFFFFF,t_70#pic_center)

![在这里插入图片描述](https://img-blog.csdnimg.cn/2020100916373817.jpg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NDAzODMzMg==,size_16,color_FFFFFF,t_70#pic_center)

![在这里插入图片描述](https://img-blog.csdnimg.cn/20201009164143823.jpg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NDAzODMzMg==,size_16,color_FFFFFF,t_70#pic_center)

##### 浅析Linux中的零拷贝技术

> [浅析Linux中的零拷贝技术](https://www.jianshu.com/p/fad3339e3448)	<=	推荐阅读原本，文章很棒。这里截取里面的图片

![img](https://upload-images.jianshu.io/upload_images/272719-9b800f62a9c0e47d.PNG?imageMogr2/auto-orient/strip|imageView2/2/w/544/format/webp)

![img](https://upload-images.jianshu.io/upload_images/272719-c955c60095647d6e.PNG?imageMogr2/auto-orient/strip|imageView2/2/w/550/format/webp)

![img](https://upload-images.jianshu.io/upload_images/272719-5c49aebc85085726.PNG?imageMogr2/auto-orient/strip|imageView2/2/w/626/format/webp)

![img](https://upload-images.jianshu.io/upload_images/272719-8461cc4141c8dd45.PNG?imageMogr2/auto-orient/strip|imageView2/2/w/833/format/webp)

### 8.0.3 指令周期、CPU周期(机械周期)、时钟周期(振荡周期)

> [CPU周期也称为()；一个CPU周期包含若干个(](https://www.nowcoder.com/questionTerminal/4ae3a61a99064d7f8f2ef734af03d8c4?toCommentId=515379)
>
> CPU周期又称**机器周期**，我们常把一条指令的执行过程划分为若干个阶段（如，取指令、存储器读、存储器写等），每一阶段完成一项工作（称为一个基本操作）。完成一个基本操作所需要的时间称为机器周期。一个指令周期常由若干CPU周期构成，  一般情况下，一个机器周期由若干个S周期（**状态周期**）组成。
>
> [试述指令周期 CPU周期和时钟周期之间的关系?](https://zhidao.baidu.com/question/24579079.html) <= 下方内容出处

相互关系如下：
　　指令周期是**取出并执行**一条指令的时间，指令周期常常有若干个CPU周期（也叫机器周期），CPU周期一般由12个时钟周期组成（时钟周期通常由晶振决定）。
　　也就是说指令周期的通常大于CPU周期，指令周期的长短与执行的指令有关，有的指令需要花费更多的CPU周期。

1. 时钟周期(振荡周期)

   ​	时钟周期也称为振荡周期，定义为时钟脉冲的倒数（可以这样来理解，时钟周期就是单片机外接晶振的倒数，例如12M的晶振，它的时间周期就是1/12 us），**是计算机中最基本的、最小的时间单位**。
   ​	**在一个时钟周期内，CPU仅完成一个最基本的动作**。对于某种单片机，若采用了1MHZ的时钟频率，则时钟周期为1us；若采用4MHZ的时钟频率，则时钟周期为250us。由于时钟脉冲是计算机的基本工作脉冲，它控制着计算机的工作节奏（使计算机的每一步都统一到它的步调上来）
   ​	**在8051单片机中把一个时钟周期定义为一个节拍（用P表示），二个节拍定义为一个状态周期（用S表示）**。

2. CPU周期(机械周期)

   ​	在计算机中，为了便于管理，常把一条指令的执行过程划分为若干个阶段，每一阶段完成一项工作。例如，取指令、存储器读、存储器写等，这每一项工作称为一个基本操作。**完成一个基本操作所需要的时间称为机器周期**。
   　8051系列单片机的一个机器周期同6个S周期（状态周期）组成。前面已说过一个时钟周期定义为一个节拍（用P表示），二个节拍定义为一个状态周期（用S表示），8051单片机的机器周期由6个状态周期组成，也就是说一个机器周期=6个状态周期=12个时钟周期。

3. 指令周期

   ​	**指令周期是执行一条指令所需要的时间，一般由若干个机器周期组成**。**指令不同，所需的机器周期数也不同**。对于一些简单的的单字节指令，在取指令周期中，指令取出到指令寄存器后，立即译码执行，不再需要其它的机器周期。
   ​	对于一些比较复杂的指令，例如转移指令、乘法指令，则需要两个或者两个以上的机器周期。
   ​	**通常含一个机器周期的指令称为单周期指令，包含两个机器周期的指令称为双周期指令**。
   ​	*CC2530的每个指令周期是一个时钟，而标准的8051每个指令周期是12个时钟。*

-----

> [计算机组成原理16----CPU结构和指令周期](https://blog.csdn.net/u014106644/article/details/95199989)  <== 以下内容主要出处。其实这文章有点杂，大致看看即可，不必较真
>
> [ARM7微处理器和单片机之间的区别？](https://zhidao.baidu.com/question/153579915.html)
>
> [写Java也得了解CPU–CPU缓存](http://www.imooc.com/article/47589)

#### 处理器结构

![img](https://img-blog.csdnimg.cn/20190709152315817.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3UwMTQxMDY2NDQ=,size_16,color_FFFFFF,t_70)

![img](https://img1.sycdn.imooc.com/5b5b1c340001b0e406900359.jpg)

​	**微处理器**(MPU)通常代表一个功能强大的CPU,但不是为任何已有的特定计算目的而设计的芯片。这种芯片往往是个人计算机和高端工作站的核心CPU。

​	早期的微控制器是将一个计算机集成到一个芯片中,实现嵌入式应用，故称**单片机**(single chip microcomputer)。

​	随后,为了更好地满足控制领域的嵌入式应用,单片机中不断扩展一些满足控制要求的电路单元。**目前,单片机已广泛称作微控制器(MCU)。 也有由微处理器发展的微控制器。**

1. 单片机结构图

   ![img](https://iknow-pic.cdn.bcebos.com/91529822720e0cf32d3b3bf00c46f21fbf09aae9?x-bce-process=image/resize,m_lfit,w_600,h_800,limit_1)

2. 微处理器结构图

   ![img](https://iknow-pic.cdn.bcebos.com/0b55b319ebc4b745f46ec21dc9fc1e178b8215c6?x-bce-process=image/resize,m_lfit,w_600,h_800,limit_1)

#### CPU主要功能

+ 指令控制：控制程序顺序执行
+ 操作控制：产生完成每条指令所需的控制命令
+ 时间控制：对各种操作加以时间上控制
+ 数据加工：对于数据进行算术和逻辑运算
+ 处理中断：处理各种中断

**CPU主要由ALU，寄存器，CU以及中断系统来组成。**

![img](https://img-blog.csdnimg.cn/2019070915280876.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3UwMTQxMDY2NDQ=,size_16,color_FFFFFF,t_70)

#### CPU寄存器

主要包括**用户可见寄存器**和**控制状态寄存器**

1. 用户可见寄存器有：

   **通用寄存器** 存放操作数；作为某种寻址方式所需的专用寄存器

   **数据寄存器**  存放操作数

   **地址寄存器**  存放地址

   **条件码寄存器**  存放条件码，用作程序分支依据

2. 控制和状态寄存器有：

   **MAR** 存储器地址寄存器

   **MDR** 存储器数据寄存器

   **PC** 程序计数器 现行指令的地址

   **IR** 指令寄存器 当前欲执行的指令

   **PSW** 存放程序状态字

   **中断标记寄存器**

例，8086架构的CPU，共有14个寄存器。[8086_14个寄存器](https://blog.csdn.net/Jinxiutf666/article/details/80894656)

![img](https://img-blog.csdn.net/20180703105056497?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0ppbnhpdXRmNjY2/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)

#### 控制单元CU

提供完成计算机全部指令操作的**微操作命令序列**部件

微操作序列形成方式：组合逻辑设计 微程序设计

#### 指令周期

CPU**取出并执行一条指令所需全部时间**称为指令周期，即CPU完成一条指令的时间

![img](https://img-blog.csdnimg.cn/20190709155431690.png)

将取指和分析称为**取指周期**；指令执行阶段称为**执行周期**

![img](https://img-blog.csdnimg.cn/20190709155842390.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3UwMTQxMDY2NDQ=,size_16,color_FFFFFF,t_70)

有些指令只需要取指周期，有些指令需要取指周期和执行周期，有些需要取指周期和多个执行周期

![img](https://img-blog.csdnimg.cn/20190709160017952.png)

如果操作数为间接寻址，则需要首先取出操作数的地址，然后访存取出操作数，因此需要增加**间址周期**

![img](https://img-blog.csdnimg.cn/20190709160212813.png)

CPU采用中断方式与IO设备通信，在每条指令执行结束时罚中断查询信号，如果有中断请求，需要进入**中断周期**

**指令周期的流程**如下所示

![img](https://img-blog.csdnimg.cn/20190709160302615.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3UwMTQxMDY2NDQ=,size_16,color_FFFFFF,t_70)

CPU指令周期由取指，间址，执行和中断组成，称为CPU4个工作周期，4个工作周期都需要访存

**取指 取指令**

**间址 取地址**

**执行 取操作数**

**中断 存程序断点**

可以用4个标志触发器来标识CPU各个工作周期

![img](https://img-blog.csdnimg.cn/20190709160632859.png)

#### 指令周期的数据流

**取指周期数据流**

![img](https://img-blog.csdnimg.cn/20190709160749966.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3UwMTQxMDY2NDQ=,size_16,color_FFFFFF,t_70)

PC中存放当前待执行指令地址，送入MAR，送入**地址总线**

CU经过**控制总线**向存储器发送读操作，使MAR所指主存地址内容经**数据总线**送入MDR，送入IR

CU控制PC内容加1，形成下一条指令地址

**间址周期数据流**

![img](https://img-blog.csdnimg.cn/20190709160803970.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3UwMTQxMDY2NDQ=,size_16,color_FFFFFF,t_70)

取指周期结束，CU检查IR中内容，如果有间址操作，将Ad(MDR)中内容送入MAR，**地址总线**，CU向存储器发送读命令，将有效地址送入MDR中

**中断周期数据流**

![img](https://img-blog.csdnimg.cn/20190709160816855.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3UwMTQxMDY2NDQ=,size_16,color_FFFFFF,t_70)

CU将保存程序断点存储器特殊地址送入MAR，送到**地址总线**，CU向存储器发写命令，将PC内容送到MDR，最终将程序断点将**数据总线**存入存储器；

CU将中断服务程序入口地址送到PC，为下一个指令周期取值周期做准备。

### 8.0.4 CPU指令流水线(优质网文转载)

>[组成原理（五）CPU指令流水线](https://www.cnblogs.com/moonlord/p/6068534.html) 这篇其实感觉内容优点杂，不是很连贯。大致看看了解下就好了。下面还有一篇推荐阅读

指令执行过程分为多个阶段，每个阶段需要的资源不一样，因此可以采用流水线技术，同时执行多条指令的不同阶段。

#### 指令流水原理

可以将指令周期简单分为**取指令**和**执行指令**

取指令由取指部件完成，执行指令由执行部件完成，如果采用串行化执行，则在执行指令1时，取指部件是空闲的。

![img](https://img-blog.csdnimg.cn/20190709164731434.png)

**指令的二级流水**

![img](https://img-blog.csdnimg.cn/20190709164912929.png)

在执行指令1阶段，同时将指令2取出放入指令缓存部件，等执行部件执行指令1结束，则直接执行指令2，理想情况，流水线的效率将加倍。

仍存在的问题：

指令执行时间一般大于取值时间，因此取值阶段要等待一段时间，存放在缓冲区的指令不能立即交给执行部件；

**当遇到条件转移指令时，下一条指令是不可知的**；采用猜测法，遇到条件转移指令时，也将下一条指令取出送入指令缓冲区，如果转移没有发生则继续执行，如果转移发生，则重新取指令。

将指令处理过程进一步细分：

+ **取指FI**  从存储器取出一条指令放入指令部件缓冲区

+ **指令译码DI**  确定操作性质和操作数地址形成方式

+ **计算操作数地址CO** 计算操作有效地址

+ **取操作数FO** 从存储器中取出操作数

+ **执行指令EI**  执行指令，将结果存入目的位置

+ **写操作数WO** 将结果存入寄存器

将指令周期分为6个阶段，可以实现指令的6级流水

![img](https://img-blog.csdnimg.cn/20190709170103477.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3UwMTQxMDY2NDQ=,size_16,color_FFFFFF,t_70)

#### 影响流水线性能因素

##### **结构相关 资源相关**

不同指令争用同一部件产生资源冲突

FO取指和FI取操作数会发生访存冲突

![img](https://img-blog.csdnimg.cn/20190709170323708.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3UwMTQxMDY2NDQ=,size_16,color_FFFFFF,t_70)

解决方式：

流水线完成前一条指令对数据的存储器访问时，暂停取后一条指令

设置独立存储器存放操作数和指令

采用指令预取技术，将指令预取到指令队列中，这样取操作数操作便可以独占存储器访问

##### 数据相关

**不同指令重叠操作，可能改变对操作数的读写访问顺序**

数据相关冲突有：

![img](https://img-blog.csdnimg.cn/20190709171216892.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3UwMTQxMDY2NDQ=,size_16,color_FFFFFF,t_70)

+ 写后读相关：先写入寄存器，再读出寄存器

+ 读后写相关：先读寄存器，再写寄存器

+ 写后写相关：指令1先写寄存器，指令2再写寄存器

解决方式：

后推法 遇到数据相关时，停顿后继指令运行，直到前面指令结果已经生成

定向技术 旁路技术  将某条指令执行结果不送回寄存器而是直接送到其他指令所需的地方

##### 控制相关

由转移指令引起。若是**条件转移**指令，则转移目标地址可能发生跳转，影响流水线效率。

> ​     ![img](https://img-blog.csdnimg.cn/20190709171820775.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3UwMTQxMDY2NDQ=,size_16,color_FFFFFF,t_70)   ![img](https://img-blog.csdnimg.cn/20190709171836838.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3UwMTQxMDY2NDQ=,size_16,color_FFFFFF,t_70)

解决方式：

尽早判别转移是否发生，尽早生成转移目标地址

**预取转移成功和不成功两个方向的目标指令**

加快和提前形成条件码

#### 流水线性能

##### 吞吐率 

单位时间内流水线完成指令或输出结果数量

最大吞吐率：流水线在连续流动达到稳定状态后所得吞吐率 

m段流水线各段时间均为t

![img](https://img-blog.csdnimg.cn/20190710071645412.png)

实际吞吐率：流水线完成n条指令实际吞吐率

![img](https://img-blog.csdnimg.cn/20190710071739600.png)

##### 加速比 

m段流水线速度与等功能非流水线速度之比

![img](https://img-blog.csdnimg.cn/20190710071921994.png)

##### 效率

 流水线中各功能段利用率

![img](https://img-blog.csdnimg.cn/20190710072107565.png)

流水线效率为流水线各段处于工作时间时空区和流水线中各段总时空区之比，如图

![img](https://img-blog.csdnimg.cn/20190710072230849.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3UwMTQxMDY2NDQ=,size_16,color_FFFFFF,t_70)

#### 流水线多发技术

假定指令周期分为四个阶段：取指FI 译码ID 执行EX 回写WR

##### 超标量技术

在每个时钟周期内同时并发多条独立指令

![img](https://img-blog.csdnimg.cn/20190710072601230.png)

处理机中配置多个功能部件和指令译码电路，多个寄存器端口和总线；编译程序决定哪几条相邻指令可并行执行

例如  三条指令是相互独立的，可以并行执行

MOV BL,8  

ADD AX,1756H  

ADD CL,4EH

##### 超流水线技术

在一个时钟周期内再分段，一个功能部件使用多次

![img](https://img-blog.csdnimg.cn/20190710072621618.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3UwMTQxMDY2NDQ=,size_16,color_FFFFFF,t_70)

##### 超长指令字技术 

由编译程序挖掘出指令间潜在的并行性，将多条能并行操作的指令组合成一条具有多个操作码字段的超长指令字

![img](https://img-blog.csdnimg.cn/20190710072636703.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3UwMTQxMDY2NDQ=,size_16,color_FFFFFF,t_70)

#### 流水线结构

##### 指令流水线结构

将指令执行阶段分为取指 指令译码 地址形成 取操作数 执行指令 回写结果几个阶段，对应相应结构如下

![img](https://img-blog.csdnimg.cn/20190710073524128.png)

##### 运算流水线结构

将浮点加法运算分成对阶 尾数加 结果规格化 3个阶段

![img](https://img-blog.csdnimg.cn/20190710073537671.png)

---

>  [计算机组成原理17----CPU指令流水线](https://blog.csdn.net/u014106644/article/details/95209474) <== 图文并茂，不亚于上面这篇。建议阅读（下面只摘取部分内容）
>
> **流水线使得原先有先后顺序的指令同时处理，当出现某些指令组合时，可能会导致使用了错误的数据**。
>
> **乱序执行**
> 指令在执行时，常常因为一些限制而等待。例如，MEM阶段访问的数据不在cache中，需要从外部存储器获取，这个动作需要几十个cycle，如果顺序执行，后面的指令MEM都要等待这个指令操作完成。乱序执行是说，**先执行后面不依赖该数据的指令**。
>
> **指令相关性**
>
> 1. **寄存器相关**：当2条指令**公用寄存器**时，他们就有可能相关。
> 2. **控制相关**：前一条指令是跳转指令，而后一条指令的执行需要跳转指令的结果，这就是控制相关
>
> **去除数据相关**
>
> <u>去除数据相关的动作不是由cpu进行的，而是由**编译器**和程序员进行处理的</u>
>
> **去除控制相关**
> **投机执行**：cpu会根据跳转预测的结果，可能会提前把跳转后的指令放到跳转指令前面执行，是一种预测的投机行为。现代分支预测的准确性能达到98%以上，所以可以一定程度上去除控制相关。
>
> **CPU如何进行乱序执行**
>
> 指令结果顺序提交
> （1）指令执行顺序虽然是乱序的，但是**指令结果的提交顺序一定要是顺序的。因为“精确中断”的存在**
> （2）精确中断：指令执行过程中，来了一个中断，此时cpu要将ISA寄存器压栈，执行中断服务程序，然后执行中断后面的指令。而精确中断要求终端钱的指令全部执行，**中断后的指令一个都不执行**。而**在乱序执行内核中，终端后面的指令可能放在中断前面的指令执行**
> （3）所以，cpu引入**重排序缓冲区**，用来缓冲指令的执行结果，这些结果会被顺序的提交到寄存器中，来实现精确中断。
>
> **线程并行**
>
> （1）**软件多线程：时分复用操作系统**
> （2）硬件多线程
>     (a) 粗粒度硬件多线程：
>        当处理器发现一个线程被长时间阻塞，eg：cache miss，发射器就发射另一个线程的指令
>     (b) 细粒度硬件多线程
>        处理器每个cycle发送不同线程的指令
>     (c) 同时多线程
>        超标量处理器同一时间可以发送多条指令，这些指令来自于不同线程
> （3）多核处理器架构
>
> ![image_1b1ul4hrk171r17j1nm81qed1dpb1t.png-110.2kB](http://static.zybuluo.com/lj72808up/h9o6h2jlq980uhreicbli4l6/image_1b1ul4hrk171r17j1nm81qed1dpb1t.png)

### 8.0.5 NUMA

​	UMA下，多核CPU通过同一个总线访问内存，如果访问同一个区域，就需要考虑同步问题，高频访问情况下，整体工作速度被总线访问内存的环节拖慢。

​	为此，出现NUMA，拆分内存给CPU核（可能每2个CPU核共享某一块内存，比如共享总的1/4内存之类的）。这种场景下，操作系统尽可能分配CPU核独享的内存给CPU使用；而访问跨不同CPU核管辖的内存时，速度会有所下降。可以理解为NUMA把总线再细分几个小总线，每个小总线上分配一部分内存供该小总线上的多个CPU使用，然后跨小总线的内存访问，需要再经过大总线，所以慢一点。

​	实际上，最影响NUMA下CPU和内存工作性能的，并非不同Node之间的QPI（Quick Path Interconnect），主要是内存分配不均匀的问题（操作系统通常优先在请求线程当前所处的CPU的Local内存上分配空间）。Linux服务器大多数workload分布是随机的，每个线程在处理逻辑时，往往需要访问的物理内存是随机分布的，而NUMA默认的内存分配模式优先分配内存在当前线程所在的CPU核上，使得Remote Access触发频繁。

---

#### 1. Non-uniform memory access -- wiki

> [Non-uniform memory access -- wiki](https://en.wikipedia.org/wiki/Non-uniform_memory_access)

​	**Non-Uniform Memory Access** (**NUMA**) is a [computer memory](https://en.wikipedia.org/wiki/Computer_storage) design used in [multiprocessing](https://en.wikipedia.org/wiki/Multiprocessing), where the memory access time depends on the memory location relative to the processor. **Under NUMA, a processor can access its own [local memory](https://en.wikipedia.org/wiki/Local_memory) faster than non-local memory (memory local to another processor or memory shared between processors).** The benefits of NUMA are limited to particular workloads, notably on servers where the data is often associated strongly with certain tasks or users.[[1\]](https://en.wikipedia.org/wiki/Non-uniform_memory_access#cite_note-nyu-numa-1)

​	<small>非一致内存访问（NUMA）是一种用于多处理器的计算机内存设计，其中内存访问时间取决于相对于处理器的内存位置。在NUMA下，处理器可以比非本地内存（另一个处理器的本地内存或处理器之间共享的内存）更快地访问自己的本地内存。NUMA的好处仅限于特定的工作负载，尤其是在数据通常与特定任务或用户紧密相关的服务器上。</small>

​	NUMA architectures logically follow in scaling from [symmetric multiprocessing](https://en.wikipedia.org/wiki/Symmetric_multiprocessing) (SMP) architectures. They were developed commercially during the 1990s by [Unisys](https://en.wikipedia.org/wiki/Unisys), [Convex Computer](https://en.wikipedia.org/wiki/Convex_Computer) (later [Hewlett-Packard](https://en.wikipedia.org/wiki/Hewlett-Packard)), [Honeywell](https://en.wikipedia.org/wiki/Honeywell) Information Systems Italy (HISI) (later [Groupe Bull](https://en.wikipedia.org/wiki/Groupe_Bull)), [Silicon Graphics](https://en.wikipedia.org/wiki/Silicon_Graphics) (later [Silicon Graphics International](https://en.wikipedia.org/wiki/Silicon_Graphics_International)), [Sequent Computer Systems](https://en.wikipedia.org/wiki/Sequent_Computer_Systems) (later [IBM](https://en.wikipedia.org/wiki/IBM)), [Data General](https://en.wikipedia.org/wiki/Data_General) (later [EMC](https://en.wikipedia.org/wiki/EMC_Corporation)), and [Digital](https://en.wikipedia.org/wiki/Digital_Equipment_Corporation) (later [Compaq](https://en.wikipedia.org/wiki/Compaq), then [HP](https://en.wikipedia.org/wiki/Hewlett-Packard), now [HPE](https://en.wikipedia.org/wiki/Hewlett_Packard_Enterprise)). Techniques developed by these companies later featured in a variety of [Unix-like](https://en.wikipedia.org/wiki/Unix-like) [operating systems](https://en.wikipedia.org/wiki/Operating_system), and to an extent in [Windows NT](https://en.wikipedia.org/wiki/Windows_NT).

​	The first commercial implementation of a NUMA-based Unix system was the Symmetrical Multi Processing XPS-100 family of servers, designed by Dan Gielan of VAST Corporation for [Honeywell Information Systems](https://en.wikipedia.org/wiki/Honeywell_Information_Systems) Italy.

##### Basic concept

​	Modern CPUs operate considerably faster than the main memory they use. In the early days of computing and data processing, the CPU generally ran slower than its own memory. The performance lines of processors and memory crossed in the 1960s with the advent of the first [supercomputers](https://en.wikipedia.org/wiki/Supercomputer). Since then, CPUs increasingly have found themselves "starved for data" and having to stall while waiting for data to arrive from memory. Many supercomputer designs of the 1980s and 1990s focused on providing high-speed memory access as opposed to faster processors, allowing the computers to work on large data sets at speeds other systems could not approach.

​	<small>现代CPU的运行速度比它们使用的主内存快得多。在计算和数据处理的早期，CPU的运行速度通常比它自己的内存慢。处理器和内存的性能线在20世纪60年代随着第一台超级计算机的出现而跨越。从那时起，cpu越来越发现自己“急需数据”，在等待数据从内存中到达时不得不暂停。上世纪80年代和90年代，许多超级计算机的设计重点是提供高速内存访问，而不是更快的处理器，使计算机能够以其他系统无法达到的速度处理大型数据集。</small>

​	Limiting the number of memory accesses provided the key to extracting high performance from a modern computer. For commodity processors, this meant installing an ever-increasing amount of high-speed [cache memory](https://en.wikipedia.org/wiki/Cache_memory) and using increasingly sophisticated algorithms to avoid [cache misses](https://en.wikipedia.org/wiki/Cache_miss). But the dramatic increase in size of the operating systems and of the applications run on them has generally overwhelmed these cache-processing improvements. Multi-processor systems without NUMA make the problem considerably worse. <u>Now a system can starve several processors at the same time, notably because only one processor can access the computer's memory at a time</u>.[[2\]](https://en.wikipedia.org/wiki/Non-uniform_memory_access#cite_note-2)

​	<small>限制内存访问的数量是从现代计算机中提取高性能的关键。对于商品处理器来说，这意味着要安装数量不断增加的高速缓存内存，并使用越来越复杂的算法来避免缓存丢失。但是，操作系统和运行在操作系统上的应用程序的规模的急剧增加，通常会使这些缓存处理的改进不堪重负。没有NUMA的多处理器系统会使问题严重得多。现在一个系统可以同时耗尽多个处理器，特别是因为一次只有一个处理器可以访问计算机的内存</small>

​	**NUMA attempts to address this problem by providing separate memory for each processor, avoiding the performance hit when several processors attempt to address the same memory**. For problems involving spread data (common for [servers](https://en.wikipedia.org/wiki/Server_(computing)) and similar applications), NUMA can improve the performance over a single shared memory by a factor of roughly the number of processors (or separate memory banks).[[3\]](https://en.wikipedia.org/wiki/Non-uniform_memory_access#cite_note-acm-zmajo-3) Another approach to addressing this problem, used mainly in non-NUMA systems, is the [multi-channel memory architecture](https://en.wikipedia.org/wiki/Multi-channel_memory_architecture), in which a linear increase in the number of memory channels increases the memory access concurrency linearly.[[4\]](https://en.wikipedia.org/wiki/Non-uniform_memory_access#cite_note-4)

​	<small>NUMA试图通过为每个处理器提供单独的内存来解决这个问题，避免了当多个处理器试图寻址同一个内存时性能受到影响。对于涉及扩展数据的问题（服务器和类似应用程序很常见），NUMA可以将单个共享内存的性能提高约为处理器（或独立内存库）数量的一倍。[3]解决此问题的另一种方法是多通道内存体系结构，其中，内存通道数的线性增加线性地增加了内存访问并发性</small>

​	**Of course, not all data ends up confined to a single task, which means that more than one processor may require the same data**. <u>To handle these cases, NUMA systems include additional hardware or software to move data between memory banks</u>. This operation slows the processors attached to those banks, so the overall speed increase due to NUMA depends heavily on the nature of the running tasks.[[3\]](https://en.wikipedia.org/wiki/Non-uniform_memory_access#cite_note-acm-zmajo-3)

​	<small>当然，并非所有的数据最终都局限于单个任务，这意味着可能有多个处理器需要相同的数据。为了处理这些情况，NUMA系统包括额外的硬件或软件来在内存库之间移动数据。这种操作会减慢连接到这些库的处理器的速度，因此NUMA导致的总体速度提高很大程度上取决于运行任务的性质</small>

​	[AMD](https://en.wikipedia.org/wiki/Advanced_Micro_Devices) implemented NUMA with its [Opteron](https://en.wikipedia.org/wiki/Opteron) processor (2003), using [HyperTransport](https://en.wikipedia.org/wiki/HyperTransport). [Intel](https://en.wikipedia.org/wiki/Intel) announced NUMA compatibility for its x86 and [Itanium](https://en.wikipedia.org/wiki/Itanium) servers in late 2007 with its [Nehalem](https://en.wikipedia.org/wiki/Nehalem_(microarchitecture)) and [Tukwila](https://en.wikipedia.org/wiki/Tukwila_(processor)) CPUs.[[5\]](https://en.wikipedia.org/wiki/Non-uniform_memory_access#cite_note-5) Both Intel CPU families share a common [chipset](https://en.wikipedia.org/wiki/Chipset); the interconnection is called Intel [Quick Path Interconnect](https://en.wikipedia.org/wiki/Intel_QuickPath_Interconnect) (QPI).[[6\]](https://en.wikipedia.org/wiki/Non-uniform_memory_access#cite_note-6)

​	<small>AMD使用其Opteron处理器（2003年）使用HyperTransport实现了NUMA。英特尔在2007年底宣布，其x86和安腾服务器的Nehalem和Tukwila CPU具有NUMA兼容性。[5]这两个Intel CPU系列共享一个共同的芯片组；这种互连称为Intel Quick Path Interconnect（QPI）。</small>

![File:NUMA.svg](https://upload.wikimedia.org/wikipedia/commons/thumb/1/1f/NUMA.svg/385px-NUMA.svg.png)

##### Cache coherent NUMA (ccNUMA)

![img](https://upload.wikimedia.org/wikipedia/commons/9/95/Hwloc.png)

​	**Nearly all CPU architectures use a small amount of very fast non-shared memory known as [cache](https://en.wikipedia.org/wiki/CPU_cache) to exploit [locality of reference](https://en.wikipedia.org/wiki/Locality_of_reference) in memory accesses**. 

​	**With NUMA, maintaining [cache coherence](https://en.wikipedia.org/wiki/Cache_coherence) across shared memory has a significant overhead**. Although simpler to design and build, **non-cache-coherent** NUMA systems become prohibitively complex to program in the standard [von Neumann architecture](https://en.wikipedia.org/wiki/Von_Neumann_architecture) programming model.[[7\]](https://en.wikipedia.org/wiki/Non-uniform_memory_access#cite_note-7)

​	<small>几乎所有的CPU架构都使用少量非常快速的非共享内存（称为cache）来利用内存访问中引用的局部性。使用NUMA，在共享内存中保持缓存一致性会带来很大的开销。虽然设计和构建更简单，但非缓存一致的NUMA系统在标准von Neumann架构编程模型中编程变得非常复杂</small>

​	Typically, ccNUMA uses inter-processor communication between cache controllers to keep a consistent memory image when more than one cache stores the same memory location. **For this reason, ccNUMA may perform poorly when multiple processors attempt to access the same memory area in rapid succession**. Support for NUMA in [operating systems](https://en.wikipedia.org/wiki/Operating_system) attempts to reduce the frequency of this kind of access by allocating processors and memory in NUMA-friendly ways and by avoiding scheduling and locking algorithms that make NUMA-unfriendly accesses necessary.[[8\]](https://en.wikipedia.org/wiki/Non-uniform_memory_access#cite_note-8)

​	<small>通常，当多个缓存存储相同的内存位置时，ccNUMA使用缓存控制器之间的处理器间通信来保持一致的内存映像。因此，当多个处理器尝试快速连续访问同一内存区域时，ccNUMA可能会表现不佳。操作系统中对NUMA的支持试图通过以NUMA友好的方式分配处理器和内存以及避免调度和锁定算法来减少这种访问的频率，因为这些算法（处理器调度和锁）使得非友好的NUMA访问变得不可避免</small>

​	<u>Alternatively, cache coherency protocols such as the [MESIF protocol](https://en.wikipedia.org/wiki/MESIF_protocol) attempt to reduce the communication required to maintain cache coherency</u>. [Scalable Coherent Interface](https://en.wikipedia.org/wiki/Scalable_Coherent_Interface) (SCI) is an [IEEE](https://en.wikipedia.org/wiki/IEEE) standard defining a directory-based cache coherency protocol to avoid scalability limitations found in earlier multiprocessor systems. For example, SCI is used as the basis for the NumaConnect technology.[[9\]](https://en.wikipedia.org/wiki/Non-uniform_memory_access#cite_note-9)[[10\]](https://en.wikipedia.org/wiki/Non-uniform_memory_access#cite_note-10)

​	<small>或者，缓存一致性协议（如MESIF协议）试图减少保持缓存一致性所需的通信。可伸缩一致接口（SCI）是一个IEEE标准，定义了一个基于目录的缓存一致性协议，以避免早期多处理器系统中的可伸缩性限制。例如，SCI被用作NumConnect技术的基础。</small>

​	As of 2011, ccNUMA systems are multiprocessor systems based on the [AMD Opteron](https://en.wikipedia.org/wiki/AMD_Opteron) processor, which can be implemented without external logic, and the Intel Itanium processor, which requires the chipset to support NUMA. Examples of ccNUMA-enabled chipsets are the SGI Shub (Super hub), the Intel E8870, the [HP](https://en.wikipedia.org/wiki/Hewlett_Packard) sx2000 (used in the Integrity and Superdome servers), and those found in NEC Itanium-based systems. Earlier ccNUMA systems such as those from [Silicon Graphics](https://en.wikipedia.org/wiki/Silicon_Graphics) were based on [MIPS](https://en.wikipedia.org/wiki/MIPS_architecture) processors and the [DEC](https://en.wikipedia.org/wiki/Digital_Equipment_Corporation) [Alpha 21364](https://en.wikipedia.org/wiki/Alpha_21364) (EV7) processor.

​	<small>截至2011年，ccNUMA系统是基于AMD Opteron处理器的多处理器系统，该处理器无需外部逻辑即可实现，而Intel Itanium处理器则要求芯片组支持NUMA。支持ccNUMA的芯片组有SGI Shub（超级集线器）、Intel E8870、HP sx2000（用于Integrity和Superdome服务器）以及基于NEC Itanium的系统中的芯片组。早期的ccNUMA系统，如Silicon Graphics的系统，都是基于MIPS处理器和DEC Alpha 21364（EV7）处理器的。</small>

##### NUMA vs. cluster computing

​	One can view NUMA as a tightly coupled form of [cluster computing](https://en.wikipedia.org/wiki/Cluster_computing). The addition of [virtual memory](https://en.wikipedia.org/wiki/Virtual_memory) paging to a cluster architecture can allow the implementation of NUMA entirely in software. However, the inter-node latency of software-based NUMA remains several orders of magnitude greater (slower) than that of hardware-based NUMA

​	<small>我们可以将NUMA看作是一种紧密耦合的集群计算形式。在集群体系结构中添加虚拟内存分页可以完全在软件中实现NUMA。然而，基于软件的NUMA的节点间延迟仍然比基于硬件的NUMA高（慢）几个数量级</small>

##### Software support

Since NUMA largely influences memory access performance, certain software optimizations are needed to allow scheduling threads and processes close to their in-memory data.

- [Silicon Graphics](https://en.wikipedia.org/wiki/Silicon_Graphics) [IRIX](https://en.wikipedia.org/wiki/IRIX) support for ccNUMA architecture over 1240 CPU with Origin server series.
- [Microsoft](https://en.wikipedia.org/wiki/Microsoft) [Windows 7](https://en.wikipedia.org/wiki/Windows_7) and [Windows Server 2008 R2](https://en.wikipedia.org/wiki/Windows_Server_2008_R2) added support for NUMA architecture over 64 logical cores.[[11\]](https://en.wikipedia.org/wiki/Non-uniform_memory_access#cite_note-11)
- [Java 7](https://en.wikipedia.org/wiki/Java_7) added support for NUMA-aware memory allocator and [garbage collector](https://en.wikipedia.org/wiki/Garbage_collection_(computer_science)).[[12\]](https://en.wikipedia.org/wiki/Non-uniform_memory_access#cite_note-12)
- Version 2.5 of the [Linux kernel](https://en.wikipedia.org/wiki/Linux_kernel) already contained basic NUMA support,[[13\]](https://en.wikipedia.org/wiki/Non-uniform_memory_access#cite_note-13) which was further improved in subsequent kernel releases. Version 3.8 of the Linux kernel brought a new NUMA foundation that allowed development of more efficient NUMA policies in later kernel releases.[[14\]](https://en.wikipedia.org/wiki/Non-uniform_memory_access#cite_note-14)[[15\]](https://en.wikipedia.org/wiki/Non-uniform_memory_access#cite_note-15) Version 3.13 of the Linux kernel brought numerous policies that aim at putting a process near its memory, together with the handling of cases such as having [memory pages](https://en.wikipedia.org/wiki/Memory_page) shared between processes, or the use of transparent [huge pages](https://en.wikipedia.org/wiki/Huge_page); new [sysctl](https://en.wikipedia.org/wiki/Sysctl) settings allow NUMA balancing to be enabled or disabled, as well as the configuration of various NUMA memory balancing parameters.[[16\]](https://en.wikipedia.org/wiki/Non-uniform_memory_access#cite_note-16)[[17\]](https://en.wikipedia.org/wiki/Non-uniform_memory_access#cite_note-17)[[18\]](https://en.wikipedia.org/wiki/Non-uniform_memory_access#cite_note-18)
- [OpenSolaris](https://en.wikipedia.org/wiki/OpenSolaris) models NUMA architecture with lgroups.
- [FreeBSD](https://en.wikipedia.org/wiki/FreeBSD) added Initial NUMA affinity and policy configuration in version 11.0 [[19\]](https://en.wikipedia.org/wiki/Non-uniform_memory_access#cite_note-19)

#### 2. 浅解NUMA机制

> [浅解NUMA机制](https://www.jianshu.com/p/0607c5f62c51)	<=	以下内容出至该文章

##### NUMA的诞生背景

在NUMA出现之前，CPU朝着高频率的方向发展遇到了天花板，转而向着多核心的方向发展。

在一开始，内存控制器还在北桥中，所有CPU对内存的访问都要通过北桥来完成。此时所有CPU访问内存都是“一致的”，如下图所示：

![img](https://upload-images.jianshu.io/upload_images/17885431-fc041608783c5f30.png?imageMogr2/auto-orient/strip|imageView2/2/w/764/format/webp)

这样的架构称为UMA(Uniform Memory Access)，直译为“统一内存访问”，这样的架构对软件层面来说非常容易，总线模型保证所有的内存访问是一致的，即**每个处理器核心共享相同的内存地址空间**。但随着CPU核心数的增加，这样的架构难免遇到问题，比如对总线的带宽带来挑战、访问同一块内存的冲突问题。为了解决这些问题，有人搞出了NUMA。

##### NUMA构架细节

​	NUMA 全称 Non-Uniform Memory Access，译为“非一致性内存访问”。这种构架下，不同的内存器件和CPU核心从属不同的 Node，每个 Node 都有自己的集成内存控制器（IMC，Integrated Memory Controller）。

​	在 Node 内部，架构类似SMP，使用 IMC Bus 进行不同核心间的通信；不同的 Node 间通过QPI（Quick Path Interconnect）进行通信，如下图所示：

![img](https://upload-images.jianshu.io/upload_images/17885431-b2381fd6ca9d1cbb.png?imageMogr2/auto-orient/strip|imageView2/2/w/1200/format/webp)

一般来说，一个内存插槽对应一个 Node。需要注意的一个特点是，QPI的延迟要高于IMC Bus，也就是说CPU访问内存有了远近（remote/local）之别，而且实验分析来看，这个**差别非常明显**。

在Linux中，对于NUMA有以下几个需要注意的地方：

- 默认情况下，内核不会将内存页面从一个 NUMA Node 迁移到另外一个 NUMA Node；
- 但是有现成的工具可以实现将冷页面迁移到远程（Remote）的节点：NUMA Balancing；
- 关于不同 NUMA Node 上内存页面迁移的规则，社区中有依然有不少争论。

对于初次了解NUMA的人来说，了解到这里就足够了，本文的细节探讨也止步于此。

##### 上机演示

###### **NUMA Node 分配**

![img](https://upload-images.jianshu.io/upload_images/17885431-775a4749e3d84a3c.png?imageMogr2/auto-orient/strip|imageView2/2/w/680/format/webp)

作者（简书作者）使用的机器中，有两个 NUMA Node，每个节点管理16GB内存。

###### **NUMA Node 绑定**

Node 和 Node 之间进行通信的代价是不等的，同样是 Remote 节点，其代价可能不一样，这个信息在 node distances 中以一个矩阵的方式展现。

![img](https://upload-images.jianshu.io/upload_images/17885431-4c2b2c9daaaccc64.png?imageMogr2/auto-orient/strip|imageView2/2/w/610/format/webp)

我们可以将一个进程绑定在某个 CPU 或 NUMA Node 的内存上执行，如上图所示。

###### **NUMA 状态**

![img](https://upload-images.jianshu.io/upload_images/17885431-6e14f872c58c63ba.png?imageMogr2/auto-orient/strip|imageView2/2/w/465/format/webp)

#### 3. NUMA架构的CPU -- 你真的用好了么？

> [NUMA架构的CPU -- 你真的用好了么？](https://cenalulu.github.io/linux/numa/)	<=	以下内容出至该文章，文章很棒，建议阅读原文

##### 为什么要有NUMA

​	在NUMA架构出现前，CPU欢快的朝着频率越来越高的方向发展。受到物理极限的挑战，又转为核数越来越多的方向发展。如果每个core的工作性质都是share-nothing（类似于map-reduce的node节点的作业属性），那么也许就不会有NUMA。**由于所有CPU Core都是通过共享一个北桥来读取内存，随着核数如何的发展，北桥在响应时间上的性能瓶颈越来越明显**。于是，聪明的硬件设计师们，先到了把内存控制器（原本北桥中读取内存的部分）也做个拆分，平分到了每个die上。于是NUMA就出现了！

##### NUMA是什么

​	**NUMA中，虽然内存直接attach在CPU上，但是由于内存被平均分配在了各个die上。只有当CPU访问自身直接attach内存对应的物理地址时，才会有较短的响应时间（后称`Local Access`）。而如果需要访问其他CPU attach的内存的数据时，就需要通过inter-connect通道访问，响应时间就相比之前变慢了（后称`Remote Access`）。所以NUMA（Non-Uniform Memory Access）就此得名**。

![](https://cenalulu.github.io/images/linux/numa/numa.png)

##### 我们需要为NUMA做什么

假设你是Linux教父Linus，对于NUMA架构你会做哪些优化？下面这点是显而易见的：

既然CPU只有在Local-Access时响应时间才能有保障，那么我们就尽量把该CPU所要的数据集中在他local的内存中就OK啦~

没错，<u>事实上Linux识别到NUMA架构后，默认的内存分配方案就是：优先尝试在请求线程当前所处的CPU的Local内存上分配空间。如果local内存不足，优先淘汰local内存中无用的Page（Inactive，Unmapped）</u>。 那么，问题来了。。。

##### NUMA的“七宗罪”

几乎所有的运维都会多多少少被NUMA坑害过，让我们看看究竟有多少种在NUMA上栽的方式：

- [MySQL – The MySQL “swap insanity” problem and the effects of the NUMA architecture](http://blog.jcole.us/2010/09/28/mysql-swap-insanity-and-the-numa-architecture/)
- [PostgreSQL – PostgreSQL, NUMA and zone reclaim mode on linux](http://frosty-postgres.blogspot.com/2012/08/postgresql-numa-and-zone-reclaim-mode.html)
- [Oracle – Non-Uniform Memory Access (NUMA) architecture with Oracle database by examples](http://blog.yannickjaquier.com/hpux/non-uniform-memory-access-numa-architecture-with-oracle-database-by-examples.html)
- [Java – Optimizing Linux Memory Management for Low-latency / High-throughput Databases](http://engineering.linkedin.com/performance/optimizing-linux-memory-management-low-latency-high-throughput-databases)

究其原因几乎都和：“因为CPU亲和策略导致的内存分配不平均”及“NUMA Zone Claim内存回收”有关，而和数据库种类并没有直接联系。所以下文我们就拿MySQL为例，来看看重内存操作应用在NUMA架构下到底会出现什么问题。

##### MySQL在NUMA架构上会出现的问题

几乎所有`NUMA + MySQL`关键字的搜索结果都会指向：Jeremy Cole大神的两篇文章

- [The MySQL “swap insanity” problem and the effects of the NUMA architecture](http://blog.jcole.us/2010/09/28/mysql-swap-insanity-and-the-numa-architecture/)
- [A brief update on NUMA and MySQL](http://blog.jcole.us/2012/04/16/a-brief-update-on-numa-and-mysql/)

大神解释的非常详尽，有兴趣的读者可以直接看原文。博主这里做一个简单的总结：

- **CPU规模因摩尔定律指数级发展，而总线发展缓慢，导致多核CPU通过一条总线共享内存成为瓶颈**
- 于是NUMA出现了，CPU平均划分为若干个Chip（不多于4个），每个Chip有自己的内存控制器及内存插槽
- **CPU访问自己Chip上所插的内存时速度快，而访问其他CPU所关联的内存（下文称Remote Access）的速度相较慢三倍左右**
- 于是Linux内核默认使用CPU亲和的内存分配策略，使内存页尽可能的和调用线程处在同一个Core/Chip中
- **由于内存页没有动态调整策略，使得大部分内存页都集中在`CPU 0`上**
- 又因为`Reclaim`默认策略优先淘汰/Swap本Chip上的内存，使得大量有用内存被换出
- 当被换出页被访问时问题就以数据库响应时间飙高甚至阻塞的形式出现了

![](https://cenalulu.github.io/images/linux/numa/imbalance.png)

##### 解决方案

Jeremy Cole大神推荐的三个方案如下，如果想详细了解可以阅读 [原文](http://blog.jcole.us/2012/04/16/a-brief-update-on-numa-and-mysql/)

- `numactl --interleave=all`
- 在MySQL进程启动前，使用`sysctl -q -w vm.drop_caches=3`清空文件缓存所占用的空间
- Innodb在启动时，就完成整个`Innodb_buffer_pool_size`的内存分配

这三个方案也被业界普遍认可可行，同时在 [Twitter 的5.5patch](https://github.com/twitter/mysql/commit/19cf63c596c0146a72583998d138190cc285df5c) 和 [Percona 5.5 Improved NUMA Support](http://www.percona.com/doc/percona-server/5.5/performance/innodb_numa_support.html) 中作为功能被支持。

**不过这种三合一的解决方案只是减少了NUMA内存分配不均，导致的MySQL SWAP问题出现的可能性。如果当系统上其他进程，或者MySQL本身需要大量内存时，Innodb Buffer Pool的那些Page同样还是会被Swap到存储上**。于是又在这基础上出现了另外几个进阶方案

- 配置`vm.zone_reclaim_mode = 0`使得内存不足时`去remote memory分配`优先于`swap out local page`
- `echo -15 > /proc/<pid_of_mysqld>/oom_adj`调低MySQL进程被`OOM_killer`强制Kill的可能
- [memlock](http://dev.mysql.com/doc/refman/5.6/en/server-options.html#option_mysqld_memlock)
- 对MySQL使用Huge Page（黑魔法，巧用了Huge Page不会被swap的特性）

##### 重新审视问题

如果本文写到这里就这么结束了，那和搜索引擎结果中大量的Step-by-Step科普帖没什么差别。虽然我们用了各种参数调整减少了问题发生概率，那么真的就彻底解决了这个问题么？问题根源究竟是什么？让我们回过头来重新审视下这个问题：

##### NUMA Interleave真的好么？

**为什么`Interleave`的策略就解决了问题？** 借用两张 [Carrefour性能测试](https://www.cs.sfu.ca/~fedorova/papers/asplos284-dashti.pdf) 的结果图，可以看到几乎所有情况下`Interleave`模式下的程序性能都要比默认的亲和模式要高，有时甚至能高达30%。<u>究其根本原因是Linux服务器的大多数workload分布都是随机的：即每个线程在处理各个外部请求对应的逻辑时，所需要访问的内存是在物理上随机分布的</u>。而`Interleave`模式就恰恰是针对这种特性将内存page随机打散到各个CPU Core上，使得每个CPU的负载和`Remote Access`的出现频率都均匀分布。<u>相较NUMA默认的内存分配模式，死板的把内存都优先分配在线程所在Core上的做法，显然普遍适用性要强很多</u>。

![](https://cenalulu.github.io/images/mysql/numa_mysql/perf1.png)

![](https://cenalulu.github.io/images/mysql/numa_mysql/perf2.png)

也就是说，像MySQL这种外部请求随机性强，各个线程访问内存在地址上平均分布的这种应用，`Interleave`的内存分配模式相较默认模式可以带来一定程度的性能提升。 **此外 [各种](https://www.cs.sfu.ca/~fedorova/papers/asplos284-dashti.pdf) [论文](http://www.lst.inf.ethz.ch/people/alumni/zmajo/publications/11-systor.pdf) 中也都通过实验证实，真正造成程序在NUMA系统上性能瓶颈的并不是`Remote Acess`带来的响应时间损耗，而是内存的不合理分布导致`Remote Access`将inter-connect这个小水管塞满所造成的结果**。而`Interleave`恰好，把这种不合理分布情况下的Remote Access请求平均分布在了各个小水管中。所以这也是`Interleave`效果奇佳的一个原因。

那是不是简简单单的配置个`Interleave`就已经把NUMA的特性和性能发挥到了极致呢？ 答案是否定的，**目前Linux的内存分配机制在NUMA架构的CPU上还有一定的改进空间。例如：Dynamic Memory Loaction, Page Replication**。

**Dynamic Memory Relocation** 我们来想一下这个情况：MySQL的线程分为两种，用户线程（SQL执行线程）和内部线程（内部功能，如：flush，io，master等）。<u>对于用户线程来说随机性相当的强，但对于内部线程来说他们的行为以及所要访问的内存区域其实是相对固定且可以预测的</u>。如果能对于这把这部分内存集中到这些内存线程所在的core上的时候，就能减少大量`Remote Access`，潜在的提升例如Page Flush，Purge等功能的吞吐量，甚至可以提高MySQL Crash后Recovery的速度（由于recovery是单线程）。 那是否能在`Interleave`模式下，把那些明显应该聚集在一个CPU上的内存集中在一起呢？ <u>很可惜，Dynamic Memory Relocation这种技术目前只停留在理论和实验阶段</u>。我们来看下难点：要做到按照线程的行为动态的调整page在memory的分布，就势必需要做线程和内存的实时监控（profile）。对于Memory Access这种非常异常频繁的底层操作来说增加profile入口的性能损耗是极大的。在 [关于CPU Cache程序应该知道的那些事](http://cenalulu.github.io/linux/all-about-cpu-cache/)的评论中我也提到过，这个道理和为什么Linux没有全局监控CPU L1/L2 Cache命中率工具的原因是一样的。当然优化不会就此停步。上文提到的[Carrefour算法](https://www.cs.sfu.ca/~fedorova/papers/asplos284-dashti.pdf)和Linux社区的`Auto NUMA patch`都是积极的尝试。什么时候内存profile出现硬件级别，类似于CPU中 [PMU](http://en.wikipedia.org/wiki/VTune) 的功能时，动态内存规划就会展现很大的价值，甚至会作为Linux Kernel的一个内部功能来实现。到那时我们再回过头来审视这个方案的实际价值。

**Page Replication** 再来看一下这些情况：一些动态加载的库，把他们放在任何一个线程所在的CPU都会导致其他CPU上线程的执行效率下降。而这些共享数据往往读写比非常高，如果能把这些数据的副本在每个Memory Zone内都放置一份，理论上会带来较大的性能提升，同时也减少在inter-connect上出现的瓶颈。实时上，仍然是上文提到的[Carrefour](https://www.cs.sfu.ca/~fedorova/papers/asplos284-dashti.pdf)也做了这样的尝试。**由于缺乏硬件级别（如MESI协议的硬件支持）和操作系统原生级别的支持，Page Replication在数据一致性上维护的成本显得比他带来的提升更多。因此这种尝试也仅仅停留在理论阶段**。当然，如果能得到底层的大力支持，相信这个方案还是有极大的实际价值的。

##### 究竟是哪里出了问题

**NUMA的问题？** NUMA本身没有错，是CPU发展的一种必然趋势。但是NUMA的出现使得操作系统不得不关注内存访问速度不平均的问题。

**Linux Kernel内存分配策略的问题？** 分配策略的初衷是好的，为了内存更接近需要他的线程，但是没有考虑到数据库这种大规模内存使用的应用场景。同时缺乏动态调整的功能，使得这种悲剧在内存分配的那一刻就被买下了伏笔。

**数据库设计者不懂NUMA？** 数据库设计者也许从一开始就不会意识到NUMA的流行，或者甚至说提供一个透明稳定的内存访问是操作系统最基本的职责。那么在现状改变非常困难的情况下（下文会提到为什么困难）是不是作为内存使用者有义务更好的去理解使用NUMA？

##### 总结

其实无论是NUMA还是Linux Kernel，亦或是程序开发他们都没有错，只是还做得不够极致。如果NUMA在硬件级别可以提供更多低成本的profile接口；如果Linux Kernel可以使用更科学的动态调整策略；如果程序开发人员更懂NUMA，那么我们完全可以更好的发挥NUMA的性能，使得无限横向扩展CPU核数不再是一个梦想。

- [Percona NUMA aware Configuration](http://www.percona.com/doc/percona-server/5.5/performance/innodb_numa_support.html)
- [Numa system performance issues – more than just swapping to consider](http://www.scalemysql.com/blog/2014/09/05/numa-system-performance-issues-more-than-just-swapping-to-consider/)
- [MySQL Server and NUMA architectures](http://mikaelronstrom.blogspot.com/2010/12/mysql-server-and-numa-architectures.html)
- [Checking /proc/pid/numa_maps can be dangerous for mysql client connections](http://blog.wl0.org/2012/09/checking-procnuma_maps-can-be-dangerous-for-mysql-client-connections/)
- [on swapping and kernels](http://dom.as/2014/01/17/on-swapping-and-kernels/)
- [Optimizing Linux Memory Management for Low-latency / High-throughput Databases](http://engineering.linkedin.com/performance/optimizing-linux-memory-management-low-latency-high-throughput-databases)
- [Memory System Performance in a NUMA Multicore Multiprocessor](http://www.lst.inf.ethz.ch/people/alumni/zmajo/publications/11-systor.pdf)
- [A Case for NUMA-aware Contention Management on Multicore Systems](http://www.sfu.ca/~sba70/files/atc11-blagodurov.pdf)

## 8.1 调度原则

### 8.1.1 程序执行模型

执行模型：进程在CPU突发和I/O操作间交替

- 每个调度决定都是关于在下一个CPU突发时将哪个工作交给CPU
- 在时间分片机制下，进程可能在结束当前CPU计算前被迫放弃CPU

![img](https://img-blog.csdn.net/20180421104516642?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0FsYXRlYmxvb21lcg==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)

上图表达的是，单进程执行时，一开始可能初始化和执行一些进程的必要逻辑，所以CPU占用率很高，后面出现I/O操作等触发中断、请求系统调用等，就会让CPU闲置，而我们需要考虑如何调度多个进程，让CPU尽量忙，提高系统吞吐量（系统吞吐量，即一定时间内，系统能执行的进程量）

> [八、CPU调度](https://blog.csdn.net/Alatebloomer/article/details/80026519) <= 图片来源。<small>虽然已经确认了github发起的CSDN的img加载会失败，但是至少我本地还能看。</small>

### 8.1.2 CPU调度算法的评价指标

+ CPU使用率：CPU处于忙状态的时间百分比
+ 吞吐量：单位时间内完成的进程数量 
+ 周转时间：一个进程从初始化到结束，包括所有等待时间所花费的时间
+ 等待时间：进程在就绪队列中的总时间 
+ 响应时间：从一个请求被提交到产生第一次响应所花费的总时间

**吞吐量与响应时间通常是相互矛盾的，无法同时保证**。

---

人们通常希望"更快"的服务

什么是更快

- 传输文件时的<u>高带宽</u>
- 玩游戏时的<u>低延迟</u>

与水管的类比：

- 低延迟：喝水的时候想要一打开水龙头水就流出来
- 高带宽：给游泳池充水时希望从水龙头里同时流出大量的水，并且不介意是否存在延迟

---

+ 减少响应时间
  + 及时处理用户的输入请求，尽快将输出反馈给用户

- 减少平均响应时间的波动
  + 在交互系统中，可预测性比高差异低平均更重要

+ 增加吞吐量
  + 减少开销（操作系统开销，上下文切换）
  + 系统资源的高效利用（CPU，I/O设备）

+ 减少等待时间
  + 减少每个进程的等待时间

<u>CPU调度算法难以保证每个指标都达到最优，很多时候需要针对实际应用情况下做取舍，或者寻求一个尽量照顾每项指标的方案</u>。

---

+ 低延迟调度增加了交互式表现
  + 如果移动了鼠标，但是屏幕中的光标没动，用户可能会重启电脑
+ 操作系统需要保证吞吐量不受影响
  + 用户想结束长时间的编程，所以操作系统必须不时进行调度，即使存在许多交互任务

**吞吐量是操作系统的计算带宽**

**响应时间是操作系统的计算延迟**

---

处理机调度的公平性目标

- 公平的定义：保证每个进程占用相同的CPU时间，保证每个进程都等待时间的相同
- 公平通常会增加平均等待时间

*这种简单粗暴的公平方式，通常只会让操作系统最后整体吞吐量不怎么高，延迟也不怎么低。*

## 8.2 通用操作系统的调度算法

### 8.2.0 常见的调度算法

+ FCFS（先来先服务，First Come，First Served）
+ SPN（SJF）    SRT（短进程优先(短作业优先)短剩余时间优先，Shortest Process Next(Shortest Job First) Shortest Remaining Time）
+ HRRN（最高响应比优先，Highest Response Ratio Next）
+ Round Robin（轮询，使用时间片和抢占来轮流执行任务）
+ Multilevel Feedback Queues（多级反馈队列）
+ Fair Share Scheduling（公平共享调度）

从上往下，算法实现复杂度可以说是越来越复杂。

### 8.2.1 先来先服务算法(FCFS)

维护一个FIFO队列，如果进程在执行中阻塞，队列的下一个会得到CPU

举例一：

3个进程，计算时间分别为12，3，3；

![img](https://img-blog.csdn.net/20180425195518637?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0FsYXRlYmxvb21lcg==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)

---

优点：

+ 简单 

缺点：

- 平均等待时间波动较大
- 花费时间少的任务可能排在花费时间长的任务后面
- 可能导致I/O和CPU之间的重叠处理
  - CPU密集型进程导致I/O设备闲置时，I/O密集型进程也在等待

> [八、CPU调度](https://blog.csdn.net/Alatebloomer/article/details/80026519) <= 同样图片来源

### 8.2.2 短进程优先算法(SPN、SRT)

选择下一个最短的进程（短任务优先），按照预期的完成时间来将任务入队（具体还分非抢占式SPN或叫SJF，和抢占式SRT）

![img](https://img-blog.csdn.net/20180421112942764?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0FsYXRlYmxvb21lcg==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)

- SPN算法的可抢占改进就是最短剩余时间优先算法（shortest remaining time,SRT）
- 短进程优先算法具有最优平均周转时间（c表示进程的执行时间）

![img](https://img-blog.csdn.net/20180421113757852?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0FsYXRlYmxvb21lcg==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)

---

缺点：

- 连续的短进程流会使长进程无法获得CPU资源，使长任务饥饿
- 短任务可用时的任何长任务的CPU时间都会增加平均等待时间

  **需要预知未来**

- 如何预估下一个CPU计算的持续时间
- 简单的解决办法：询问用户
- 如何用户欺骗就杀死相应进程
- 如果用户不知道怎么办

大意就是，如果有短进程一直加入就绪队列，那么长进程就永远得不到执行。则样子也违反了公平性原则。

---

![img](E:\ITstudy\MD_notes\docsify\docs\study\LeetCode_Study\做题笔记\70)

​	虽然没法真正预知未来，但是可以通过算法，来预估进程所需的执行时间。

![img](https://img-blog.csdn.net/20180421134622445?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0FsYXRlYmxvb21lcg==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)

​	根据过去，预估未来。

> [八、CPU调度](https://blog.csdn.net/Alatebloomer/article/details/80026519)

### 8.2.3 最高响应比优先算法(HRRN)

- 在短进程优先算法（SPN）的基础上改进
- 不可抢占
- 关注进程的等待时间
- 防止无限期推迟

$$
Ｒ＝（ｗ+s)/s
$$

```none
w: 等待时间(waiting time)
s: 执行时间(service time) => 需要预估
```

*R越大，说明进程等待时间越长。而我们需要优先执行R较大的进程，即尽量选择进程响应时间短的先执行。（这样能让每个进程平均下来的等待时间尽量短）。*

*最高响应比优先算法，本身设计时不考虑是否抢占，但是完全可以人为实现成抢占式的。*

*相对短任务优先（SPN和SRT），还综合考虑了进程的等待时间。*

---

优点：防止无限期延迟即进程饥饿 ,交互性、响应性更好

缺点：目前该算法不可抢占，对抢占性的支持不够，也需要预估执行时间

> [八、CPU调度](https://blog.csdn.net/Alatebloomer/article/details/80026519)

### 8.2.4 轮询算法(RR)

- 在叫作**量子（或时间切片）**的离散单元中分配处理器
- 时间片结束时，切换到下一个准备好的进程

![img](https://img-blog.csdn.net/2018042520255525?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0FsYXRlYmxvb21lcg==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)

每个进程占有相同大小的时间片。

![img](https://img-blog.csdn.net/20180421140350873?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0FsYXRlYmxvb21lcg==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)

可以看出来，使用轮询算法（RR），进程的平均等待时间还是挺长的。

RR开销：额外的上下文切换

+ 时间量子太大
  + 等待时间过长
  + 极限情况退化成FCFS

+ 时间量子太小
  + 反应迅速，但需要进行多次上下文切换
  + 吞吐量由于大量的上下文切换开销收到影响

目标：选择一个合适的量子

经验规则：维持上下文切换开销处于1%以内

*早期硬件不大行，Linux设置成1/100秒作为时间片大小。现在随着硬件升级，往往设置1/1000秒或更短的时间作为时间片大小*

---

![img](https://img-blog.csdn.net/20180421141415773?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0FsYXRlYmxvb21lcg==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)

Round Robin算法， 和前面的FCFS先来先服务算法比较，让小任务先执行（变成SPN）的FCFS的平均等待时间比RR算法短。

RR算法虽然考虑了一定的公平性（指每个进程使用相同大小的时间片），但是作为代价需要频繁上下文切换等操作，使得各个进程平均等待时间增加。

> [八、CPU调度](https://blog.csdn.net/Alatebloomer/article/details/80026519)

### 8.2.5 多级队列调度算法(MLFQ)

- 就绪队列被划分成多个独立的子队列：例如前台（交互），后台（批处理）
- 每个队列拥有自己的调度策略：例如 前台（RR），后台（FCFS）
- 调度必须在队列间进行
  + 固定优先级
    + 先处理前台，然后处理后台
    + 可能导致饥饿
  + 时间接片
    + 每个队列都得到一个确定的能够调度其进程的CPU总时间
    + 如：80%CPU时间用于RR前台，20%CPU时间用于FCFS后台

问题：不同优先级进程在一开始就划分好了级别，在进程动态执行过程中有可能会变化，如在某一阶段可能侧重交互性，在某阶段可能要做大量计算处理，能否动态调整进程在队列中的级别？ 

- 一个进程可在不同队列间移动

- 例如：N级优先级——优先级调度在所有队列中，每个级别内部RR轮循 

  + 时间量子大小随优先级级别增加而增加

  + 如果进程在当前的时间量子中没有完成，则降到下一个优先级

![img](https://img-blog.csdn.net/20180421143946149?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0FsYXRlYmxvb21lcg==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)

土话就是，让很快可以处理好的进程优先级高一点；那种计算量本来就很大，需要长时间占用CPU资源的进程，就让它优先级第一点（反正本来就要很久，再多等一会也没差）

> [八、CPU调度](https://blog.csdn.net/Alatebloomer/article/details/80026519)

### 8.2.6 公平共享调度算法(FSS)

FSS控制用户对系统资源的访问

- 一些用户组比其他用户组更重要
- 保证不重要的组无法垄断资源

+ 未使用的资源按每个组分配的资源的比例来分配

+ 没有达到资源使用率目标的组获得更高的优先级

![img](https://img-blog.csdn.net/20180421144438935?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0FsYXRlYmxvb21lcg==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)

​	前面的算法或多或少对公平性有一定考虑，但是没有把公平性当作重要指标。

​	FSS主要从用户的角度考虑公平，而不是从进程的角度。操作系统中，每个用户权限不同，能够启动的进程数量自然不相同。FSS算法希望用户即时运行少量的进程，也不会比运行超多个进程的用户少很多程序被执行的机会，即尽量保证每个用户都能占用CPU，避免CPU被某些用户长时间独占的情况。

评价调度方法：

- 确定性建模

  确定一个工作量，然后计算每个算法的表现

- 队列模型

  用来处理随机工作负载的数学方法

- 实现/模拟

  + 建立一个允许算法运行实际数据的系统
  + 最灵活/具有一般性

>  [八、CPU调度](https://blog.csdn.net/Alatebloomer/article/details/80026519)

### 8.2.7 通用操作系统调度算法-总结

+ 先来先服务算法（FCFS）

  不公平，平均等待时间较差

+ 短进程优先算法（SPN/SRT）

  + 不公平，平均周转时间最小
  + 需要精确预测计算时间
  + 可能导致饥饿

+ 最高响应比优先算法(HRRN)
  + 基于SPN调度
  + 不可抢占

+ 轮循（RR）
  
+ 公平，但是平均等待时间较差
  
+ 多级反馈队列（MLFQ）
  
+ 和SPN类似
  
+ 公平共享调度（FSS）
  
  + 公平是第一要素

实际通用操作系统的调度算法比这些要复杂得多。不过实际操作系统的调度算法的特点，在上述介绍的算法中，或多或少有所体现。

## 8.3 实时系统的调度算法

### 8.3.0 实时系统

+ 定义

  正确性依赖于其**时间**和**功能**两方面的一种操作系统（确定性和可预测性是实时操作系统的最大特征）

- 性能指标

  + 时间约束的及时性（deadlines）
  + 速度和平均性能相对不重要

- 主要特性

  时间约束的可预测性

- 分类：

  + 强实时操作系统：需要保证在保证的时间内必须完成重要的任务，必须完成

  + 弱实时操作系统：要求重要进程的高优先级更高，尽量完成，并非必须

  *<small>(举例，看视频尽量希望它快速缓存并播放，偶尔掉帧也可以接受；强实时系统，比如水坝放水，如果没能及时防水，那可能导致严重洪灾)</small>*

实时系统，一般出现在火车站、工厂等控制环境中，它们往往需要某些任务在指定时间内完成。

### 8.3.1 实时任务

- 任务（工作单元）

  一次计算，一次文件读取，一次信息传递等等

- 属性

  + 取得进展所需要的资源
  + 定时参数

![img](https://img-blog.csdn.net/20180421151210313?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0FsYXRlYmxvb21lcg==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)

任务请求可以对应到进程就绪。

在相对截止时间内任意时刻结束任务，都可以接受。

绝对截止时间，就是必须保证任务（进程）在期限之前执行结束。

![img](https://img-blog.csdn.net/20180421151442934?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0FsYXRlYmxvb21lcg==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)

### 8.3.2 硬时限和软时限

硬时限（Hard deadline）

- 如果错过了最后期限，可能会导致灾难性或非常严重的后果
- 必须验证：在最坏情况下能够满足时限吗？
- **保证确定性**

软时限(Soft deadline)

- 理想情况下，时限应该被最大满足。如果有时限没有被满足，那么就相应地降低要求。
- 尽最大努力去保证

表示一个实时操作系统能够满足任务时限deadline要求

- 决定实时任务执行地顺序
- 静态优先级调度（任务执行之前，优先级已经确定）
- 动态优先级调度（在任务执行中，优先级还可能变化）

*<small>（前面介绍过的通用操作系统调度算法中的FCFS满足静态调度的特点；而FSS明显就是动态调度的策略）</small>*

![img](https://img-blog.csdn.net/20180421152531985?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0FsYXRlYmxvb21lcg==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)

>  [八、CPU调度](https://blog.csdn.net/Alatebloomer/article/details/80026519)

### 8.3.3 速率单调调度、最早期限调度

##### RM(Rate Monotonic) 速率单调调度

- 最佳静态优先级调度
- 通过**周期**安排优先级
- 周期越短优先级越高
- 执行周期最短的任务

*（执行周期越短，优先级越高）*

#####  EDF(Earliest Deadline First) 最早期限调度

- 最佳的动态优先级调度
- Deadline越早优先级越高
- 执行Deadline最早的任务

*（离Deadline越近，优先级越高）*

## 8.4 多处理器调度

### 8.4.1 概述

多处理器调度更加复杂

- 多个相同的单处理器组成一个多处理器
- 优点：负载共享

对称多处理器(SMP, Symmetric multiprocessing)调度

- 每个处理器运行自己的调度程序
- 需要在调度程序中同步<small>（调度程序对共享资源的访问需要进行同步）</small>

##### ![img](https://img-blog.csdn.net/20180421154339404?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0FsYXRlYmxvb21lcg==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70) 

对称多处理器的进程分配

静态进程分配

- 进程从开始到结束都被分配到一个固定的处理机上执行
- 每个处理机有自己的就绪队列

+ 调度开销小

+ 各处理机可能忙闲不均

动态进程分配

- 进程在执行中可分配到任意空闲处理机执行
- 所有处理机共享一个公共的就绪队列
- 调度开销大
- 各处理机的负载是均衡的

>  [八、CPU调度](https://blog.csdn.net/Alatebloomer/article/details/80026519) <= 这次内容基本都来自该文章

## 8.5 优先级反转

### 8.5.1 概述

- 可发生在任何基于优先级的可抢占的调度机制中
- 当系统内的环境强制使高优先级任务要等待低优先级任务时发生 
- 优先级反转的持续时间取决于其他不相关任务的不可预测的行为 

![img](https://img-blog.csdn.net/20180421160425856?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0FsYXRlYmxvb21lcg==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)

反转原因：T3先执行，到t2时访问共享资源，t3时T1抢占，开始执行T1，某时刻需要访问已经被T3占用的共享资源，但该资源还没有释放，所以不能继续T1开始等待，T3继续执行，t5时T2又抢占执行，此时T1受制于T2的执行时间，因为T1必须要等T3，导致T1的时间延长了，引起不稳定状态，系统重启。 

解决办法：

1.优先级继承：如果有共享资源，低优先级任务继承等于它所占的资源的最高优先级任务的优先级，当阻塞发生时资源的拥有者的优先级会自动提升，使中间优先级的不能抢占。

2.优先级天花板：资源的优先级=所有可以锁定该资源的任务中优先级最高的那个任务的优先级。 
除非优先级高于系统中所有被锁定的资源的优先级上限，否则任务在尝试执行临界区的时候会被阻塞。 
持有最高优先级上限信号量锁的任务，会继承被该锁阻塞的任务的优先级

>  [八、CPU调度](https://blog.csdn.net/Alatebloomer/article/details/80026519) <= 这次内容基本都来自该文章

# 9. 进程同步、互斥

## 9.0 背景和相关概念

### 9.0.1 进程和线程并发、并行错误

独立的进程/线程：

- 不和其他进程共享资源或状态

- 确定性：输入状态决定结果

+ 可重现：能够重现起始条件，I/O

+ 调度顺序不重要

合作进程/线程:

- 在多个进程/线程中共享状态

- 不确定性

+ 不可重现

不确定性和不可重现意味着bug可能间歇性发生。

---

进程/线程并发执行的优点：

- 优点1：共享资源

  + 一台电脑，多个用户

  + 一个银行存款余额，多台ATM机

  + 嵌入式系统（机器人控制：手臂和手的协调）

- 优点2：加速

  + I/O操作和计算可以重叠

  + 多处理器，将程序分成多个部分并行执行

- 优点3：模块化

  + 将大程序分解成小程序：以编译为例，gcc会调用cpp，cc1，cc2，as，ld

  + 使系统易于扩展

---

并发创建新进程时的标识分配

程序可以调用函数`fork()`来创建一个新的进程

- 操作系统需要分配一个新的并且唯一的进程ID

- 在内核中，这个系统调用会运行：

  `new_pid = next_pid++`   （next_pid为共享的全局变量）

- 翻译成机器指令

  `LOAD  next_pid Reg1`

  `STORE Reg1 new_pid`

  `INC Reg1`

  `STORE Reg1 next_pid`

- 假设两个进程并发执行 

  如果`next_pid`等于100，那么其中一个进程得到的ID应该是100，另一个进程的ID应该是101，`next_pid`应该增加到102

*(这里就是把C语言指令翻译成机器指令，可以得知，本来C语言的一条指令，其实对应多条机器指令。这就引出并发编程的问题)*

![img](https://img-blog.csdn.net/20180422160654749?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0FsYXRlYmxvb21lcg==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)

无论多个线程的指令序列怎么交替执行，程序都必须正常工作

- 多线程具有不确定性和不可重现的特点
- 不经过专门设计，**调试难度很高**

不确定性要求并行程序的正确性

- 先思考清楚问题，把程序的行为设计清楚
- 切忌基于着手编写代码，碰到问题再调试

上面的错误的现象称为：Race condition（竞态条件）

系统缺陷：结果依赖于并发执行或者事件的顺序/时间

- 不确定性
- 不可重复性

怎样避免竞态？

- 让指令不被打断

> [九、同步与互斥](https://blog.csdn.net/Alatebloomer/article/details/80039397)

### 9.0.2 原子操作

> [原子操作--百度百科](https://blog.csdn.net/Alatebloomer/article/details/80039397) <= 下述内容来自此
>
> "原子操作(atomic operation)是不需要synchronized"，这是多线程编程的老生常谈了。所谓原子操作是指不会被[线程调度](https://baike.baidu.com/item/线程调度/10226112)机制打断的操作；这种操作一旦开始，就一直运行到结束，中间不会有任何 context switch （切换到另一个线程）。
>
> **原子操作可以是一个步骤，也可以是多个操作步骤，但是其顺序不可以被打乱，也不可以被切割而只执行其中的一部分。**
>
> **将整个操作视作一个整体是原子性的核心特征。**

1. 简介

   ​	在[多进程](https://baike.baidu.com/item/多进程)（[线程](https://baike.baidu.com/item/线程)）访问共享资源时，能够确保所有其他的进程（线程）都不在同一时间内访问相同的资源。原子操作（atomic operation）是不需要`synchronized`，这是Java多线程编程的老生常谈了。所谓**原子操作是指不会被[线程调度](https://baike.baidu.com/item/线程调度)机制打断的操作**；这种操作一旦开始，就一直运行到结束，中间不会有任何`context switch`（切换到另一个线程）。<u>通常所说的原子操作包括对非long和double型的primitive进行赋值，以及返回这两者之外的primitive。之所以要把它们排除在外是因为它们都比较大，而JVM的设计规范又没有要求读操作和赋值操作必须是原子操作（JVM可以试着去这么作，但并不保证）</u>。

   ​	首先**处理器会自动保证基本的内存操作的原子性**。处理器保证从系统内存当中读取或者写入一个字节是原子的，意思是<u>当一个处理器读取一个字节时，其他处理器不能访问这个字节的内存地址</u>。奔腾6和最新的处理器能自动保证单处理器对同一个缓存行里进行16/32/64位的操作是原子的，但是<u>复杂的内存操作处理器不能自动保证其原子性，比如跨总线宽度，跨多个缓存行，跨页表的访问。</u>但是处理器提供**总线锁定**和**缓存锁定**两个机制来保证复杂内存操作的原子性。

2. 特性

   ​	**原子操作是不可分割的，在执行完毕之前不会被任何其它任务或事件中断**。在单处理器系统（**UniProcessor**）中，能够在单条指令中完成的操作都可以认为是"原子操作"，因为**中断只能发生于指令之间**。这也是某些CPU[指令系统](https://baike.baidu.com/item/指令系统)中引入了`test_and_set`、`test_and_clear`等指令用于[临界资源](https://baike.baidu.com/item/临界资源)互斥的原因。但是，在对称多处理器（**Symmetric Multi-Processor**）结构中就不同了，由于系统中有多个处理器在独立地运行，即使能在单条指令中完成的操作也有可能受到干扰。我们以`decl` （递减指令）为例，这是一个典型的"读－改－写"过程，涉及两次内存访问。设想在不同CPU运行的两个进程都在递减某个计数值，可能发生的情况是：

   1. CPU A(CPU A上所运行的进程，以下同）从内存单元把当前计数值⑵装载进它的寄存器中；
   2. CPU B从内存单元把当前计数值⑵装载进它的寄存器中。
   3. CPU A在它的寄存器中将计数值递减为1；
   4. CPU B在它的寄存器中将计数值递减为1；
   5. CPU A把修改后的计数值⑴写回内存单元。
   6. CPU B把修改后的计数值⑴写回内存单元。

   ​	我们看到，内存里的计数值应该是0，然而它却是1。如果该计数值是一个共享资源的[引用计数](https://baike.baidu.com/item/引用计数)，每个进程都在递减后把该值与0进行比较，从而确定是否需要释放该共享资源。这时，两个进程都去掉了对该共享资源的引用，但没有一个进程能够释放它--两个进程都推断出：计数值是1，共享资源仍然在被使用。

   ![img](https://bkimg.cdn.bcebos.com/pic/4a36acaf2edda3ccc1c022470de93901213f92b5?x-bce-process=image/watermark,image_d2F0ZXIvYmFpa2U4MA==,g_7,xp_5,yp_5)

3. 硬件支持

   ​	**原子性不可能由软件单独保证--必须需要硬件的支持**，因此是和架构相关的。**在x86 平台上，CPU提供了在指令执行期间对总线加锁的手段**。CPU芯片上有一条引线#HLOCK pin，如果汇编语言的程序中在一条指令前面加上前缀"LOCK"，经过汇编以后的机器代码就使CPU在执行这条指令的时候把#HLOCK pin的电位拉低，持续到这条指令结束时放开，从而<u>把总线锁住，这样同一总线上别的CPU就暂时不能通过总线访问内存了，保证了这条指令在多处理器环境中的原子性</u>。

4. Linux

   ​	**原子操作大部分使用汇编语言实现，因为c语言并不能实现这样的操作。**

   在x86的原子操作实现代码中，定义了LOCK宏，这个宏可以放在随后的[内联汇编](https://baike.baidu.com/item/内联汇编)指令之前。如果是SMP，LOCK宏被扩展为lock指令；否则被定义为空 -- <u>单CPU无需防止其它CPU的干扰，锁内存总线完全是在浪费时间</u>。

   ```c
   #ifdef CONFIG_SMP
   #define LOCK "lock ; "
   #else
   #define LOCK ""
   #endif
   * typedef struct { volatile int counter; } atomic_t;
   ```

   ​	在所有支持的[体系结构](https://baike.baidu.com/item/体系结构)上原子类型`atomic_t`都保存一个`int`值。在x86的某些处理器上，由于工作方式的原因，原子类型能够保证的可用范围只有24位。<u>**volatile是一个类型描述符，要求**[编译器](https://baike.baidu.com/item/编译器)不要对其描述的对象作优化处理，对它的读写都需要从内存中访问。</u>

   `#define ATOMIC_INIT(i) { (i) }`

   用于在定义原子变量时，初始化为指定的值。如：

   `static atomic_t count = ATOMIC_INIT(1)；`

   `static __inline__ void atomic_add(int i,atomic_t *v)`

   将v指向的原子变量加上`i`。该函数不关心原子变量的新值，返回void类型。

5. 相关C代码

   ```c
   #define atomic_read(v) ((v)->counter)
   读取v指向的原子变量的值。
   #define atomic_set(v,i) (((v)->counter) = (i))
   设置v指向的原子变量的值为i。
   static __inline__ void atomic_sub(int i,atomic_t *v)
   从v指向的原子变量减去i。
   static __inline__ void atomic_inc(atomic_t *v)
   递增v指向的原子变量。
   static __inline__ void atomic_dec(atomic_t *v)
   递减v指向的原子变量。
   static __inline__ int atomic_dec_and_test(atomic_t *v)
   递减v指向的原子变量，并测试是否为0。若为0，返回真，否则返回假。
   static __inline__ int atomic_inc_and_test(atomic_t *v)
   递增v指向的原子变量，并测试是否为0。若为0，返回真，否则返回假。
   static __inline__ int atomic_add_negative(int i,atomic_t *v)
   将v指向的原子变量加上i，并测试结果是否为负。若为负，返回真，否则返回假。这个操作用于实现semaphore。
   ```

---

原子操作是指一次不存在任何中断或失败的操作

- 该执行成功结束
- 或者根本没有执行
- 并且不应该发现任何部分执行的状态

实际上操作往往不是原子的

- 有些看上去是原子操作，实际上不是
- 连`x++`这样的简单语句，实际上是由3条指令构成的
- 有时候甚至连单条机器指令都不是原子的
  + `Pipiline`,`supper-scalar`,`out-of-oeder`,`page fault`

> [单核,多核CPU的原子操作](https://www.cnblogs.com/javaleon/p/4292656.html)
>
> 多核CPU的原子操作:
> 在多核CPU的时代(确实moore定律有些过时了，我们需要更多的CPU，而不是更快的CPU，无法处理快速CPU中的热量散发问题), 体系中运行着多个独立的CPU, 即使是可以在单个指令中完成的操作也可能会被干扰. 典型的例子就是decl指令(递减指令), 它细分为三个过程: "**读->改->写**", 涉及两次内存操作. 如果多个CPU运行的多个进程在同时对同一块内存执行这个指令, 那情况是无法预测的. 
>
> 硬件支持 & 多核原子操作:
> 软件级别的原子操作是依赖于硬件支持的. 在x86体系中, CPU提供了HLOCK pin引线, 允许CPU在执行某一个指令(仅仅是一个指令)时拉低HLOCK pin引线的电位, 直到这个指令执行完毕才放开.  从而锁住了总线, 如此在同一总线的CPU就暂时无法通过总线访问内存了, 这样就保证了多核处理器的原子性. (想想这机制对性能影响挺大的). 
>
> spinlock CPU同步: 
> spin lock必须基于CPU的数据总线锁定, 它通过读取一个内存单元(spinlock_t)来判断这个spinlock是否已经被别的CPU锁住. 如果否, 它写进一个特定值, 表示锁定了总线, 然后返回. 如果是, 它会重复以上操作直到成功, 或者spin次数超过一个设定值. 记住上面提及到的: 锁定数据总线的指令只能保证一个指令操作期间CPU独占数据总线. (spinlock在锁定的时侯, 不会睡眠而是会持续的尝试).

### 9.0.3 临界区、互斥、死锁、饥饿-概述

+ Critical section（临界区）

  临界区是指进程中的一段需要访问共享资源且当另一个进程处于相应代码区域时便不会被执行的代码区域。

+ Mutual exclusion（互斥）

  当一个进程处于临界区并访问共享资源时，没有其他进程会处于临界区并且访问任何相同的共享资源。

+ Dead lock（死锁）

  两个或者以上进程，在相互等待完成特定任务，而最终没法将自身任务进行下去

+ Starvation（饥饿）

  一个可执行的进程，被调度器持续忽略，以至于虽然处于可执行状态却不被执行

----

现实生活中的同步问题：

![img](https://img-blog.csdn.net/20180422171924444?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0FsYXRlYmxvb21lcg==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)

什么是"面包太多"问题的正确性质？

+ 最多有一个人去买面包

- 如果需要，有人会去买面包

可能的解决方法：

- 在冰箱上设置一个锁和钥匙（lock&key）
- 去买面包之前锁住冰箱并且拿走钥匙

+ 修复了"太多"的问题，要是有人想要果汁怎么办？

+ 可以改变"锁（lock）"的含义

+ "锁（lock）"包含"等待（waiting）"

Lock（锁）：在门、抽屉等物理上加上保护性装置，使得外人无法访问物体内的东西。只能等待解锁后才能访问。

Unlock（解锁）：打开保护性装置，使得可以访问之前被锁保护的物体类的东西

Deadlock（死锁）：A拿到锁1，B拿到锁2，A想继续拿到锁2后在继续执行，B想继续拿到锁1后再继续执行。导致A和B谁也无法继续执行。

---

使用便签来避免购买太多面包

+ 购买之前留下便签（一种"锁（lock）"）
+ 买完后移除便签（一种"解锁（unlock）"）
+ 如果存在便签就不要购买面包（在便签被移除之前一直等待）

程序样例：

```none
if(nobread){
	if(noNote){
		leave Note;
		buy bread;
		remove Note;
	}
}
```

![img](https://img-blog.csdn.net/20180422192110614?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0FsYXRlYmxvb21lcg==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)

该解决方案由于会间歇性地失败，使得问题更糟糕了。

![img](https://img-blog.csdn.net/20180422192202830?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0FsYXRlYmxvb21lcg==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)

![img](https://img-blog.csdn.net/2018042219233827?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0FsYXRlYmxvb21lcg==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)

![img](https://img-blog.csdn.net/20180422190459759?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0FsYXRlYmxvb21lcg==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)

假设我们有一些锁的实现

- `Lock.Acquire()`  在锁被释放前一直等待，然后获得锁
- `Lock.Release()` 解锁并唤醒任何等待中的进程

这些一定是原子操作——如果两个线程都在等待同一个锁，并且同时发现锁被释放了，那么只有一个能够获得锁

这样面包问题得到很好的解决：

```none
breadlock.Acquire(); // 进去临界区
if(nobread){
	buy bread; // 临界区
}
bread.Release(); // 退出临界区
```



![img](https://img-blog.csdn.net/20180422192545840?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0FsYXRlYmxvb21lcg==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)

> [九、同步与互斥](https://blog.csdn.net/Alatebloomer/article/details/80039397)

### 9.0.4 临界区

> [critical section--百度百科]([https://baike.baidu.com/item/critical%20section/7448983?fr=aladdin](https://baike.baidu.com/item/critical section/7448983?fr=aladdin))
>
> critical section是每个线程中访问[临界资源](https://baike.baidu.com/item/临界资源/1880269)的那段代码，不论是硬件临界资源，还是软件临界资源，多个[线程](https://baike.baidu.com/item/线程/103101)必须互斥地对它进行访问。
>
> [CRITICAL_SECTION的详细说明](https://www.cnblogs.com/crj8812/p/4499058.html)

临界区的特征：

- 互斥：同一时间临界区中最多存在一个线程
- Progress：如果一个线程想要进入临界区，那么它最终会成功
- **有限等待**：如果一个线程i处于入口区，那么在i的请求被接受之前，其他线程进入临界区的时间是有限制的。
- 无忙等待（可选）：如果一个进程在等待进入临界区，那么在它可以进入之前会被挂起。

---

进入临界区和离开临界区的代码

+ ENTER_CRITICAL_SECTION
+ EXIT_CRITICAL_SECTION

基本的机制

+ 禁用中断
+ 软件方法（例如，Peterson算法）
+ 更高级的抽象

比较不同的机制

+ 性能：并发级别

---

#### 方法1：禁用硬件中断

没有中断，没有上下文切换，因此没有并发

- 硬件将中断处理延迟到中断被启用之后
- 大多数现代计算机体系结构都提供指令来完成

进入临界区

+ 禁止所有中断 

离开临界区

+ 开启中断

缺点：

一旦中断被禁用后，进程无法被停止

- 整个系统都会为此停下来
- 可能导致其他进程处于饥饿状态

要是临界区可以任意长怎么办

+ 无法限制响应中断所需的时间（可能存在硬件影响）

要小心使用

*方法1，屏蔽硬件中断（尤其是时钟中断，操作系统有个时钟任务，主要用于刷新系统时间和刷新CPU状态等，这个时钟中断是周期性自动发生的，也就是CPU需要周期性响应时钟中断，这之后也可能切换别的进程再调度，而不是继续执行原来的进程）。*

*多核CPU如果仅屏蔽中断无用处，因为两个不同的CPU核上可以同时运行两个进程，还是达不到互斥的目的。*

---

#### 方法2：基于软件的解决方法

两个线程，T0和T1。（常用在并发、并行的情况下的经典例子。也可用在分布式系统中，实现互斥）

```none
Ti的通常结构
do{
		enter section 进入区域
			critical section 临界区
        exit section 离开区域
        	reminder secction 提醒区域
}while(1);
// 线程可能共享一些共有的变量来同步它们的行为
```

![img](https://img-blog.csdn.net/20180422195157966?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0FsYXRlYmxvb21lcg==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)

![img](https://img-blog.csdn.net/20180422200444138?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0FsYXRlYmxvb21lcg==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)

![img](https://img-blog.csdn.net/20180422201028588?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0FsYXRlYmxvb21lcg==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)

![img](https://img-blog.csdn.net/20180422201547634?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0FsYXRlYmxvb21lcg==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)

满足进程Pi和Pj之间互斥的经典的基于软件的解决方法（1981年）

Use two shared data items（**使用两个共享数据项**）

![img](https://img-blog.csdn.net/20180422202151154?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0FsYXRlYmxvb21lcg==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)

![img](https://img-blog.csdn.net/20180422202323462?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0FsYXRlYmxvb21lcg==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)

Dekkers算法

![img](https://img-blog.csdn.net/20180422202647774?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0FsYXRlYmxvb21lcg==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)

N线程的软件方法（Eisenberg和McGuire）

![img](https://img-blog.csdn.net/2018042220272954?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0FsYXRlYmxvb21lcg==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)

Bakery算法 

N个进程的临界区

- 进入临界区之前，进程接收一个数字
- 得到的数字最小的进入临界区
- 如果Pi和Pj得到相同的数字，那么如果i<j，Pi先进入临界区，否则Pj先进入临界区
- 编号方案总是按照枚举的增加顺序生成数字

总结：

Dekker算法（1965）：第一个针对双线程例子的正确解决方案

Bakery算法（Lamport 1979）:针对n线程的临界区问题的解决方案

复杂：需要两个进程间的共享数据项

**需要忙等待**：浪费CPU时间

没有硬件保证的情况下无真正的软件解决方案：Peterson算法需要原子的LOAD和STORE指令

*（软件层面的互斥实现，往往会出现忙等待，在某个while不断自循环直到满足条件为止。忙等待浪费CPU资源。）*

*（比较理想的还是，软硬件结合，使用硬件提供的原子操作来实现软件对应的代码。这样可以避免忙等待。）*

---

#### 方法3：更高级的抽象

硬件提供了一些原语

- 像中断禁用，原子操作指令等
- 大多数现代体系结构都这样

操作系统提供更高级的编程抽象来简化并行编程

- 例如：**锁、信号量**
- 用**硬件原语**来构建

锁是一个抽象的数据结构

- 一个二进制状态（锁定/解锁），两种方法
- `Lock::Acquire()`-锁被释放前一直等待，然后获得锁

- `Lock::Release()`-释放锁，唤醒任何等待中的进程

*<small>（获得锁，对应前面进入临界区的情况；释放锁，对应前面退出临界区的场景）</small>*

使用锁来编写临界区

```none
lock_next_pid->Acquire();
new_pid = next_pid++ ;
lock_next_pid->Release();

//显然看上去简单了不少
```

大多数现代体系结构都提供特殊的原子操作指令

- 通过特殊的内存访问电路
- 针对单处理器和多处理器

Test-and-Set 测试和置位*<small>(下面三个指令是作为一个原子操作被CPU处理的，要么3个步骤都进行，要么都不进行)</small>*

- 从内存单元中读取值
- 测试该值是否为1（然后返回真或假）
- 内存值设置为1

交换

+ 交换内存中的两个值

*（只要使用Test-and-Set或者交换，两种其中一种，就可以简洁地实现原子操作了。）*

```cpp
boolean TestAndSet (boolean *target)
{
    boolean rv = *target;
    *target = true;
    return rv:
}
// 不管之前target是0还是1，原样返回，并且之后把这个target设置成1
```

```cpp
void Exchange (boolean *a, boolean *b)
{
    boolean temp = *a;
    *a = *b;
    *b = temp:
}
```

*（`TestAndSet()`、`Exchange()`被当作原子操作哦，由硬件提供原子性支持，所以i这两个函数，可以看成是单独的一个机械指令，而CPU执行的最小单元就是机械指令。一条机械指令，要么完成，要么失败。）*

使用TS（Test-and-Set）指令实现自旋锁(spinlock)

```cpp
class Lock{
    int value = 0;
}

Lock::Acquire(){
    while(test-and-set(value))
        ; // spin
}

Lock::Release(){
    value = 0;
}

// 如果锁被释放，那么test-and-set读取0并将值设置为1→锁被设置为忙并且需要等待完成
// 如果锁出于忙状态，那么test-and-set读取1并将值设置为1→不改变锁的状态并且需要循环(自旋spin)
```

![img](https://img-blog.csdn.net/20180422212028691?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0FsYXRlYmxvb21lcg==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)

*（有人调用`Release`后，第一个自旋spin正好`test-and-set`返回值得到0，就能够执行了。）*

*（由于是原子操作，所以`test-and-set(value)`要么得到执行，要么没得到执行就切换进程，所以不可能出现有两个进程都得到0的情况。毕竟这个原子操作就相当于一条机械指令，要么被CPU完整执行，要么没执行。）*

*（可以看出来Lock上锁和释放，实现都很简单。两个进程or多个进程之间通信都可以采用这种方式。）*

上面使用`test-and-set`实现**忙等待**的锁，其实，`test-and-set`也可实现**无忙等待**的锁。*<small>（忙等待的线程在等待所释放时还是会消耗CPU周期，即还是有机会获取到占用CPU的时间分片，也就是自旋时，进程处在**就绪态**。）</small>*

![img](https://img-blog.csdn.net/20180422212115552?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0FsYXRlYmxvb21lcg==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)

```cpp
// 忙等待
class Lock{
    int value = 0;
}
Lock::Acquire(){
    while(test-and-set(value))
        ; // spin
}
Lock::Release(){
    value = 0;
}

// 无忙等待
class Lock{
    int value = 0;
    WaitQueue q;
}
Lock::Acquire(){
    while(test-and-set(value)){
        add this TCB to wait queue q;
        schedule();
    }
}
Lock::Release(){
    value = 0;
    remove one thread t from q;
    wakeup(t);
}
```

*(为了避免忙等，可以让得不到锁的进程阻塞（睡眠），把进程移动到等待队列，把CPU让出来给其他进程使用。）*

*（原本占有锁的进程释放了锁之后，唤醒阻塞队列中的一个进程。）*

**如果临界区执行时间很短，可以选择"忙等"的方式；如果临界区很长，远远大于执行上下文切换开销，则选择"无忙等"。**

使用Exchange来实现

```none
// 共享数据(初始化为0)
int lock = 0;
线程Ti
 int key;
 do{
 	key = 1;
 	while(key==1) exchange(lock,key);
 	  critical section
 	lock = 0;
 	  remainder section
 }
```

*(当某个进程key=1且执行了exchange操作后，由于初始lock为0，所以该进程获取到锁并继续执行。该进程释放锁后，其他进程中第一个成功执行了exchange的会获取到key=0，于是获得锁，能够执行；而其他慢一步的则仍然忙等)*

*（进入临界区后，lock会自动从0变成1；其他进程想访问临界区会由于lock和key都=1，一直在while循环spin自旋。）*

![img](https://img-blog.csdn.net/20180422212825771?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0FsYXRlYmxvb21lcg==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)

原子操作指令锁的特征

- 优点:

  + 适用于单处理器或者共享主存的**多处理器**中**任意数量的进程**

  + 简单并且容易证明

  + 可以用于支持多临界区

  *<small>(这种基于原始机械指令的互斥方案，现在是广泛使用的)</small>*

+ 缺点

  + 忙等待消耗处理器时间

  + 当进程离开临界区并且多个进程在等待的时候可能导致饥饿

  + 死锁

    如果一个低优先级的进程拥有临界区并且一个高优先级的进程也需求，那么高优先级进程会获得处理器并等到临界区

    *<small>(如果这个拥有锁的低优先级进程没有其他CPU内核可以用来执行，那就真死锁了。因为低优先级不是放锁，高优先级占用CPU但是又拿不到锁，拿了CPU就做个while自旋个寂寞。)</small>*

    *<small>可以通过前面`8.5 优先级反转`介绍的方式解决这类死锁问题。提高原本占有锁的进程的进程优先级到和当前所有想抢占锁的最高的一个优先级一样高；或者让资源本身具有优先级，资源总是达到和想占用它的进程中最高的一个的优先级相同，然后真正占用该资源的进程则动态调整优先级来和资源相同优先级即可。</small>*

---

#### 同步方法总结

锁是更高级的编程抽象

+ 互斥可以使用锁来实现
+ **通常需要一定等级的需要硬件支持**

常用的三种实现方法

- 禁用中断（仅限于单处理器）
- 软件方法（复杂）
- **原子操作指令（单处理器或多处理器均可）**

可选的实现内容：

- **有忙等待**
- **无忙等待**

*（原子操作命令，就是需要硬件支持的。目前对应单操作机，常采用硬件支持的原子操作指令来实现进程/线程的互斥。）*

*（需要根据临界区的特征，即临界区代码执行开销与CPU上下文切换开销作比较，来判断使用让进程忙等待但仍占用时间片，还是休眠/阻塞不占用CPU的忙等待方式）*

**有忙等待无需用户态/内核态切换来检查阻塞队列，只需要硬件提供对应的test-and-set、Exchange或其他原语即可**。

​	<u>高级语言中，有忙等待的同步(锁)机制，通常用于线程量适中、任务量较小、尽量纯CPU计算的场景;而无忙等待的同步(锁)机制，通常用于线程量大 or 任务量大 or CPU计算量少/占用率低的场景。</u>
其实就是**根据不同场景，预估"自旋"和"上下文切换"的开销总量，再使用不同的同步(锁)机制**。

> [九、同步与互斥](https://blog.csdn.net/Alatebloomer/article/details/80039397)

### 9.0.5 Linux锁(优质网文转载)

#### 个人总结

##### 注

*现在就开发而言，程序员往底层基本就只用到CPU提供的**CMPXCHG**指令，而test-and-set等一般不再由程序员显式使用，而是作为底层机械码内部实现or在CPU译码期间再由CPU优化成其他机械码指令来执行。*

*在20世纪80年代，就已有研究表明所有算法都可通过无等待wait-free形式实现，其需要借助某些手段将代码串行化，其称为universal constructions。但是强行把所有算法都串行化，往往效率偏低，即使后来不断对通用结构进行优化，效率还是比不上分块执行的代码。*

*wait-free无等待算法的研发很困难。现在常用的wait-free原语：CAS和LL/SC，在超多并发量的情况下需要占用大量内存资源，且不能很好地解决饥饿问题。*

*2011之前，高质量的wait-free算法少见。2011一堆计算机科学家，先后提出基于CAS的无等待队列的实现，其工作效率近似lock-free队列，随后又提出一系列算法能将lock-free数据结构自动升级成wait-free数据结构。所以现在有很多wait-free的数据结构可用。*

*Wait-freedom∈ Lock-freedom∈ Obstruction-freedom。Wait-freedom无阻塞，Lock-freedom开发语言层面无锁（没有多个线程竞争mutex lock，spinlock等），Obstruction-freedom要求任何仅部分完成操作可被中止，其已发生的影响/更改能够被回滚（比如通过数据结构的标记来判别一致性，决定是否保留已执行了部分的操作所导致的影响/更改。）*

*"Non-blocking" was used as a synonym for "lock-free" in the literature until the introduction of obstruction-freedom in 2003.*

##### test-and-set和Test and test-and-set

​	Linux锁实现，早期主要用的CPU提供的原子性操作test-and-set（或类似的实现形式），多核CPU为保证互斥读写CPU，需要锁总线和通知其他CPU核缓存的相同内存数据失效<small>（即要求正好缓存对应数据的其他CPU核，标记自己缓存的数据unavailable数据过期，需要重新重内存读）</small>。对应的语言层面实现的test-and-set，采取自旋spin来保证进程/线程执行同步互斥。由于语言层面的test-and-set每次都需要抢占CPU的lock<small>（根据硬件架构，可能是总线锁、缓存锁等不同形式）</small>,也需要一定的资源消耗，后来出现了优化语言层test-and-set的Test and test-and-set。

​	语言层的Test and test-and set，通过用户空间共享的内存区域<small>（比如线程共享的进程资源or进程之间申请直接内存用于共享资源）</small>保存CPU层的lock状态信息，当用户空间的lock状态信息解锁后，才让线程/进程去尝试test-and-set抢占CPU锁，而不是每次都直接申请抢占test-and-set的CPU锁。这样可以有效地减少CPU锁的抢占请求的发生。

*（打个比方，纯test-and-set时，假设1000核的CPU，原本1000线程中的一个解锁，其他999全因为原语的锁需求，CPU层的其他999核中运行的线程全都要在Cache、总线之间来回通知，看到底谁抢到锁了，消耗大。而Test and test-and-set，1000核的CPU中1个核解锁，其他999核先查看寄存器、L1、L2缓存、Cache或内存中对应的用户空间保存的CPU锁lock状态信息，一般不至于999核都正好获取到lock=false，这样就可能就1-500个CPU进行接下去的test-and-set锁抢占。而本身起初Test读用户空间lock变量无加锁，这样总体明显比test-and-set好不少。Test and test-and-set能有效减少CPU语言级别的锁申请、抢占引起的资源损耗。）*

##### load-link and store-conditional (LL/SC) 和 CAS

​	 **load-link** and **store-conditional** (**LL/SC**) 虽然比其CAS更健壮，能够预防ABA问题的发生。但是LL/SC实现更复杂。在早期的实现中，load-link执行完而未执行store-conditional操作，只要总线在这段时间内出现任何数据update操作，都会导致之后的store-conditional执行失败<small>（预测bus的update会影响load-link的数据，为避免ABA问题的粗暴方法）</small>,这种形式也被称为*weak* LL/SC。 

​	另外在设计load-store体系的解决方案时，LL/SC对比CAS有两个优势：

+ LL/SC的read和write是两个分开的原子操作，而CAS的compare和set必须作为一个原子操作执行

+ LL/SC只需两个寄存器（address、value），CAS需要三个寄存器（address、old value、new value）

  尽管如此，**支持LL/SC指令的现代CPU芯片能够在指令周期的译码阶段将CAS指令转换成LL/SC位操作。**

  （**言外之意,LL/SC要灵活使用的话比CAS更复杂，所以我们开发只要用CAS就够了，要是当前系统的CPU支持LL/SC，内部也会自动优化将CAS再转译成LL/CS操作**）

##### 事务内存、事务

​	事务内存transactional memory，作为高级的抽象，底层强制原子性、一致性、隔离性来保证指定的代码区域能够在多线程下同步互斥进行。事务，由一系列操作组成，这些操作的执行和提交仿佛与其他线程无冲突（底层保证原子性、一致性、隔离性执行这些代码片段。内部会借助锁机制来实现）。

​	尽管使用事务，避免了我们人为地加锁操作，进而降低死锁的可能性。**但是如果事务较长，容易被较小的事务影响而多次重新执行，浪费时间又耗费系统资源。**

*（所以如果事务长，需要考虑人工加锁，或者提升事务的等级=>使内部锁的粒度变粗。再者，考虑拆分事务。通常如果开发的业务中出现显眼的长事务，多半是设计问题，可以通过细化拆分事务来解决。当然如果安全性要求高的业务，比起拆分事务，提升事务等级要更合理一些。）*

​	事务内存配合CPU的L1、L2、Cache以及其他硬件，在设计上协同合作，比如Cache增加读写bit位，用于侦测事务之间对数据的读写是否冲突。尽管使用LL/SC来支持硬件层面实现的事务是可行的，但是LL/SC通常只操作当前系统一个字大小的数据，所以只支持单字事务。**比其软件实现的事务机制，硬件的事务性内存提供了最高程度的性能，但是使用起来局限性很明显。**

​	软件事务性内存Software transactional memory提供具有事务性质的内存语义，且通常只需要少量的硬件支持（比如compare-and-set机械原语，或者其他等效操作）。软件事务性内存，性能上不如硬件实现，但是适用性更好<small>（硬件的事务内存实现一般也都是针对某些特殊功能/操作进行内部优化，对程序员透明）</small>。

##### Lock

​	开发常说的Lock锁，通常指的是开发语言层面使用的lib库实现的锁。尽管每个语言的锁实现方式不同，但底层都需要依赖硬件支持。比如利用CPU架构提供的指令集中的LOCK原语，LOCK只能在特定的几个指令前使用，否则抛出异常（可利用这点故意抛出异常，在异常处理函数中执行我们自定义的代码）。对应的硬件会采取Cache锁or总线锁的形式，保证CPU处理LOCK的下一条指令前能够保证其他CPU核无法访问相同Cache数据or从内存获取相同的数据。

##### Mutex exclusion

+ 硬件实现：
  （1）单核，可以通过禁用中断实现。这样可以避免进程在执行时由于中断而切换到其他进程。虽然这很有效，但也引发问题。比如周期性触发的时钟中断信号不能被处理，将导致进程执行时，系统的时间不精确。且如果进程长时间等待或甚至死锁，将导致整个系统"瘫痪"。
  （2）忙等待，对单核or多核都适用。但是仍然存在抢占共享资源的情况，即使某进程已占有锁，其他进程无感知，仍抢占CPU资源。
  （3）其他原子操作，最常见的是CAS。CAS维护一张链表，每个结点表示需要执行的操作。在插入新节点的过程中，CAS移动指针。同一时刻只能有一个进程成功添加结点（需执行的操作），所有其他试图添加结点的进程都需要重新等待。每个进程可以保留该数据结构的副本到本地，并且在遍历该链表副本时，可以执行每个结点的操作。

+ 软件实现：
  计算机科学家提出了诸多采用忙等待实现的算法，但是需要保证指令的内存操作按序进行（需要严格要求CPU按照顺序执行代码，不能乱序）。

  再者，也可以借助操作系统的线程同步库，如果硬件提供了互斥的解决方案，操作系统线程库将内部使用硬件解决方案，否则使用软件算法解决方案。使用操作系统的锁库实现互斥时，一个线程试图获取已被占用的锁时，操作系统可以通过上下文切换将该线程挂起，并且让某一个就绪的线程开始执行，如果没有其他就绪线程时则将当前线程的优先级降低。
  大多数现代的互斥方案，通过使用队列和上下文切换来减少忙等待。但是如果使用队列和上下文切换挂起和唤醒线程的开销比忙等待大时，还是应该采用忙等待。

  *（现在CPU执行速度都很快，一般尽量使用忙等待，除非线程量实在太多or线程需执行的时间实在太长，不然尽量不要考虑采用队列和上下文切换的方案。=>上下文切换等涉及操作系统的用户态/内核态切换，这需要不少开销，这些开销与用户进程/线程自身的代码无关。）*

  纯软件算法实现的互斥算法，往往隐含前提就是在临界区执行的代码不会出错。但是一旦电路出故障等问题导致临界区代码执行出错，可能导致进程/线程出现死锁。因此还需要考虑系统崩溃时的状态恢复机制。（即，纯依靠算法实现互斥，本身是不可靠的。）

##### Futex

​	*Futex提供自旋方案、等待队列方案，内部可spin自旋，也non-block地使用阻塞队列配合等待-唤醒操作，来编写进程/线程互斥的代码。如果用得好，完全可以尽量避免上下文切换，只采用自旋，仅当线程/进程执行/阻塞过久才升级成使用阻塞队列的方案。=>java的synchronized就是采取锁升级方案完成互斥（无锁=>偏向锁=>自旋锁=>重量级锁）*

​	In [computing](https://en.wikipedia.org/wiki/Computing), a **futex** (short for "fast userspace [mutex](https://en.wikipedia.org/wiki/Mutual_exclusion)") is a [kernel](https://en.wikipedia.org/wiki/Kernel_(operating_system)) [system call](https://en.wikipedia.org/wiki/System_call) that [programmers](https://en.wikipedia.org/wiki/Programmer) can use to implement basic [locking](https://en.wikipedia.org/wiki/Lock_(computer)), or as a building block for higher-level locking abstractions such as [semaphores](https://en.wikipedia.org/wiki/Semaphore_(programming)) and [POSIX](https://en.wikipedia.org/wiki/POSIX) mutexes or [condition variables](https://en.wikipedia.org/wiki/Condition_variable).

​	**A futex consists of a [kernelspace](https://en.wikipedia.org/wiki/Kernel_(computing)) *wait queue* that is attached to an [atomic](https://en.wikipedia.org/wiki/Atomic_operations) [integer](https://en.wikipedia.org/wiki/Integer) in [userspace](https://en.wikipedia.org/wiki/Userspace)**.  Multiple [processes](https://en.wikipedia.org/wiki/Process_(computing)) or [threads](https://en.wikipedia.org/wiki/Thread_(computer_science)) operate on the integer entirely in userspace (using [atomic operations](https://en.wikipedia.org/wiki/Atomic_operation) to avoid interfering with one another), and only resort to relatively expensive [system calls](https://en.wikipedia.org/wiki/System_call) to request operations on the wait queue (for example to wake up waiting processes, or to put the current process on the wait queue).  **A  properly programmed futex-based lock will not use system calls except  when the lock is contended; since most operations do not require  arbitration between processes,  this will not happen in most cases.**

​	The `futex()` system call provides a method for waiting until a certain condition becomes true.  It is typically used as a blocking construct in the context of shared-memory synchronization.  **When using futexes, the majority of the synchronization operations are performed in user space**.  **A user-space program employs the `futex()` system call only when it is likely that the program has to block for a longer time until the condition becomes true**.  Other `futex()` operations can be used to wake any processes or threads waiting for a particular condition.

---



>  [Linux进程锁和线程锁的本质区别](https://www.cnblogs.com/wangshaowei/p/12842756.html) 以下内容转自该文章。有些内容重复了，我就缩略整理了。

#### Linux进程锁和线程锁的本质区别
线程/进程同步：多线程/进程编程中，解决共享资源冲突的问题

互斥锁和条件变量能不能同时用于线程同步和进程同步，本质上有什么区别？

+ Linux下每个进程都有自己的独立进程空间，假设A进程和B进程各有一个互斥锁，这个锁放在进程的全局静态区，那么AB进程都是无法感知对方的互斥锁的。

+ 互斥锁和条件变量出自Posix.1线程标准，它们总是可以用来同步一个进程内的各个线程的。如果一个互斥锁或者条件变量存放在多个进程共享的某个内存区中，那么Posix还允许它用在这些进程间的同步。

**线程同步和进程同步的本质区别在于锁放在哪，放在私有的进程空间还是放在多进程共享的空间，并且看锁是否具备进程共享的属性**。

下面列举了常用的同步机制，使用时需要注意线程同步和进程同步初始化的参数的不同。

+ 线程同步

  1. 互斥锁

  2. 读写锁

  3. 条件变量

  4. 信号量

  5. 自旋锁

  6. 屏障（barrier）

+ 进程同步

  1. 互斥锁
  2. 读写锁
  3. 条件变量
  4. 信号量
  5. 记录锁(record locking)
  6. 屏障（barrier）

补充:[文件锁的本质核心和原理](https://blog.csdn.net/junwua/article/details/80576433)

---

> [Test-and-set--Wiki](https://en.wikipedia.org/wiki/Test-and-set) <==挺详细的，需要翻墙，这里就直接转过来省事。

#### Test-and-set--Wiki

​	In [computer science](https://en.wikipedia.org/wiki/Computer_science), the **test-and-set** instruction is an instruction used to write 1 (set) to a memory location and return its old value as a single [atomic](https://en.wikipedia.org/wiki/Atomic_(computer_science)) (i.e., non-interruptible) operation. If multiple processes may access  the same memory location, and if a process is currently performing a  test-and-set, no other process may begin another test-and-set until the  first process's test-and-set is finished.  A [CPU](https://en.wikipedia.org/wiki/Central_processing_unit) may use a test-and-set instruction offered by another electronic component, such as [dual-port RAM](https://en.wikipedia.org/wiki/DPRAM); a CPU itself may also offer a test-and-set instruction.

​	A lock can be built using an atomic test-and-set[[1\]](https://en.wikipedia.org/wiki/Test-and-set#cite_note-1) instruction as follows:

```c
function Lock(boolean *lock) { 
    while (test_and_set(lock) == 1); 
}
```

​	The calling process obtains the lock if the old value was 0,  otherwise the while-loop spins waiting to acquire the lock. This is  called a [spinlock](https://en.wikipedia.org/wiki/Spinlock). "[Test and Test-and-set](https://en.wikipedia.org/wiki/Test_and_test-and-set)" is another example.

​	**[Maurice Herlihy](https://en.wikipedia.org/wiki/Maurice_Herlihy) (1991) proved that test-and-set has a finite [consensus number](https://en.wikipedia.org/wiki/Consensus_(computer_science)) and can solve the wait-free [consensus problem](https://en.wikipedia.org/wiki/Consensus_(computer_science)) for at-most two concurrent processes.[[2\]](https://en.wikipedia.org/wiki/Test-and-set#cite_note-2) In contrast, [compare-and-swap](https://en.wikipedia.org/wiki/Compare-and-swap) offers a more general solution to this problem.**

##### Hardware implementation of test-and-set

​	[DPRAM](https://en.wikipedia.org/wiki/DPRAM) test-and-set instructions can work in many ways.  Here are two  variations, both of which describe a DPRAM which provides exactly 2  ports, allowing 2 separate electronic components (such as 2 CPUs) access to every memory location on the DPRAM.

+ Variation 1

  ​	When CPU 1 issues a test-and-set instruction, the DPRAM first makes an  "internal note" of this by storing the address of the memory location in a special place.  If at this point, CPU 2 happens to issue a  test-and-set instruction for the same memory location, the DPRAM first  checks its "internal note", recognizes the situation, and issues a BUSY  interrupt, which tells CPU 2 that it must wait and retry.  This is an  implementation of a [busy waiting](https://en.wikipedia.org/wiki/Busy_waiting) or [spinlock](https://en.wikipedia.org/wiki/Spinlock) using the interrupt mechanism.  Since all this happens at hardware  speeds, CPU 2's wait to get out of the spin-lock is very short.

  ​	Whether or not CPU 2 was trying to access the memory location,  the DPRAM performs the test given by CPU 1.  If the test succeeds, the  DPRAM sets the memory location to the value given by CPU 1.  Then the  DPRAM wipes out its "internal note" that CPU 1 was writing there.  At  this point, CPU 2 could issue a test-and-set, which would succeed.

+ Variation 2

  ​	CPU 1 issues a test-and-set instruction to write to "memory location A".   The DPRAM does not immediately store the value in memory location A, but instead simultaneously moves the current value to a special register,  while setting the contents of memory location A to a special "flag  value".  If at this point, CPU 2 issues a test-and-set to memory  location A, the DPRAM detects the special flag value, and as in  Variation 1, issues a BUSY interrupt.

  ​	Whether or not CPU 2 was trying to access the memory location,  the DPRAM now performs CPU 1's test.  If the test succeeds, the DPRAM  sets memory location A to the value specified by CPU 1.  If the test  fails, the DPRAM copies the value back from the special register to  memory location A.  Either operation wipes out the special flag value.   If CPU 2 now issues a test-and-set, it will succeed.

##### Software implementation of test-and-set

​	Some instruction sets have an atomic test-and-set machine language instruction. Examples include [x86](https://en.wikipedia.org/wiki/X86)[[3\]](https://en.wikipedia.org/wiki/Test-and-set#cite_note-3) and [IBM System/360](https://en.wikipedia.org/wiki/IBM_System/360) and its successors (including [z/Architecture](https://en.wikipedia.org/wiki/Z/Architecture)).[[4\]](https://en.wikipedia.org/wiki/Test-and-set#cite_note-4) Those that do not can still implement an atomic test-and-set using a [read-modify-write](https://en.wikipedia.org/wiki/Read-modify-write) or [compare-and-swap](https://en.wikipedia.org/wiki/Compare-and-swap) instruction.

​	The test and set instruction, when used with boolean values, uses logic like that shown in the following function, except that the  function must execute [atomically](https://en.wikipedia.org/wiki/Atomic_(computer_science)). That is, no other process must be able to interrupt the function  mid-execution, thereby seeing a state that only exists while the  function executes. That requires hardware support; it cannot be  implemented as shown. Nevertheless, the code shown helps to explain the  behaviour of test-and-set.  NOTE: In this example, 'lock' is assumed to  be passed by reference (or by name) but the assignment to 'initial'  creates a new value (not just copying a reference).

```c
function TestAndSet(boolean_ref lock) {
    boolean initial = lock;
    lock = true;
    return initial;
}
```

​	**Not only is the code shown not atomic, in the sense of the test-and-set  instruction, it also differs from the descriptions of DPRAM hardware  test-and-set above**. <u>Here, the value being set and the test are fixed and invariant, and the value is updated regardless of the outcome of the  test, whereas for the DPRAM test-and-set, the memory is set only when  the test succeeds, and the value to set and the test condition are  specified by the CPU</u>.  Here, the value to set can only be 1, but if 0  and 1 are considered the only valid values for the memory location, and  "value is nonzero" is the only allowed test, then this equates to the  case described for DPRAM hardware (or, more specifically, the DPRAM case reduces to this under these constraints).  From that viewpoint, this  can, correctly, be called "test-and-set" in the full, conventional sense of that term.  **The essential point to note is the general intent and  principle of test-and-set: a value is both tested and set in one atomic  operation such that no other program thread or process can change the  target memory location after it is tested but before it is set.** (This is because the location must only be set if it currently has a certain  value, not if it had that value sometime earlier.)

​	In the [C programming language](https://en.wikipedia.org/wiki/C_(programming_language)), the implementation would be like：

```c
#define LOCKED 1

int TestAndSet(int* lockPtr) {
    int oldValue;

    // -- Start of atomic segment --
    // This should be interpreted as pseudocode for illustrative purposes only.
    // Traditional compilation of this code will not guarantee atomicity, the
    // use of shared memory (i.e., non-cached values), protection from compiler
    // optimizations, or other required properties.
    oldValue = *lockPtr;
    *lockPtr = LOCKED;
    // -- End of atomic segment --

    return oldValue;
}
```

​	The code also shows that there are really two operations: an atomic  read-modify-write and a test. **Only the read-modify-write needs to be  atomic.**  (This is true because delaying the value comparison by any  amount of time will not change the result of the test once the value to  test has been obtained.  Once the code writes the initial value, the  result of the test has been established, even if it has not been  computed yet — e.g., by the == operator.)

##### Mutual exclusion using test-and-set

​	One way to implement [mutual exclusion](https://en.wikipedia.org/wiki/Mutual_exclusion) is by using a test-and-set based lock as follows:

```c
 volatile int lock = 0;
 void Critical() {
     while (TestAndSet(&lock) == 1);
     critical section // only one process can be in this section at a time
     lock = 0 // release lock when finished with the critical section
 }
```

​	The lock variable is a shared variable i.e. it can be accessed by all processors/threads. **Note the *[volatile](https://en.wikipedia.org/wiki/Volatile_variable)* keyword. In absence of volatile, the compiler and/or the CPU(s) may  optimize access to lock and/or use cached values, thus rendering the  above code erroneous.** Conversely, and unfortunately, the presence of *volatile* does *not* guarantee that reads and writes are committed to memory. <u>Some compilers issue [memory barriers](https://en.wikipedia.org/wiki/Memory_barrier) to ensure that operations are committed to memory, but since the semantics of *volatile* in C/C++ is quite vague, not all compilers will do that. Consult your compiler's documentation to determine if it does.</u>

​	**This function can be called by multiple processes, but it is guaranteed that only one process will be in the [critical section](https://en.wikipedia.org/wiki/Critical_section) at a time. The rest of the processes will keep spinning until they get  the lock. It is possible that a process is never granted the lock. In  such a case it will loop endlessly. This is a drawback of this  implementation as it doesn't ensure fairness. These issues are further  elaborated in the [performance section](https://en.wikipedia.org/wiki/Test-and-set#Performance_Issues_with_test-and-set_locks).**

```assembly
enter_region:        ; A "jump to" tag; function entry point.

  tsl reg, flag      ; Test and Set Lock; flag is the
                     ; shared variable; it is copied
                     ; into the register reg and flag
                     ; then atomically set to 1.

  cmp reg, #0        ; Was flag zero on entry_region?

  jnz enter_region   ; Jump to enter_region if
                     ; reg is non-zero; i.e.,
                     ; flag was non-zero on entry.

  ret                ; Exit; i.e., flag was zero on
                     ; entry. If we get here, tsl
                     ; will have set it non-zero; thus,
                     ; we have claimed the resource
                     ; associated with flag.

leave_region:
  move flag, #0      ; store 0 in flag
  ret                ; return to caller
```

Here `tsl` is an atomic instruction and `flag` is the lock variable. The process doesn't return unless it acquires the lock.

#### Performance evaluation of test-and-set locks

​	<u>The four major evaluation metrics for locks in general are  uncontended lock-acquisition latency, bus traffic, fairness, and  storage</u>.[[7\]](https://en.wikipedia.org/wiki/Test-and-set#cite_note-Solihin-7)

​	Test-and-set scores low on two of them, namely, high bus traffic and unfairness.

​	When processor P1 has obtained a lock and processor P2 is also  waiting for the lock, P2 will keep incurring bus transactions in  attempts to acquire the lock. <u>When a processor has obtained a lock, all  other processors which also wish to obtain the same lock keep trying to  obtain the lock by initiating bus transactions repeatedly until they get hold of the lock</u>. This increases the bus traffic requirement of  test-and-set significantly. This slows down all other traffic from [cache](https://en.wikipedia.org/wiki/Cache_(computing)) and [coherence](https://en.wikipedia.org/wiki/Cache_coherence) misses. It slows down the overall section, since the traffic is saturated by failed lock acquisition attempts. **[Test-and-test-and-set](https://en.wikipedia.org/wiki/Test_and_test-and-set) is an improvement over TSL since it does not initiate lock acquisition requests continuously.**

​	When we consider fairness, we consider if a processor gets a fair chance of acquiring the lock when it is set free. In an extreme  situation the processor might starve i.e. it might not be able to  acquire the lock for an extended period of time even though it has  become free during that time.

​	**Storage overhead for TSL is next to nothing since only one lock  is required. Uncontended latency is also low since only one atomic  instruction and branch are needed.**

---

> [Fetch-and-add--wiki](https://en.wikipedia.org/wiki/Fetch-and-add)

#### Fetch-and-add--wiki

​	In [computer science](https://en.wikipedia.org/wiki/Computer_science), the **fetch-and-add** [CPU](https://en.wikipedia.org/wiki/Central_processing_unit) instruction (FAA) [atomically](https://en.wikipedia.org/wiki/Atomic_(computer_science)) increments the contents of a memory location by a specified value. 

​	That is, fetch-and-add performs the operation.

​	in such a way that if this operation is executed by one process in a [concurrent](https://en.wikipedia.org/wiki/Concurrent_computing) system, no other process will ever see an intermediate result. 

​	Fetch-and-add can be used to implement [concurrency control](https://en.wikipedia.org/wiki/Concurrency_control) structures such as [mutex locks](https://en.wikipedia.org/wiki/Mutual_exclusion) and [semaphores](https://en.wikipedia.org/wiki/Semaphore_(programming)).

##### Overview

​	The motivation for having an atomic fetch-and-add is that operations that appear in programming languages as

​	`x=x+a`

​	are not safe in a concurrent system, where multiple [processes](https://en.wikipedia.org/wiki/Process_(computing)) or [threads](https://en.wikipedia.org/wiki/Thread_(computing)) are running concurrently (either in a [multi-processor](https://en.wikipedia.org/wiki/Multi-processor) system, or [preemptively](https://en.wikipedia.org/wiki/Preemption_(computing)) scheduled onto some single-core systems). The reason is that such an  operation is actually implemented as multiple machine instructions:

1. Fetch the value at the location *x*, say *x<sub>old</sub>*, into a register;
2. add *a* to *x<sub>old</sub>* in the register;
3. store the new value of the register back into *x*.

​	When one process is doing x = x + a and another is doing x = x + b concurrently, there is a [race condition](https://en.wikipedia.org/wiki/Race_condition). They might both fetch *x<sub>old</sub>* and operate on that, then both store their results with the effect that one overwrites the other and the stored value becomes either *x<sub>old</sub>* + *a* or *x<sub>old</sub>* + *b*, not *x<sub>old</sub>* + *a* + *b* as might be expected.

​	In [uniprocessor](https://en.wikipedia.org/wiki/Uniprocessor) systems with no [kernel preemption](https://en.wikipedia.org/wiki/Kernel_preemption) supported, it is sufficient to disable [interrupts](https://en.wikipedia.org/wiki/Interrupt) before accessing a [critical section](https://en.wikipedia.org/wiki/Critical_section). However, in multiprocessor systems (even with interrupts disabled) two  or more processors could be attempting to access the same memory at the  same time. The fetch-and-add instruction allows any processor to  atomically increment a value in memory, preventing such multiple  processor collisions.

​	[Maurice Herlihy](https://en.wikipedia.org/wiki/Maurice_Herlihy) (1991) proved that fetch-and-add has a finite [consensus](https://en.wikipedia.org/wiki/Consensus_(computer_science)) number, in contrast to the [compare-and-swap](https://en.wikipedia.org/wiki/Compare-and-swap) operation. The fetch-and-add operation can solve the wait-free consensus problem for no more than two concurrent processes.[[1\]](https://en.wikipedia.org/wiki/Fetch-and-add#cite_note-1)

##### Implementation

​	The fetch-and-add instruction behaves like the following function. Crucially, the entire function is executed [atomically](https://en.wikipedia.org/wiki/Atomic_(computer_science)): no process can interrupt the function mid-execution and hence see a  state that only exists during the execution of the function. This code  only serves to help explain the behaviour of fetch-and-add; atomicity  requires explicit hardware support and hence can not be implemented as a simple high level function.

```c
<< atomic >>
function FetchAndAdd(address location, int inc) {
    int value := *location
    *location := value + inc
    return value
}
```

​	To implement a mutual exclusion lock, we define the operation FetchAndIncrement, which is equivalent to FetchAndAdd with inc=1. With this operation, a mutual exclusion lock can be implemented using the [ticket lock](https://en.wikipedia.org/wiki/Ticket_lock) algorithm as:

```c
 record locktype {
    int ticketnumber
    int turn
 }
 procedure LockInit( locktype* lock ) {
    lock.ticketnumber := 0
    lock.turn := 0
 }
 procedure Lock( locktype* lock ) {
    int myturn := FetchAndIncrement( &lock.ticketnumber ) //must be atomic, since many threads might ask for a lock at the same time
    while lock.turn ≠ myturn 
        skip // spin until lock is acquired
 }
 procedure UnLock( locktype* lock ) {
    FetchAndIncrement( &lock.turn ) //this need not be atomic, since only the possessor of the lock will execute this
 }
```

These routines provide a mutual-exclusion lock when following conditions are met:

- Locktype data structure is initialized with function LockInit before use
- Number of tasks waiting for the lock does not exceed INT_MAX at any time
- Integer datatype used in lock values can 'wrap around' when continuously incremented

##### Hardware and software support

​	An atomic fetch_add function appears in the [C++11](https://en.wikipedia.org/wiki/C%2B%2B11) standard.[[2\]](https://en.wikipedia.org/wiki/Fetch-and-add#cite_note-2) It is available as a proprietary extension to [C](https://en.wikipedia.org/wiki/C_(programming_language)) in the [Itanium](https://en.wikipedia.org/wiki/Itanium) [ABI](https://en.wikipedia.org/wiki/Application_binary_interface) specification,[[3\]](https://en.wikipedia.org/wiki/Fetch-and-add#cite_note-3) and (with the same syntax) in [GCC](https://en.wikipedia.org/wiki/GNU_Compiler_Collection).[[4\]](https://en.wikipedia.org/wiki/Fetch-and-add#cite_note-4)

###### x86 implementation

​	In the x86 architecture, the instruction ADD with the destination  operand specifying a memory location is a fetch-and-add instruction that has been there since the [8086](https://en.wikipedia.org/wiki/8086) (it just wasn't called that then), and with the LOCK prefix, is atomic  across multiple processors. However, it could not return the original  value of the memory location (though it returned some flags) until the [486](https://en.wikipedia.org/wiki/80486) introduced the XADD instruction.

​	The following is a [C](https://en.wikipedia.org/wiki/C_(programming_language)) implementation for the [GCC](https://en.wikipedia.org/wiki/GNU_Compiler_Collection) compiler, for both 32 and 64 bit x86 Intel platforms, based on extended asm syntax:

```c
static inline int fetch_and_add(int* variable, int value)
{
    __asm__ volatile("lock; xaddl %0, %1"
                     : "+r" (value), "+m" (*variable) // input+output
                     : // No input-only
                     : "memory"
                    );
    return value;
}
```

---

> [Test and test-and-set--wiki](https://en.wikipedia.org/wiki/Test_and_test-and-set)

​	**In [computer science](https://en.wikipedia.org/wiki/Computer_science), the [test-and-set](https://en.wikipedia.org/wiki/Test-and-set) CPU [instruction](https://en.wikipedia.org/wiki/Instruction_(computer_science)) is used to implement  [mutual exclusion](https://en.wikipedia.org/wiki/Mutual_exclusion) in [multiprocessor](https://en.wikipedia.org/wiki/Multiprocessor) environments. Although a correct [lock](https://en.wikipedia.org/wiki/Lock_(computer_science)) can be implemented with test-and-set, it can lead to [resource contention](https://en.wikipedia.org/wiki/Resource_contention) in busy lock (caused by bus locking and cache invalidation when test-and-set operation needs to access memory [atomically](https://en.wikipedia.org/wiki/Atomic_(computer_science))).**

​	To lower the overhead a more elaborate locking protocol **test and test-and-set** is used. The main idea is to reduce [writeback](https://en.wikipedia.org/wiki/Writeback) that can create [resource contention](https://en.wikipedia.org/wiki/Resource_contention) when two separate threads want the same lock. <u>If *n* threads are competing for the lock, they will attempt to acquire it as soon as it is released if only using [test and set](https://en.wikipedia.org/wiki/Test_and_set), causing each thread to invalidate the lock flag, meaning it must be propagated through the cache of the remaining processors *n* times, before any one thread may safely read it.</u> **By adding the *check-yield* step, only the first thread of execution to notice the lock is free will attempt to obtain it, eliminating the writeback.**

```c
boolean locked := false // shared lock variable
    procedure EnterCritical() {
    do {
        while (locked == true) yield(); // lock looks busy so yield to scheduler
    } while TestAndSet(locked) // actual atomic locking
}
procedure TestAndSet(lock) {
    boolean initial = lock;
    lock = true;
    return initial;
}
```

Exit protocol is:

```c
procedure ExitCritical() {
    locked := false
}
```

​	**The entry protocol uses normal memory reads to spin, waiting for the  lock to become free. Test-and-set is only used to try to get the lock  when normal memory read says it's free. Thus the expensive atomic memory operations happen less often than in simple spin around test-and-set.**

​	If the [programming language](https://en.wikipedia.org/wiki/Programming_language) used supports [short-circuit evaluation](https://en.wikipedia.org/wiki/Short-circuit_evaluation), the entry protocol could be implemented as:

```c
procedure EnterCritical() {
    while ( locked == true or TestAndSet(locked) == true )
        skip // spin until locked
}
```

##### Caveat

​	<u>Although this [optimization](https://en.wikipedia.org/wiki/Optimization_(computer_science)) is useful in [system programming](https://en.wikipedia.org/wiki/System_programming) it should be avoided in high level [concurrent programming](https://en.wikipedia.org/wiki/Concurrent_programming) unless all constraints are clear and understood</u>. One example of bad usage is a similar [idiom](https://en.wikipedia.org/wiki/Programming_idiom) called [double-checked locking](https://en.wikipedia.org/wiki/Double-checked_locking), which is [unsafe without special precautions](https://en.wikipedia.org/wiki/Double-checked_locking#Usage_in_Java) and can be an [anti-pattern](https://en.wikipedia.org/wiki/Anti-pattern).[[1\]](https://en.wikipedia.org/wiki/Test_and_test-and-set#cite_note-1)

----

> [Load-link/store-conditional--wiki](https://en.wikipedia.org/wiki/Load-link/store-conditional)

#### Load-link/store-conditional--wiki

​	In [computer science](https://en.wikipedia.org/wiki/Computer_science), **load-link** and **store-conditional** (**LL/SC**) are a pair of [instructions](https://en.wikipedia.org/wiki/Instruction_(computer_science)) used in [multithreading](https://en.wikipedia.org/wiki/Thread_(computer_science)#Multithreading) to achieve [synchronization](https://en.wikipedia.org/wiki/Synchronization_(computer_science)). **Load-link returns the current value of a memory location, while a  subsequent store-conditional to the same memory location will store a  new value only if no updates have occurred to that location since the  load-link**. Together, this implements a [lock-free](https://en.wikipedia.org/wiki/Lock-free_and_wait-free_algorithms) [atomic](https://en.wikipedia.org/wiki/Linearizability) [read-modify-write](https://en.wikipedia.org/wiki/Read-modify-write) operation.

​	LL/SC was originally proposed by Jensen, Hagensen, and Broughton for the [S-1 AAP multiprocessor](http://forum.stanford.edu/wiki/index.php/S-1_project) at Lawrence Livermore National Laboratory. Load-link is also known as "load-linked", "load and reserve", or "load-locked".

##### Comparison of LL/SC and compare-and-swap

​	**If any updates have occurred, the store-conditional is guaranteed to  fail, even if the value read by the load-link has since been restored.**  <u>As such, an LL/SC pair is stronger than a read followed by a [compare-and-swap](https://en.wikipedia.org/wiki/Compare-and-swap) (CAS), which will not detect updates if the old value has been restored (see [ABA problem](https://en.wikipedia.org/wiki/ABA_problem)).</u>

​	Real implementations of LL/SC do not always succeed even if there are no concurrent updates to the memory location in question. Any  exceptional events between the two operations, such as a [context switch](https://en.wikipedia.org/wiki/Context_switch), another load-link, or even (on many platforms) another load or store  operation, will cause the store-conditional to spuriously fail. **Older implementations will fail if there are *any* updates broadcast over the memory bus.**  This is often called ***weak* LL/SC** by researchers, as it breaks many theoretical LL/SC algorithms.<sup>[*[citation needed](https://en.wikipedia.org/wiki/Wikipedia:Citation_needed)*]</sup>. Weakness is relative, and some weak implementations can be used for some algorithms.

​	LL/SC is more difficult to emulate than CAS. Additionally,  stopping running code between paired LL/SC instructions, such as when  single-stepping through code, can prevent forward progress, making  debugging tricky.[[1\]](https://en.wikipedia.org/wiki/Load-link/store-conditional#cite_note-1)

​	Nevertheless, LL/SC can be implemented in [O(1)](https://en.wikipedia.org/wiki/O(1)) and in [wait-free](https://en.wikipedia.org/wiki/Wait-free) manner using CAS and vice versa, meaning that the two primitives are equivalent from this viewpoint.[[2\]](https://en.wikipedia.org/wiki/Load-link/store-conditional#cite_note-2)

##### Implementations

LL/SC instructions are supported by:

- [Alpha](https://en.wikipedia.org/wiki/DEC_Alpha): ldl_l/stl_c and ldq_l/stq_c
- [PowerPC](https://en.wikipedia.org/wiki/PowerPC)/[Power ISA](https://en.wikipedia.org/wiki/Power_ISA): lwarx/stwcx and ldarx/stdcx
- [MIPS](https://en.wikipedia.org/wiki/MIPS_architecture): ll/sc
- [ARM](https://en.wikipedia.org/wiki/ARM_architecture): ldrex/strex (ARMv6 and v7), and ldxr/stxr (ARM version 8)
- [RISC-V](https://en.wikipedia.org/wiki/RISC-V): lr/sc
- [ARC](https://en.wikipedia.org/wiki/ARC_(processor)): LLOCK/SCOND

​	Some CPUs<sup>[*[which?](https://en.wikipedia.org/wiki/Wikipedia:Avoid_weasel_words)*]</sup> require the address being accessed exclusively to be configured in write-through mode.

​	**Typically, CPUs track the load-linked address at a [cache-line](https://en.wikipedia.org/wiki/Cache_line) or other granularity, such that any modification to any portion of the  cache line (whether via another core's store-conditional or merely by an ordinary store) is sufficient to cause the store-conditional to fail.**

​	**All of these platforms provide weak[*[clarification needed](https://en.wikipedia.org/wiki/Wikipedia:Please_clarify)*] LL/SC**. The PowerPC implementation allows an LL/SC pair to wrap loads and even stores to other [cache lines](https://en.wikipedia.org/wiki/Cache_line) (although this approach is vulnerable to false cache line sharing). This allows it to implement, for example, lock-free [reference counting](https://en.wikipedia.org/wiki/Reference_counting) in the face of changing object graphs with arbitrary counter reuse (which otherwise requires [double compare-and-swap](https://en.wikipedia.org/wiki/Double_compare-and-swap), DCAS).  RISC-V provides an architectural guarantee of eventual progress for LL/SC sequences of limited length.

​	<u>Some ARM implementations define platform dependent blocks,  ranging from 8 bytes to 2048 bytes, and an LL/SC attempt in any given  block fails if there is between the LL and SC a normal memory access  inside the same block.  Other ARM implementations fail if there is a  modification anywhere in the whole address space.  The former  implementation is the stronger and most practical.</u>

​	LL/SC has two advantages over CAS when designing a [load-store architecture](https://en.wikipedia.org/wiki/Load-store_architecture): reads and writes are separate instructions, as required by the design philosophy (and [pipeline architecture](https://en.wikipedia.org/wiki/Instruction_pipeline)); and both instructions can be performed using only two [registers](https://en.wikipedia.org/wiki/Processor_register) (address and value), fitting naturally into common [2-operand ISAs](https://en.wikipedia.org/wiki/Instruction_set#Number_of_operands). CAS, on the other hand, requires three registers (address, old value,  new value) and a dependency between the value read and the value  written. [x86](https://en.wikipedia.org/wiki/X86), being a [CISC](https://en.wikipedia.org/wiki/Complex_instruction_set_computing) architecture, does not have this constraint; **though modern chips may well translate a CAS instruction into separate LL/SC [micro-operations](https://en.wikipedia.org/wiki/Micro-operation) internally.**

##### Extensions

​	**Hardware LL/SC implementations typically do not allow nesting of LL/SC pairs**.[[3\]](https://en.wikipedia.org/wiki/Load-link/store-conditional#cite_note-LarusRajwar2007-3) A nesting LL/SC mechanism can be used to provide a MCAS primitive (multi-word CAS, where the words can be scattered).[[4\]](https://en.wikipedia.org/wiki/Load-link/store-conditional#cite_note-4)  In 2013, Trevor Brown, [Faith Ellen](https://en.wikipedia.org/wiki/Faith_Ellen), and Eric Ruppert have implemented in software a multi-address LL/SC  extension (which they call LLX/SCX) that relies on automated code  generation;[[5\]](https://en.wikipedia.org/wiki/Load-link/store-conditional#cite_note-5) <u>they have used it to implement one of the best performing concurrent [binary search tree](https://en.wikipedia.org/wiki/Binary_search_tree) (actually a [chromatic tree](https://en.wikipedia.org/w/index.php?title=Chromatic_tree&action=edit&redlink=1)), slightly beating the [JDK](https://en.wikipedia.org/wiki/JDK) CAS-based [skip list](https://en.wikipedia.org/wiki/Skip_list) implementation.</u>

---

> [Compare-and-swap--wiki](https://en.wikipedia.org/wiki/Compare-and-swap)

#### Compare-and-swap--wiki

​	In [computer science](https://en.wikipedia.org/wiki/Computer_science), **compare-and-swap** (**CAS**) is an [atomic](https://en.wikipedia.org/wiki/Atomic_(computer_science)) [instruction](https://en.wikipedia.org/wiki/Instruction_(computer_science)) used in [multithreading](https://en.wikipedia.org/wiki/Thread_(computer_science)#Multithreading) to achieve [synchronization](https://en.wikipedia.org/wiki/Synchronization_(computer_science)). It compares the contents of a memory location with a given value and,  only if they are the same, modifies the contents of that memory location to a new given value. This is done as a single atomic operation. **The  atomicity guarantees that the new value is calculated based on  up-to-date information**; if the value had been updated by another thread  in the meantime, the write would fail. The result of the operation must indicate whether it performed the substitution; this can be done either with a simple [boolean](https://en.wikipedia.org/wiki/Boolean_logic) response (this variant is often called **compare-and-set**), or by returning the value read from the memory location (*not* the value written to it).

##### Overview

​	A compare-and-swap operation is an atomic version of the following [pseudocode](https://en.wikipedia.org/wiki/Pseudocode), where * denotes [access through a pointer](https://en.wikipedia.org/wiki/Indirection):

```pseudocode
function cas(p : pointer to int, old : int, new : int) returns bool {
    if *p ≠ old {
        return false
    }
    *p ← new
    return true
}
```

​	This operation is used to implement [synchronization primitives](https://en.wikipedia.org/wiki/Synchronization_(computer_science)#Thread_or_process_synchronization) like [semaphores](https://en.wikipedia.org/wiki/Semaphore_(programming)) and [mutexes](https://en.wikipedia.org/wiki/Mutex),[[1\]](https://en.wikipedia.org/wiki/Compare-and-swap#cite_note-plan9-1) as well as more sophisticated [lock-free and wait-free algorithms](https://en.wikipedia.org/wiki/Lock-free_and_wait-free_algorithms). [Maurice Herlihy](https://en.wikipedia.org/wiki/Maurice_Herlihy) (1991) proved that CAS can implement more of these algorithms than [atomic](https://en.wikipedia.org/wiki/Atomic_operation) read, write, or [fetch-and-add](https://en.wikipedia.org/wiki/Fetch-and-add), and assuming a fairly large<sup>[*[clarification needed](https://en.wikipedia.org/wiki/Wikipedia:Please_clarify)*]</sup> amount of memory, that it can implement all of them.[[2\]](https://en.wikipedia.org/wiki/Compare-and-swap#cite_note-herlihy91-2) <u>CAS is equivalent to [load-link/store-conditional](https://en.wikipedia.org/wiki/Load-link/store-conditional), in the sense that a constant number of invocations of either primitive can be used to implement the other one in a [wait-free](https://en.wikipedia.org/wiki/Wait-free) manner.</u>[[3\]](https://en.wikipedia.org/wiki/Compare-and-swap#cite_note-3)

​	Algorithms built around CAS typically read some key memory  location and remember the old value. Based on that old value, they  compute some new value. Then they try to swap in the new value using  CAS, where the comparison checks for the location still being equal to  the old value. If CAS indicates that the attempt has failed, it has to  be repeated from the beginning: the location is re-read, a new value is  re-computed and the CAS is tried again. Instead of immediately retrying  after a CAS operation fails, researchers have found that **total system  performance can be improved in multiprocessor systems—where many threads constantly update some particular shared variable—if threads that see  their CAS fail use [exponential backoff](https://en.wikipedia.org/wiki/Exponential_backoff)—in other words, wait a little before retrying the CAS.**

##### Example application: atomic adder

​	As an example use case of compare-and-swap, here is an algorithm for [atomically incrementing or decrementing an integer](https://en.wikipedia.org/wiki/Fetch-and-add). This is useful in a variety of applications that use counters. The function add performs the action *p ← *p + a, atomically (again denoting pointer indirection by *, as in C) and returns the final value stored in the counter. Unlike in the cas pseudocode above, there is no requirement that any sequence of operations is atomic except for cas.

```pseudocode
function add(p : pointer to int, a : int) returns int {
    done ← false
    while not done {
        value ← *p  // Even this operation doesn't need to be atomic.
        done ← cas(p, value, value + a)
    }
    return value + a
}
```

​	In this algorithm, if the value of *p  changes after (or while!) it is fetched and before the CAS does the  store, CAS will notice and report this fact, causing the algorithm to  retry.

##### ABA problem

​	Main article: [ABA problem](https://en.wikipedia.org/wiki/ABA_problem)

​	Some CAS-based algorithms are affected by and must handle the problem of a [false positive](https://en.wikipedia.org/wiki/Type_I_error#False_negative_vs._false_positive) match, or the [ABA problem](https://en.wikipedia.org/wiki/ABA_problem). It is possible that between the time the old value is read and the time CAS is attempted, some other processors or threads change the memory  location two or more times such that it acquires a bit pattern which  matches the old value. <u>The problem arises if this new bit pattern, which looks exactly like the old value, has a different meaning: for  instance, it could be a recycled address, or a wrapped version counter.</u>

​	A general solution to this is to use a double-length CAS (DCAS).  E.g. on a 32-bit system, a 64-bit CAS can be used. The second half is  used to hold a counter.  The compare part of the operation compares the  previously read value of the pointer *and* the counter, with the  current pointer and counter.  If they match, the swap occurs - the new  value is written - but the new value has an incremented counter.  This  means that if ABA has occurred, although the pointer value will be the  same, the counter is exceedingly unlikely to be the same (for a 32-bit  value, a multiple of 232 operations would have to have  occurred, causing the counter to wrap and at that moment, the pointer  value would have to also by chance be the same).

​	An alternative form of this (useful on CPUs which lack DCAS) is  to use an index into a freelist, rather than a full pointer, e.g. with a 32-bit CAS, use a 16-bit index and a 16-bit counter.  However, the  reduced counter lengths begin to make ABA possible at modern CPU speeds.

​	<u>One simple technique which helps alleviate this problem is to  store an ABA counter in each data structure element, rather than using a single ABA counter for the whole data structure.</u>

​	A more complicated but more effective solution is to implement  safe memory reclamation (SMR).  This is in effect lock-free garbage  collection.  The advantage of using SMR is the assurance a given pointer will exist only once at any one time in the data structure, thus the  ABA problem is completely solved.  (Without SMR, something like a  freelist will be in use, to ensure that all data elements can be  accessed safely (no memory access violations) even when they are no  longer present in the data structure.  With SMR, only elements actually  currently in the data structure will be accessed).

##### Costs and benefits

​	CAS, and other atomic instructions, are sometimes thought to be  unnecessary in uniprocessor systems, because the atomicity of any  sequence of instructions can be achieved by disabling interrupts while  executing it. However, disabling interrupts has numerous downsides. For  example, code that is allowed to do so must be trusted not to be  malicious and monopolize the CPU, as well as to be correct and not  accidentally hang the machine in an infinite loop or page fault.  Further, **disabling interrupts is often deemed too expensive to be  practical**. Thus, even programs only intended to run on uniprocessor  machines will benefit from atomic instructions, as in the case of  Linux's [futexes](https://en.wikipedia.org/wiki/Futex).

​	In multiprocessor systems, it is usually impossible to disable  interrupts on all processors at the same time. Even if it were possible, two or more processors could be attempting to access the same  semaphore's memory at the same time, and thus atomicity would not be  achieved. <u>The compare-and-swap instruction allows any processor to  atomically test and modify a memory location, preventing such  multiple-processor collisions.</u>

​	On server-grade multi-processor architectures of the 2010s,  compare-and-swap is cheap relative to a simple load that is not served  from cache. A 2013 paper points out that a CAS is only 1.15 times more  expensive than a non-cached load on Intel Xeon ([Westmere-EX](https://en.wikipedia.org/wiki/Westmere-EX)) and 1.35 times on AMD [Opteron](https://en.wikipedia.org/wiki/Opteron) (Magny-Cours).[[6\]](https://en.wikipedia.org/wiki/Compare-and-swap#cite_note-6)

##### Implementations

​	**Compare-and-swap (and compare-and-swap-double) has been an integral part of the [IBM 370](https://en.wikipedia.org/wiki/System/370) (and all successor) architectures since 1970**. The operating systems  that run on these architectures make extensive use of this instruction  to facilitate process (i.e., system and user tasks) and processor (i.e., central processors) [parallelism](https://en.wikipedia.org/wiki/Parallel_computing) while eliminating, to the greatest degree possible, the "disabled [spin locks](https://en.wikipedia.org/wiki/Spinlock)" which had been employed in earlier IBM operating systems. **Similarly, the use of [test-and-set](https://en.wikipedia.org/wiki/Test-and-set) was also eliminated.** <u>In these operating systems, new units of work may  be instantiated "globally", into the global service priority list, or  "locally", into the local service priority list, by the execution of a  single compare-and-swap instruction.</u> This substantially improved the  responsiveness of these operating systems.

​	**In the [x86](https://en.wikipedia.org/wiki/X86) (since [80486](https://en.wikipedia.org/wiki/80486)) and [Itanium](https://en.wikipedia.org/wiki/Itanium) architectures this is implemented as the compare and exchange (CMPXCHG) instruction (on a multiprocessor the LOCK prefix must be used).**

​	**As of 2013, most [multiprocessor](https://en.wikipedia.org/wiki/Multiprocessor) architectures support CAS in hardware, and the compare-and-swap operation is the most popular [synchronization primitive](https://en.wikipedia.org/wiki/Synchronization_primitive) for implementing both lock-based and non-blocking [concurrent data structures](https://en.wikipedia.org/wiki/Concurrent_data_structure).**[[4\]](https://en.wikipedia.org/wiki/Compare-and-swap#cite_note-dice-4)

​	<u>The atomic counter and atomic bitmask operations in the Linux  kernel typically use a compare-and-swap instruction in their  implementation. The [SPARC-V8](https://en.wikipedia.org/wiki/SPARC) and [PA-RISC](https://en.wikipedia.org/wiki/PA-RISC) architectures are two of the very few recent architectures that do not  support CAS in hardware; the Linux port to these architectures uses a [spinlock](https://en.wikipedia.org/wiki/Spinlock).</u>[[7\]](https://en.wikipedia.org/wiki/Compare-and-swap#cite_note-7)

##### Implementation in C

​	Many C compilers support using compare-and-swap either with the [C11](https://en.wikipedia.org/wiki/C11_(C_standard_revision)) `<stdatomic.h>` functions,[[8\]](https://en.wikipedia.org/wiki/Compare-and-swap#cite_note-8) or some non-standard C extension of that particular C compiler,[[9\]](https://en.wikipedia.org/wiki/Compare-and-swap#cite_note-9) or by calling a function written directly in assembly language using the compare-and-swap instruction.

​	The following C function shows the basic behavior of a  compare-and-swap variant that returns the old value of the specified  memory location; however, this version does not provide the crucial  guarantees of atomicity that a real compare-and-swap operation would:

```c
int compare_and_swap(int* reg, int oldval, int newval)
{
  ATOMIC();
  int old_reg_val = *reg;
  if (old_reg_val == oldval)
     *reg = newval;
  END_ATOMIC();
  return old_reg_val;
}
```

​	`old_reg_val` is always returned, but it can be tested following the `compare_and_swap` operation to see if it matches `oldval`, as it may be different, meaning that another process has managed to succeed in a competing `compare_and_swap` to change the reg value from `oldval`.

​	<u>For example, an election protocol can be implemented such that every process checks the result of `compare_and_swap` against its own PID (= newval). The winning process finds the `compare_and_swap` returning the initial non-PID value (e.g., zero).  For the losers it will return the winning PID.</u>

```c
bool compare_and_swap(int *accum, int *dest, int newval)
{
  if (*accum == *dest) {
      *dest = newval;
      return true;
  } else {
      *accum = *dest;
      return false;
  }
}
```

This is the logic in the Intel Software Manual Vol 2A.

##### Extensions

​	<u>Since CAS operates on a single [pointer](https://en.wikipedia.org/wiki/Pointer_(computer_programming))-sized memory location, while most [lock-free and wait-free algorithms](https://en.wikipedia.org/wiki/Lock-free_and_wait-free_algorithms) need to modify multiple locations, several extensions have been implemented.</u>

- [Double compare-and-swap](https://en.wikipedia.org/wiki/Double_compare-and-swap) (DCAS)

  Compares two unrelated memory locations with two expected values,  and if they're equal, sets both locations to new values. The  generalization of DCAS to multiple (non-adjacent) words is called MCAS  or CASN. DCAS and MCAS are of practical interest in the convenient  (concurrent) implementation of some data structures like [dequeues](https://en.wikipedia.org/wiki/Double-ended_queue) or [binary search trees](https://en.wikipedia.org/wiki/Binary_search_tree).[[10\]](https://en.wikipedia.org/wiki/Compare-and-swap#cite_note-10)[[11\]](https://en.wikipedia.org/wiki/Compare-and-swap#cite_note-Fraser2004-11) DCAS and MCAS may be implemented however using the more expressive hardware [transactional memory](https://en.wikipedia.org/wiki/Transactional_memory)[[12\]](https://en.wikipedia.org/wiki/Compare-and-swap#cite_note-12) present in some recent processors such as IBM [POWER8](https://en.wikipedia.org/wiki/POWER8) or in Intel processors supporting [Transactional Synchronization Extensions](https://en.wikipedia.org/wiki/Transactional_Synchronization_Extensions) (TSX).

- Double-wide compare-and-swap

  Operates on two adjacent pointer-sized locations (or, equivalently,  one location twice as big as a pointer). On later x86 processors, the  CMPXCHG8B and CMPXCHG16B instructions[[13\]](https://en.wikipedia.org/wiki/Compare-and-swap#cite_note-13) serve this role, although early 64-bit AMD CPUs did not support  CMPXCHG16B  (modern AMD CPUs do). Some Intel motherboards from the [Core 2](https://en.wikipedia.org/wiki/Core_2) era also hamper its use, even though the processors support it. These issues came into the spotlight at the launch of [Windows 8.1](https://en.wikipedia.org/wiki/Windows_8.1) because it required hardware support for CMPXCHG16B.[[14\]](https://en.wikipedia.org/wiki/Compare-and-swap#cite_note-14)

- Single compare, double swap

  Compares one pointer but writes two. The Itanium's cmp8xchg16 instruction implements this,[[15\]](https://en.wikipedia.org/wiki/Compare-and-swap#cite_note-15) where the two written pointers are adjacent.

- Multi-word compare-and-swap

  Is a generalisation of normal compare-and-swap. It can be used to  atomically swap an arbitrary number of arbitrarily located memory  locations. Usually, multi-word compare-and-swap is implemented in  software using normal double-wide compare-and-swap operations.[[16\]](https://en.wikipedia.org/wiki/Compare-and-swap#cite_note-16)  The drawback of this approach is a lack of scalability.

---

> [Non-blocking algorithm--wiki](https://en.wikipedia.org/wiki/Non-blocking_algorithm)

#### Non-blocking algorithm--wiki

​	In [computer science](https://en.wikipedia.org/wiki/Computer_science), an [algorithm](https://en.wikipedia.org/wiki/Algorithm) is called **non-blocking** if failure or [suspension](https://en.wikipedia.org/wiki/Scheduling_(computing)) of any [thread](https://en.wikipedia.org/wiki/Thread_(computing)) cannot cause failure or suspension of another thread;[[1\]](https://en.wikipedia.org/wiki/Non-blocking_algorithm#cite_note-1)  for some operations, these algorithms provide a useful alternative to traditional [blocking implementations](https://en.wikipedia.org/wiki/Lock_(computer_science)). A non-blocking algorithm is **lock-free** if there is guaranteed system-wide [progress](https://en.wikipedia.org/wiki/Resource_starvation), and **wait-free** if there is also guaranteed per-thread progress.

​	<small>The word "non-blocking" was traditionally used to describe [telecommunications networks](https://en.wikipedia.org/wiki/Telecommunications_network) that could route a connection through a set of relays "without having to re-arrange existing calls", see [Clos network](https://en.wikipedia.org/wiki/Clos_network). Also, if the telephone exchange "is not defective, it can always make the connection", see [nonblocking minimal spanning switch](https://en.wikipedia.org/wiki/Nonblocking_minimal_spanning_switch).</small>

##### Motivation

Main article: [Disadvantages of locks](https://en.wikipedia.org/wiki/Lock_(computer_science)#Disadvantages)

​	**The traditional approach to multi-threaded programming is to use [locks](https://en.wikipedia.org/wiki/Lock_(computer_science)) to synchronize access to shared [resources](https://en.wikipedia.org/wiki/Resource_(computer_science))**. Synchronization primitives such as [mutexes](https://en.wikipedia.org/wiki/Mutual_exclusion), [semaphores](https://en.wikipedia.org/wiki/Semaphore_(programming)), and [critical sections](https://en.wikipedia.org/wiki/Critical_section) are all mechanisms by which a programmer can ensure that certain  sections of code do not execute concurrently, if doing so would corrupt  shared memory structures. If one thread attempts to acquire a lock that  is already held by another thread, the thread will block until the lock  is free.

​	**Blocking a thread can be undesirable for many reasons.  An  obvious reason is that while the thread is blocked, it cannot accomplish anything: if the blocked thread had been performing a high-priority or [real-time](https://en.wikipedia.org/wiki/Real-time_computing) task, it would be highly undesirable to halt its progress.**

​	**Other problems are less obvious.  For example, certain interactions between locks can lead to error conditions such as [deadlock](https://en.wikipedia.org/wiki/Deadlock), [livelock](https://en.wikipedia.org/wiki/Livelock), and [priority inversion](https://en.wikipedia.org/wiki/Priority_inversion).  Using locks also involves a trade-off between coarse-grained locking, which can significantly reduce opportunities for [parallelism](https://en.wikipedia.org/wiki/Parallel_computing), and fine-grained locking, which requires more careful design, increases locking overhead and is more prone to bugs.**

​	<u>Unlike blocking algorithms, non-blocking algorithms do not suffer from these downsides, and in addition are safe for use in [interrupt handlers](https://en.wikipedia.org/wiki/Interrupt_handler): even though the [preempted](https://en.wikipedia.org/wiki/Pre-emptive_multitasking) thread cannot be resumed, progress is still possible without it</u>. In  contrast, global data structures protected by mutual exclusion cannot  safely be accessed in an interrupt handler, as the preempted thread may  be the one holding the lock -- but this can be rectified easily by  masking the interrupt request during the critical section.[[2\]](https://en.wikipedia.org/wiki/Non-blocking_algorithm#cite_note-monit-2)

​	<u>A lock-free data structure can be used to improve performance. A lock-free data structure increases the amount of time spent in  parallel execution rather than serial execution, improving performance  on a [multi-core processor](https://en.wikipedia.org/wiki/Multi-core_processor), because access to the shared data structure does not need to be serialized to stay coherent.</u>[[3\]](https://en.wikipedia.org/wiki/Non-blocking_algorithm#cite_note-3)

##### Implementation

​	**With few exceptions, non-blocking algorithms use [atomic](https://en.wikipedia.org/wiki/Linearizability) [read-modify-write](https://en.wikipedia.org/wiki/Read-modify-write) primitives that the hardware must provide, the most notable of which is [compare and swap (CAS)](https://en.wikipedia.org/wiki/Compare-and-swap)**. [Critical sections](https://en.wikipedia.org/wiki/Critical_section) are almost always implemented using standard interfaces over these  primitives (**in the general case, critical sections will be blocking,  even when implemented with these primitives**). Until recently, all  non-blocking algorithms had to be written "natively" with the underlying primitives to achieve acceptable performance. However, the emerging  field of [software transactional memory](https://en.wikipedia.org/wiki/Software_transactional_memory) promises standard abstractions for writing efficient non-blocking code.[[4\]](https://en.wikipedia.org/wiki/Non-blocking_algorithm#cite_note-lightweight-transactions-4)[[5\]](https://en.wikipedia.org/wiki/Non-blocking_algorithm#cite_note-composable-memory-transactions-5)

​	<u>Much research has also been done in providing basic [data structures](https://en.wikipedia.org/wiki/Data_structure) such as [stacks](https://en.wikipedia.org/wiki/Stack_(data_structure)), [queues](https://en.wikipedia.org/wiki/Queue_(data_structure)), [sets](https://en.wikipedia.org/wiki/Set_(computer_science)), and [hash tables](https://en.wikipedia.org/wiki/Hash_table).  These allow programs to easily exchange data between threads asynchronously</u>.

​	Additionally, some non-blocking data structures are weak enough  to be implemented without special atomic primitives. These exceptions  include:

- a single-reader single-writer [ring buffer](https://en.wikipedia.org/wiki/Circular_buffer) [FIFO](https://en.wikipedia.org/wiki/FIFO_(computing_and_electronics)), with a size which evenly divides the overflow of one of the available unsigned integer types, can unconditionally be [implemented safely](https://en.wikipedia.org/wiki/Producer–consumer_problem#Without_semaphores_or_monitors) **using only a memory barrier**.
- [Read-copy-update](https://en.wikipedia.org/wiki/Read-copy-update) with a single writer and any number of readers. (The readers are  wait-free; the writer is usually lock-free, until it needs to reclaim  memory).
- **[Read-copy-update](https://en.wikipedia.org/wiki/Read-copy-update) with multiple writers and any number of readers. (The readers are  wait-free; multiple writers generally serialize with a lock and are not  obstruction-free).**

<u>Several libraries internally use lock-free techniques,[[6\]](https://en.wikipedia.org/wiki/Non-blocking_algorithm#cite_note-6)[[7\]](https://en.wikipedia.org/wiki/Non-blocking_algorithm#cite_note-7)[[8\]](https://en.wikipedia.org/wiki/Non-blocking_algorithm#cite_note-8) but it is difficult to write lock-free code that is correct</u>.[[9\]](https://en.wikipedia.org/wiki/Non-blocking_algorithm#cite_note-A_FALSE_SENSE_OF_SECURITY-9)[[10\]](https://en.wikipedia.org/wiki/Non-blocking_algorithm#cite_note-A_CORRECTED_QUEUE-10)[[11\]](https://en.wikipedia.org/wiki/Non-blocking_algorithm#cite_note-11)[[12\]](https://en.wikipedia.org/wiki/Non-blocking_algorithm#cite_note-12)

##### Wait-freedom

​	**Wait-freedom is the strongest non-blocking guarantee of progress, combining guaranteed system-wide throughput with [starvation](https://en.wikipedia.org/wiki/Resource_starvation)-freedom**. An algorithm is wait-free if every operation has a bound on the number  of steps the algorithm will take before the operation completes.[[13\]](https://en.wikipedia.org/wiki/Non-blocking_algorithm#cite_note-awilliams-13) This property is critical for real-time systems and is always nice to have as long as the performance cost is not too high.

​	**It was shown in the 1980s[[14\]](https://en.wikipedia.org/wiki/Non-blocking_algorithm#cite_note-imp-14) that all algorithms can be implemented wait-free, and many transformations from serial code, called *universal constructions*, have been demonstrated**. However, the resulting performance does not in  general match even naïve blocking designs. Several papers have since  improved the performance of universal constructions, but still, their  performance is far below blocking designs.

​	Several papers have investigated the difficulty of creating wait-free algorithms. For example, it has been shown[[15\]](https://en.wikipedia.org/wiki/Non-blocking_algorithm#cite_note-cond-sync-15) that **the widely available atomic *conditional* primitives, [CAS](https://en.wikipedia.org/wiki/Compare-and-swap) and [LL/SC](https://en.wikipedia.org/wiki/Load-link/store-conditional), cannot provide starvation-free implementations of many common data  structures without memory costs growing linearly in the number of  threads.**

​	But in practice these lower bounds do not present a real barrier  as spending a cache line or exclusive reservation granule (up to 2 KB on ARM) of store per thread in the shared memory is not considered too  costly for practical systems (typically the amount of store logically  required is a word, but <u>physically CAS operations on the same cache line will collide, and LL/SC operations in the same exclusive reservation  granule will collide, so the amount of store physically required[*[citation needed](https://en.wikipedia.org/wiki/Wikipedia:Citation_needed)*] is greater).</u>

​	Wait-free algorithms were rare until 2011, both in research and in practice. However, in 2011 Kogan and [Petrank](https://en.wikipedia.org/wiki/Erez_Petrank)[[16\]](https://en.wikipedia.org/wiki/Non-blocking_algorithm#cite_note-wf-queue-16) presented a wait-free queue building on the [CAS](https://en.wikipedia.org/wiki/Compare-and-swap) primitive, generally available on common hardware. Their construction expanded the lock-free queue of Michael and Scott,[[17\]](https://en.wikipedia.org/wiki/Non-blocking_algorithm#cite_note-lf-queue-17) which is an efficient queue often used in practice. A follow-up paper by Kogan and [Petrank](https://en.wikipedia.org/wiki/Erez_Petrank)[[18\]](https://en.wikipedia.org/wiki/Non-blocking_algorithm#cite_note-wf-fpsp-18) provided <u>a method for making wait-free algorithms fast and used this  method to make the wait-free queue practically as fast as its lock-free  counterpart</u>. A subsequent paper by Timnat and [Petrank](https://en.wikipedia.org/wiki/Erez_Petrank)[[19\]](https://en.wikipedia.org/wiki/Non-blocking_algorithm#cite_note-wf-simulation-19) provided <u>an automatic mechanism for generating wait-free data  structures from lock-free ones</u>. Thus, <u>wait-free implementations are now  available for many data-structures.</u>

##### Lock-freedom

​	**Lock-freedom allows individual threads to starve but guarantees  system-wide throughput**. An algorithm is lock-free if, when the program  threads are run for a sufficiently long time, at least one of the  threads makes progress (for some sensible definition of progress). **All wait-free algorithms are lock-free.**

​	In particular, if one thread is suspended, then a lock-free  algorithm guarantees that the remaining threads can still make progress. Hence, **if two threads can contend for the same mutex lock or spinlock, then the algorithm is *not* lock-free.** (If we suspend one thread that holds the lock, then the second thread will block.)

​	An algorithm is lock-free if infinitely often operation by some  processors will succeed in a finite number of steps. For instance, if N  processors are trying to execute an operation, some of the N processes  will succeed in finishing the operation in a finite number of steps and  others might fail and retry on failure. **The difference between wait-free and lock-free is that wait-free operation by each process is guaranteed to succeed in a finite number of steps, regardless of the other  processors.**

​	<u>In general, a lock-free algorithm can run in four phases:  completing one's own operation, assisting an obstructing operation,  aborting an obstructing operation, and waiting</u>. Completing one's own  operation is complicated by the possibility of concurrent assistance and abortion, but is invariably the fastest path to completion.

​	<u>The decision about when to assist, abort or wait when an obstruction is met is the responsibility of a *contention manager*. This may be very simple (assist higher priority operations, abort lower priority ones), or may be more optimized to achieve better throughput,  or lower the latency of prioritized operations.</u>

​	<u>Correct concurrent assistance is typically the most complex part  of a lock-free algorithm, and often very costly to execute: not only  does the assisting thread slow down, but thanks to the mechanics of  shared memory, the thread being assisted will be slowed, too, if it is  still running.</u>

##### Obstruction-freedom

​	Obstruction-freedom is the weakest natural non-blocking progress  guarantee. An algorithm is obstruction-free if at any point, a single  thread executed in isolation (i.e., with all obstructing threads  suspended) for a bounded number of steps will complete its operation.[[13\]](https://en.wikipedia.org/wiki/Non-blocking_algorithm#cite_note-awilliams-13) **All lock-free algorithms are obstruction-free.**

​	Obstruction-freedom demands only that any partially completed  operation can be aborted and the changes made rolled back. Dropping  concurrent assistance can often result in much simpler algorithms that  are easier to validate. <u>Preventing the system from continually [live-locking](https://en.wikipedia.org/wiki/Livelock) is the task of a contention manager.</u>

​	**Some obstruction-free algorithms use a pair of "consistency  markers" in the data structure.  Processes reading the data structure  first read one consistency marker, then read the relevant data into an  internal buffer, then read the other marker, and then compare the  markers.  The data is consistent if the two markers are identical.   Markers may be non-identical when the read is interrupted by another  process updating the data structure.  In such a case, the process  discards the data in the internal buffer and tries again.**

​	**"Non-blocking" was used as a synonym for "lock-free" in the literature until the introduction of obstruction-freedom in 2003.**

---

> [Transactional memory--wiki](https://en.wikipedia.org/wiki/Transactional_memory)

#### Transactional memory--wiki

​	In [computer science](https://en.wikipedia.org/wiki/Computer_science) and [engineering](https://en.wikipedia.org/wiki/Computer_engineering), **transactional memory** attempts to simplify [concurrent programming](https://en.wikipedia.org/wiki/Concurrent_programming) by **allowing a group of load and store instructions to execute in an [atomic](https://en.wikipedia.org/wiki/Linearizability) way**. It is a [concurrency control](https://en.wikipedia.org/wiki/Concurrency_control) mechanism analogous to [database transactions](https://en.wikipedia.org/wiki/Database_transaction) for controlling access to [shared memory](https://en.wikipedia.org/wiki/Shared_memory_(interprocess_communication)) in [concurrent computing](https://en.wikipedia.org/wiki/Concurrent_computing). Transactional memory systems provide high-level abstraction as an  alternative to low-level thread synchronization. **This abstraction allows for coordination between concurrent reads and writes of shared data in  parallel systems.**[[1\]](https://en.wikipedia.org/wiki/Transactional_memory#cite_note-1)

##### Motivation

​	**In concurrent programming, synchronization is required when parallel  threads attempt to access a shared resource**. Low level thread  synchronization constructs such as locks are pessimistic and prohibit  threads that are outside a [critical section](https://en.wikipedia.org/wiki/Critical_section) from making any changes. The process of applying and releasing locks  often functions as additional overhead in workloads with little conflict among threads. Transactional memory provides [optimistic concurrency control](https://en.wikipedia.org/wiki/Optimistic_concurrency_control) by allowing threads to run in parallel with minimal interference.[[2\]](https://en.wikipedia.org/wiki/Transactional_memory#cite_note-:0-2) **The goal of transactional memory systems is to transparently support regions of code marked as transactions by enforcing [atomicity](https://en.wikipedia.org/wiki/Atomicity_(database_systems)), [consistency](https://en.wikipedia.org/wiki/Consistency_(database_systems)) and [isolation](https://en.wikipedia.org/wiki/Isolation_(database_systems)).**

​	**A transaction is a collection of operations that can execute and  commit changes as long as a conflict is not present.** When a conflict is  detected, a transaction will revert to its initial state (prior to any  changes) and will rerun until all conflicts are removed. <u>Before a  successful commit, the outcome of any operation is purely speculative  inside a transaction.</u> **In contrast to lock-based synchronization where  operations are serialized to prevent data corruption, transactions allow for additional parallelism as long as few operations attempt to modify a shared resource**. <u>Since the programmer is not responsible for explicitly identifying locks or the order in which they are acquired, programs  that utilize transactional memory cannot produce a [deadlock](https://en.wikipedia.org/wiki/Deadlock).[[2\]](https://en.wikipedia.org/wiki/Transactional_memory#cite_note-:0-2)</u> 

​	With these constructs in place, transactional memory provides a  high level programming abstraction by allowing programmers to enclose  their methods within transactional blocks. Correct implementations  ensure that data cannot be shared between threads without going through a transaction and produce a [serializable](https://en.wikipedia.org/wiki/Serializability) outcome. For example, code can be written as:

```pseudocode
def transfer_money(from_account, to_account, amount):
    """Transfer money from one account to another."""
    with transaction():
        from_account -= amount
        to_account   += amount
```

​	In the code, the block defined by "transaction" is guaranteed  atomicity, consistency and isolation by the underlying transactional  memory implementation and is transparent to the programmer. The  variables within the transaction are protected from external conflicts,  ensuring that either the correct amount is transferred or no action is  taken at all. <u>Note that concurrency related bugs are still possible in  programs that use a large number of transactions, especially in software implementations where the library provided by the language is unable to enforce correct use.</u> **Bugs introduced through transactions can often be  difficult to debug since breakpoints cannot be placed within a  transaction.**[[2\]](https://en.wikipedia.org/wiki/Transactional_memory#cite_note-:0-2)

​	**Transactional memory is limited in that it requires a  shared-memory abstraction. Although transactional memory programs cannot produce a deadlock, programs may still suffer from a livelock or  resource [starvation](https://en.wikipedia.org/wiki/Starvation_(computer_science)). For example, longer transactions may repeatedly revert in response to  multiple smaller transactions, wasting both time and energy.**[[2\]](https://en.wikipedia.org/wiki/Transactional_memory#cite_note-:0-2)

##### Hardware vs. software

​	**The abstraction of atomicity in transactional memory requires a  hardware mechanism to detect conflicts and undo any changes made to  shared data**.[[3\]](https://en.wikipedia.org/wiki/Transactional_memory#cite_note-:1-3) Hardware transactional memory systems may comprise modifications in processors, cache and bus protocol to support transactions.[[4\]](https://en.wikipedia.org/wiki/Transactional_memory#cite_note-:2-4)[[5\]](https://en.wikipedia.org/wiki/Transactional_memory#cite_note-5)[[6\]](https://en.wikipedia.org/wiki/Transactional_memory#cite_note-6)[[7\]](https://en.wikipedia.org/wiki/Transactional_memory#cite_note-7)[[8\]](https://en.wikipedia.org/wiki/Transactional_memory#cite_note-8) **Speculative values in a transaction must be buffered and remain unseen  by other threads until commit time.** Large buffers are used to store  speculative values while avoiding write propagation through the  underlying [cache coherence](https://en.wikipedia.org/wiki/Cache_coherence) protocol. **Traditionally, buffers have been implemented using different  structures within the memory hierarchy such as store queues or caches.**  Buffers further away from the processor, such as the L2 cache, can hold  more speculative values (up to a few megabytes). <u>The optimal size of a  buffer is still under debate due to the limited use of transactions in  commercial programs</u>.[[3\]](https://en.wikipedia.org/wiki/Transactional_memory#cite_note-:1-3) **In a cache implementation, the cache lines are generally augmented with read and write bits.** **When the hardware controller receives a request,  the controller uses these bits to detect a conflict.** If a  serializability conflict is detected from a parallel transaction, then  the speculative values are discarded. When caches are used, the system  may introduce the risk of [false conflicts](https://en.wikipedia.org/wiki/False_sharing) due to the use of cache line granularity.[[3\]](https://en.wikipedia.org/wiki/Transactional_memory#cite_note-:1-3) **[Load-link/store-conditional](https://en.wikipedia.org/wiki/Load-link/store-conditional) (LL/SC) offered by many [RISC](https://en.wikipedia.org/wiki/RISC) processors can be viewed as the most basic transactional memory  support**; **however, LL/SC usually operates on data that is the size of a  native machine word, so only single-word transactions are supported.[[4\]](https://en.wikipedia.org/wiki/Transactional_memory#cite_note-:2-4) Although hardware transactional memory provides maximal performance  compared to software alternatives, limited use has been seen at this  time.** 

​	**[Software transactional memory](https://en.wikipedia.org/wiki/Software_transactional_memory) provides transactional memory semantics in a software [runtime library](https://en.wikipedia.org/wiki/Runtime_library) or the programming language,[[9\]](https://en.wikipedia.org/wiki/Transactional_memory#cite_note-9) and requires minimal hardware support (typically an atomic [compare and swap](https://en.wikipedia.org/wiki/Compare_and_swap) operation, or equivalent)**.  As the downside, software implementations  usually come with a performance penalty, when compared to hardware  solutions. [Hardware acceleration](https://en.wikipedia.org/wiki/Hardware_acceleration) can reduce some of the overheads associated with software transactional memory.

​	**Owing to the more limited nature of hardware transactional memory (in current implementations), software using it may require fairly  extensive tuning to fully benefit from it.** For example, the dynamic  memory allocator may have a significant influence on performance and  likewise structure padding may affect performance (owing to cache  alignment and false sharing issues); in the context of a virtual  machine, various background threads may cause unexpected transaction  aborts.[[10\]](https://en.wikipedia.org/wiki/Transactional_memory#cite_note-10)

##### History

​	One of the earliest implementations of transactional memory was the gated store buffer used in [Transmeta](https://en.wikipedia.org/wiki/Transmeta)'s [Crusoe](https://en.wikipedia.org/wiki/Transmeta_Crusoe) and [Efficeon](https://en.wikipedia.org/wiki/Transmeta_Efficeon) processors.  However, this was only used to facilitate speculative  optimizations for binary translation, rather than any form of [speculative multithreading](https://en.wikipedia.org/wiki/Speculative_multithreading), or exposing it directly to programmers.  Azul Systems also implemented hardware transactional memory to accelerate their [Java](https://en.wikipedia.org/wiki/Java_(programming_language)) appliances, but this was similarly hidden from outsiders.[[11\]](https://en.wikipedia.org/wiki/Transactional_memory#cite_note-haswell-tm-11)

​	[Sun Microsystems](https://en.wikipedia.org/wiki/Sun_Microsystems) implemented hardware transactional memory and a limited form of speculative multithreading in its high-end [Rock processor](https://en.wikipedia.org/wiki/Rock_processor).  This implementation proved that it could be used for lock elision and  more complex hybrid transactional memory systems, where transactions are handled with a combination of hardware and software.  The Rock  processor was canceled in 2009, just before the acquisition by [Oracle](https://en.wikipedia.org/wiki/Oracle_Corporation); while the actual products were never released, a number of prototype systems were available to researchers.[[11\]](https://en.wikipedia.org/wiki/Transactional_memory#cite_note-haswell-tm-11)

​	In 2009, [AMD](https://en.wikipedia.org/wiki/AMD) proposed the [Advanced Synchronization Facility](https://en.wikipedia.org/wiki/Advanced_Synchronization_Facility) (ASF), a set of [x86](https://en.wikipedia.org/wiki/X86) extensions that provide a very limited form of hardware transactional  memory support.  The goal was to provide hardware primitives that could  be used for higher-level synchronization, such as software transactional memory or lock-free algorithms.  However, AMD has not announced whether ASF will be used in products, and if so, in what timeframe.[[11\]](https://en.wikipedia.org/wiki/Transactional_memory#cite_note-haswell-tm-11)

​	More recently, [IBM](https://en.wikipedia.org/wiki/IBM) announced in 2011 that [Blue Gene/Q](https://en.wikipedia.org/wiki/Blue_Gene/Q) had hardware support for both transactional memory and speculative  multithreading.  The transactional memory could be configured in two  modes; the first is an unordered and single-version mode, where a write  from one transaction causes a conflict with any transactions reading the same memory address.  The second mode is for speculative  multithreading, providing an ordered, multi-versioned transactional  memory.  Speculative threads can have different versions of the same  memory address, and hardware implementation keeps track of the age for  each thread.  The younger threads can access data from older threads  (but not the other way around), and writes to the same address are based on the thread order.  In some cases, dependencies between threads can  cause the younger versions to abort.[[11\]](https://en.wikipedia.org/wiki/Transactional_memory#cite_note-haswell-tm-11)

​	[Intel](https://en.wikipedia.org/wiki/Intel)'s [Transactional Synchronization Extensions](https://en.wikipedia.org/wiki/Transactional_Synchronization_Extensions) (TSX) is available in some of the [Skylake](https://en.wikipedia.org/wiki/Skylake_(microarchitecture)) processors. It was earlier implemented in [Haswell](https://en.wikipedia.org/wiki/Haswell_(microarchitecture)) and [Broadwell](https://en.wikipedia.org/wiki/Broadwell_(microarchitecture)) processors as well, but the implementations turned out both times to be defective and support for TSX was disabled. The TSX specification  describes the transactional memory API for use by software developers,  but withholds details on technical implementation.[[11\]](https://en.wikipedia.org/wiki/Transactional_memory#cite_note-haswell-tm-11) [ARM architecture](https://en.wikipedia.org/wiki/ARM_architecture) has a similar extension.[[12\]](https://en.wikipedia.org/wiki/Transactional_memory#cite_note-sve2-tme-12)

​	As of GCC 4.7, an experimental library for transactional memory  is available which utilizes a hybrid implementation. The PyPy variant of Python also introduces transactional memory to the language.

##### Available implementations

- Hardware:
  - Arm Transactional Memory Extension (TME)[[13\]](https://en.wikipedia.org/wiki/Transactional_memory#cite_note-13)
  - [Rock processor](https://en.wikipedia.org/wiki/Rock_processor) (canceled by [Oracle](https://en.wikipedia.org/wiki/Oracle_Corporation))
  - [Blue Gene/Q](https://en.wikipedia.org/wiki/Blue_Gene/Q) processor from [IBM](https://en.wikipedia.org/wiki/IBM) (Sequoia supercomputer)[[14\]](https://en.wikipedia.org/wiki/Transactional_memory#cite_note-14)
  - IBM [zEnterprise EC12](https://en.wikipedia.org/wiki/ZEnterprise_EC12), the first commercial server to include transactional memory processor instructions
  - Intel's [Transactional Synchronization Extensions](https://en.wikipedia.org/wiki/Transactional_Synchronization_Extensions) (TSX), available in select Haswell-based processors and newer
  - [IBM](https://en.wikipedia.org/wiki/IBM) [POWER8](https://en.wikipedia.org/wiki/POWER8) and newer[[15\]](https://en.wikipedia.org/wiki/Transactional_memory#cite_note-HallArnold2014-15)[[16\]](https://en.wikipedia.org/wiki/Transactional_memory#cite_note-16)
- Software:
  - Vega 2 from [Azul Systems](https://en.wikipedia.org/wiki/Azul_Systems)[[17\]](https://en.wikipedia.org/wiki/Transactional_memory#cite_note-17)
  - STM Monad in the [Glasgow Haskell Compiler](https://en.wikipedia.org/wiki/Glasgow_Haskell_Compiler)[[18\]](https://en.wikipedia.org/wiki/Transactional_memory#cite_note-18)
  - STMX in [Common Lisp](https://en.wikipedia.org/wiki/Common_Lisp)[[19\]](https://en.wikipedia.org/wiki/Transactional_memory#cite_note-19)
  - Refs in [Clojure](https://en.wikipedia.org/wiki/Clojure)
  - [gcc](https://en.wikipedia.org/wiki/GNU_Compiler_Collection) 4.7+ for C/C++[[20\]](https://en.wikipedia.org/wiki/Transactional_memory#cite_note-20)[[21\]](https://en.wikipedia.org/wiki/Transactional_memory#cite_note-21)[[22\]](https://en.wikipedia.org/wiki/Transactional_memory#cite_note-22)[[23\]](https://en.wikipedia.org/wiki/Transactional_memory#cite_note-23)
  - [PyPy](https://en.wikipedia.org/wiki/PyPy)[[24\]](https://en.wikipedia.org/wiki/Transactional_memory#cite_note-24)
  - Part of the picotm Transaction Framework for C[[25\]](https://en.wikipedia.org/wiki/Transactional_memory#cite_note-25)
  - The TVar in concurrent-ruby, a concurrency library for Ruby[[26\]](https://en.wikipedia.org/wiki/Transactional_memory#cite_note-26)

---

> [Lock (computer science)--wiki](https://en.wikipedia.org/wiki/Lock_(computer_science))

#### Lock (computer science)--wiki

​	In [computer science](https://en.wikipedia.org/wiki/Computer_science), a **lock** or **mutex** (from [mutual exclusion](https://en.wikipedia.org/wiki/Mutual_exclusion)) is a [synchronization](https://en.wikipedia.org/wiki/Synchronization_(computer_science)) mechanism for enforcing limits on access to a resource in an environment where there are many [threads of execution](https://en.wikipedia.org/wiki/Thread_(computer_science)). <u>A lock is designed to enforce a [mutual exclusion](https://en.wikipedia.org/wiki/Mutual_exclusion) [concurrency control](https://en.wikipedia.org/wiki/Concurrency_control) policy.</u>

##### Granularity

Before being introduced to lock granularity, one needs to understand three concepts about locks:

- *lock overhead*: the extra resources for using locks, like  the memory space allocated for locks, the CPU time to initialize and  destroy locks, and the time for acquiring or releasing locks. The more  locks a program uses, the more overhead associated with the usage;
- *lock [contention](https://en.wikipedia.org/wiki/Resource_contention)*: this occurs whenever one process or thread attempts to acquire a lock  held by another process or thread. **The more fine-grained the available locks, the less likely one process/thread will request a lock held by  the other**. (For example, locking a row rather than the entire table, or  locking a cell rather than the entire row.);
- *[deadlock](https://en.wikipedia.org/wiki/Deadlock)*: the situation when each of at least two tasks is waiting for a lock  that the other task holds. Unless something is done, the two tasks will  wait forever.

​	There is a tradeoff between decreasing lock overhead and decreasing  lock contention when choosing the number of locks in synchronization.

​	**An important property of a lock is its *[granularity](https://en.wikipedia.org/wiki/Granularity_(parallel_computing))***. The granularity is a measure of the amount of data the lock is  protecting. In general, choosing a coarse granularity (a small number of locks, each protecting a large segment of data) results in less *lock overhead* when a single process is accessing the protected data, but worse  performance when multiple processes are running concurrently. This is  because of increased *lock contention*. **The more coarse the lock,  the higher the likelihood that the lock will stop an unrelated process  from proceeding.** Conversely, using a fine granularity (a larger number  of locks, each protecting a fairly small amount of data) increases the  overhead of the locks themselves but reduces lock contention. Granular  locking where each process must hold multiple locks from a common set of locks can create subtle lock dependencies. This subtlety can increase  the chance that a programmer will unknowingly introduce a *deadlock*.[*[citation needed](https://en.wikipedia.org/wiki/Wikipedia:Citation_needed)*]

​	<u>In a [database management system](https://en.wikipedia.org/wiki/Database_management_system), for example, a lock could protect, in order of decreasing granularity,  part of a field, a field, a record, a data page, or an entire table.  Coarse granularity, such as using table locks, tends to give the best  performance for a single user, whereas fine granularity, such as record  locks, tends to give the best performance for multiple users.</u>

##### Database locks

**[	Database locks](https://en.wikipedia.org/wiki/Lock_(database)) can be used as a means of ensuring transaction synchronicity**. i.e. when making transaction processing concurrent (interleaving transactions),  using [2-phased locks](https://en.wikipedia.org/wiki/Two-phase_locking) ensures that the concurrent execution of the transaction turns out  equivalent to some serial ordering of the transaction. However,  deadlocks become an unfortunate side-effect of locking in databases.  Deadlocks are either prevented by pre-determining the locking order  between transactions or are detected using [waits-for graphs](https://en.wikipedia.org/wiki/Wait-for_graph). An alternate to locking for database synchronicity while avoiding  deadlocks involves the use of totally ordered global timestamps.

​	There are mechanisms employed to manage the actions of multiple [concurrent users](https://en.wikipedia.org/wiki/Concurrent_user) on a database—the purpose is to prevent lost updates and dirty reads. The two types of locking are *pessimistic locking* and *optimistic locking*:

- *Pessimistic locking*: **a user who reads a record with the  intention of updating it places an exclusive lock on the record to  prevent other users from manipulating it.** This means no one else can  manipulate that record until the user releases the lock. The downside is that users can be locked out for a very long time, thereby slowing the  overall system response and causing frustration.

- *[Optimistic locking](https://en.wikipedia.org/wiki/Optimistic_locking)*: **this allows multiple concurrent users access to the database whilst the system keeps a copy of the initial-read made by each user.** When a user  wants to update a record, the application determines whether another  user has changed the record since it was last read. The application does this by comparing the initial-read held in memory to the database  record to verify any changes made to the record. Any discrepancies  between the initial-read and the database record violates concurrency  rules and hence causes the system to disregard any update request. An  error message is generated and the user is asked to start the update  process again. It improves database performance by reducing the amount  of locking required, thereby reducing the load on the database server.  It works efficiently with tables that require limited updates since no  users are locked out. However, some updates may fail. The downside is  constant update failures due to high volumes of update requests from  multiple concurrent users - it can be frustrating for users.

**Where to use optimistic locking: this is appropriate in environments  where there is low contention for data, or where read-only access to  data is required.** Optimistic concurrency is used extensively in .NET to  address the needs of mobile and disconnected applications,[[4\]](https://en.wikipedia.org/wiki/Lock_(computer_science)#cite_note-4) where locking data rows for prolonged periods of time would be  infeasible. Also, **maintaining record locks requires a persistent  connection to the database server, which is not possible in disconnected applications.**

##### Disadvantages

Lock-based resource protection and thread/process synchronization have many disadvantages:

- Contention: some threads/processes have to wait until a lock (or a whole set of locks) is released. If one of the threads holding a lock dies, stalls, blocks, or enters an infinite loop, other threads waiting for the lock may wait forever.
- Overhead: the use of locks adds overhead for each access to a  resource, even when the chances for collision are very rare. (However,  any chance for such collisions is a [race condition](https://en.wikipedia.org/wiki/Race_condition).)
- **Debugging: bugs associated with locks are time dependent and can be very subtle and extremely hard to replicate, such as [deadlocks](https://en.wikipedia.org/wiki/Deadlock).**
- Instability: the optimal balance between lock overhead and lock  contention can be unique to the problem domain (application) and  sensitive to design, implementation, and even low-level system  architectural changes. These balances may change over the life cycle of  an application and may entail tremendous changes to update (re-balance).
- **Composability: locks are only composable (e.g., managing multiple  concurrent locks in order to atomically delete item X from table A and  insert X into table B) with relatively elaborate (overhead) software  support and perfect adherence by applications programming to rigorous  conventions.**
- **[Priority inversion](https://en.wikipedia.org/wiki/Priority_inversion): a low-priority thread/process holding a common lock can prevent high-priority threads/processes from proceeding. [Priority inheritance](https://en.wikipedia.org/wiki/Priority_inheritance) can be used to reduce priority-inversion duration. The [priority ceiling protocol](https://en.wikipedia.org/wiki/Priority_ceiling_protocol) can be used on uniprocessor systems to minimize the worst-case priority-inversion duration, as well as prevent [deadlock](https://en.wikipedia.org/wiki/Deadlock).**
- [<u>Convoying](https://en.wikipedia.org/wiki/Lock_convoy): all other threads have to wait if a thread holding a lock is descheduled due to a time-slice interrupt or page fault.</u>

Some [concurrency control](https://en.wikipedia.org/wiki/Concurrency_control) strategies avoid some or all of these problems. For example, a [funnel](https://en.wikipedia.org/wiki/Funnel_(Concurrent_computing)) or [serializing tokens](https://en.wikipedia.org/wiki/Serializing_tokens) can avoid the biggest problem: deadlocks. Alternatives to locking include [non-blocking synchronization](https://en.wikipedia.org/wiki/Non-blocking_synchronization) methods, like [lock-free](https://en.wikipedia.org/wiki/Lock-free_and_wait-free_algorithms) programming techniques and [transactional memory](https://en.wikipedia.org/wiki/Transactional_memory). However, such alternative methods often require that the actual lock  mechanisms be implemented at a more fundamental level of the operating  software. Therefore, they may only relieve the *application* level  from the details of implementing locks, with the problems listed above  still needing to be dealt with beneath the application.

In most cases, proper locking depends on the CPU providing a  method of atomic instruction stream synchronization (for example, the  addition or deletion of an item into a pipeline requires that all  contemporaneous operations needing to add or delete other items in the  pipe be suspended during the manipulation of the memory content required to add or delete the specific item). Therefore, **an application can  often be more robust when it recognizes the burdens it places upon an  operating system and is capable of graciously recognizing the reporting  of impossible demands.**<sup>[*[citation needed](https://en.wikipedia.org/wiki/Wikipedia:Citation_needed)*]</sup>

##### Language support

Programming languages vary in their support for synchronization:

- The ISO/IEC [C](https://en.wikipedia.org/wiki/C_(programming_language)) standard provides a standard [mutual exclusion](https://en.wikipedia.org/wiki/Mutual_exclusion) (locks) [API](https://en.wikipedia.org/wiki/Application_programming_interface) since [C11](https://en.wikipedia.org/wiki/C11_(C_standard_revision)). The current ISO/IEC [C++](https://en.wikipedia.org/wiki/C%2B%2B) standard supports [threading facilities](https://en.wikipedia.org/wiki/C%2B%2B0x#Threading_facilities) since [C++11](https://en.wikipedia.org/wiki/C%2B%2B11). The [OpenMP](https://en.wikipedia.org/wiki/OpenMP) standard is supported by some compilers, and allows critical sections to be specified using pragmas. The [POSIX pthread](https://en.wikipedia.org/wiki/POSIX_Threads) API provides lock support.[[6\]](https://en.wikipedia.org/wiki/Lock_(computer_science)#cite_note-6) [Visual C++](https://en.wikipedia.org/wiki/Visual_C%2B%2B) provides the `synchronize` attribute of methods to be synchronized, but this is specific to COM objects in the [Windows](https://en.wikipedia.org/wiki/Microsoft_Windows) architecture and [Visual C++](https://en.wikipedia.org/wiki/Visual_C%2B%2B) compiler.[[7\]](https://en.wikipedia.org/wiki/Lock_(computer_science)#cite_note-7)  C and C++ can easily access any native operating system locking features.
- [Objective-C](https://en.wikipedia.org/wiki/Objective-C) provides the keyword `@synchronized`[[8\]](https://en.wikipedia.org/wiki/Lock_(computer_science)#cite_note-8) to put locks on blocks of code and also provides the classes NSLock,[[9\]](https://en.wikipedia.org/wiki/Lock_(computer_science)#cite_note-9) NSRecursiveLock,[[10\]](https://en.wikipedia.org/wiki/Lock_(computer_science)#cite_note-10) and NSConditionLock[[11\]](https://en.wikipedia.org/wiki/Lock_(computer_science)#cite_note-11) along with the NSLocking protocol[[12\]](https://en.wikipedia.org/wiki/Lock_(computer_science)#cite_note-12) for locking as well.
- [C#](https://en.wikipedia.org/wiki/C_Sharp_(programming_language)) provides the `lock` keyword on a thread to ensure its exclusive access to a resource.
- [VB.NET](https://en.wikipedia.org/wiki/VB.NET) provides a `SyncLock` keyword like C#'s `lock` keyword.
- [Java](https://en.wikipedia.org/wiki/Java_(programming_language)) provides the keyword `synchronized` to lock code blocks, [methods](https://en.wikipedia.org/wiki/Method_(computer_science)) or [objects](https://en.wikipedia.org/wiki/Object_(computer_science))[[13\]](https://en.wikipedia.org/wiki/Lock_(computer_science)#cite_note-13) and libraries featuring concurrency-safe data structures.
- [Python](https://en.wikipedia.org/wiki/Python_(programming_language)) provides a low-level [mutex](https://en.wikipedia.org/wiki/Mutual_exclusion) mechanism with a `Lock` class from the `threading` module.[[14\]](https://en.wikipedia.org/wiki/Lock_(computer_science)#cite_note-14)
- The ISO/IEC [Fortran](https://en.wikipedia.org/wiki/Fortran_(programming_language)) standard (ISO/IEC 1539-1:2010) provides the `lock_type` derived type in the intrinsic module `iso_fortran_env` and the `lock`/`unlock` statements since [Fortran 2008](https://en.wikipedia.org/wiki/Fortran#Fortran_2008).[[15\]](https://en.wikipedia.org/wiki/Lock_(computer_science)#cite_note-15)
- [Ruby](https://en.wikipedia.org/wiki/Ruby_(programming_language)) provides a low-level [mutex](https://en.wikipedia.org/wiki/Mutual_exclusion) object and no keyword.[[16\]](https://en.wikipedia.org/wiki/Lock_(computer_science)#cite_note-16)
- [Ada](https://en.wikipedia.org/wiki/Ada_(programming_language)) provides protected objects that have visible protected subprograms or entries[[17\]](https://en.wikipedia.org/wiki/Lock_(computer_science)#cite_note-17) as well as rendezvous.[[18\]](https://en.wikipedia.org/wiki/Lock_(computer_science)#cite_note-18)
- [x86 assembly](https://en.wikipedia.org/wiki/X86_assembly) provides the `LOCK` prefix on certain operations to guarantee their atomicity.
- [PHP](https://en.wikipedia.org/wiki/PHP) provides a file-based locking [[19\]](https://en.wikipedia.org/wiki/Lock_(computer_science)#cite_note-19) as well as a `Mutex` class in the `pthreads` extension. 

---

> [Mutual exclusion--wiki](https://en.wikipedia.org/wiki/Mutual_exclusion#Hardware_solutions)

#### mutual exclusion

​	In [computer science](https://en.wikipedia.org/wiki/Computer_science), **mutual exclusion** is a property of [concurrency control](https://en.wikipedia.org/wiki/Concurrency_control), which is instituted for the purpose of preventing [race conditions](https://en.wikipedia.org/wiki/Race_condition). It is the requirement that one [thread of execution](https://en.wikipedia.org/wiki/Thread_(computing)) never enters its [critical section](https://en.wikipedia.org/wiki/Critical_section) at the same time that another [concurrent](https://en.wikipedia.org/wiki/Concurrent_computing) thread of execution enters its own critical section, which refers to an interval of time during which a thread of execution accesses a shared  resource, such as [shared memory](https://en.wikipedia.org/wiki/Shared_memory_(interprocess_communication)).

​	The requirement of mutual exclusion was first identified and solved by [Edsger W. Dijkstra](https://en.wikipedia.org/wiki/Edsger_W._Dijkstra) in his seminal 1965 paper "Solution of a problem in concurrent programming control",[[1\]](https://en.wikipedia.org/wiki/Mutual_exclusion#cite_note-1)[[2\]](https://en.wikipedia.org/wiki/Mutual_exclusion#cite_note-Taubenfeld:2004-2) which is credited as the first topic in the study of [concurrent algorithms](https://en.wikipedia.org/wiki/Concurrent_algorithm).[[3\]](https://en.wikipedia.org/wiki/Mutual_exclusion#cite_note-3)

​	This problem (called a *race condition*) can be avoided by  using the requirement of mutual exclusion to ensure that simultaneous  updates to the same part of the list cannot occur.

​	The term mutual exclusion is also used in reference to the  simultaneous writing of a memory address by one thread while the  aforementioned memory address is being manipulated or read by one or  more other threads. 

- Non-Critical Section

  Operation is outside the critical section; the process is not using or requesting the shared resource.

- Trying

  The process attempts to enter the critical section.

- Critical Section

  The process is allowed to access the shared resource in this section.

- Exit

  The process leaves the critical section and makes the shared resource available to other processes.

If a process wishes to enter the critical section, it must first execute the trying section and wait until it acquires access to the critical  section. After the process has executed its critical section and is  finished with the shared resources, it needs to execute the exit section to release them for other processes' use. The process then returns to  its non-critical section.

##### Enforcing mutual exclusion

###### Hardware solutions

​	On [uniprocessor](https://en.wikipedia.org/wiki/Uniprocessor) systems, the simplest solution to achieve mutual exclusion is to **disable [interrupts](https://en.wikipedia.org/wiki/Interrupt) during a process's critical section**. This will prevent any [interrupt service routines](https://en.wikipedia.org/wiki/Interrupt_service_routine) from running (effectively preventing a process from being [preempted](https://en.wikipedia.org/wiki/Preemption_(computing))). **Although this solution is effective, it leads to many problems. If a critical section is long, then the [system clock](https://en.wikipedia.org/wiki/System_clock) will drift every time a critical section is executed because the timer  interrupt is no longer serviced, so tracking time is impossible during  the critical section**. Also, if a process halts during its critical  section, control will never be returned to another process, effectively  halting the entire system. A more elegant method for achieving mutual  exclusion is the [busy-wait](https://en.wikipedia.org/wiki/Busy-wait).

​	**Busy-waiting is effective for both uniprocessor and [multiprocessor](https://en.wikipedia.org/wiki/Multiprocessor) systems**. The use of shared memory and an [atomic](https://en.wikipedia.org/wiki/Linearizability) [test-and-set](https://en.wikipedia.org/wiki/Test-and-set) instruction provide the mutual exclusion. A process can [test-and-set](https://en.wikipedia.org/wiki/Test-and-set) on a location in shared memory, and since the operation is atomic, only one process can set the flag at a time. Any process that is  unsuccessful in setting the flag can either go on to do other tasks and  try again later, release the processor to another process and try again  later, or continue to loop while checking the flag until it is  successful in acquiring it. **[Preemption](https://en.wikipedia.org/wiki/Preemption_(computing)) is still possible, so this method allows the system to continue to function—even if a process halts while holding the lock.**

​	Several other atomic operations can be used to provide mutual exclusion of data structures; most notable of these is [compare-and-swap](https://en.wikipedia.org/wiki/Compare-and-swap) (CAS). **CAS can be used to achieve [wait-free](https://en.wikipedia.org/wiki/Wait-free) mutual exclusion for any shared data structure by creating a [linked list](https://en.wikipedia.org/wiki/Linked_list) where each node represents the desired operation to be performed.** CAS is then used to change the [pointers](https://en.wikipedia.org/wiki/Pointer_(computer_programming)) in the linked list[[6\]](https://en.wikipedia.org/wiki/Mutual_exclusion#cite_note-6) during the insertion of a new node. Only one process can be successful  in its CAS; all other processes attempting to add a node at the same  time will have to try again. <u>Each process can then keep a local copy of  the data structure, and upon traversing the linked list, can perform  each operation from the list on its local copy.</u>

##### Software solutions

​	In addition to hardware-supported solutions, some software solutions exist that use [busy waiting](https://en.wikipedia.org/wiki/Busy_waiting) to achieve mutual exclusion. Examples include:

- [Dekker's algorithm](https://en.wikipedia.org/wiki/Dekker's_algorithm)
- [Peterson's algorithm](https://en.wikipedia.org/wiki/Peterson's_algorithm)
- [Lamport's bakery algorithm](https://en.wikipedia.org/wiki/Lamport's_bakery_algorithm)[[7\]](https://en.wikipedia.org/wiki/Mutual_exclusion#cite_note-7)
- [Szymanski's algorithm](https://en.wikipedia.org/wiki/Szymanski's_algorithm)
- Taubenfeld's black-white bakery algorithm[[2\]](https://en.wikipedia.org/wiki/Mutual_exclusion#cite_note-Taubenfeld:2004-2)
- [Maekawa's algorithm](https://en.wikipedia.org/wiki/Maekawa's_algorithm)

​	**These algorithms do not work if [out-of-order execution](https://en.wikipedia.org/wiki/Out-of-order_execution) is used on the platform that executes them. Programmers have to specify strict ordering on the memory operations within a thread.**[[8\]](https://en.wikipedia.org/wiki/Mutual_exclusion#cite_note-8)

​	It is often preferable to use synchronization facilities provided by an [operating system](https://en.wikipedia.org/wiki/Operating_system)'s multithreading library, which will take advantage of hardware solutions if possible but will use software solutions if no hardware solutions  exist. **For example, when the operating system's [lock](https://en.wikipedia.org/wiki/Lock_(computer_science)) library is used and a thread tries to acquire an already acquired lock, the operating system could suspend the thread using a [context switch](https://en.wikipedia.org/wiki/Context_switch) and swap it out with another thread that is ready to be run, or could  put that processor into a low power state if there is no other thread  that can be run.** Therefore, <u>most modern mutual exclusion methods attempt to reduce [latency](https://en.wikipedia.org/wiki/Latency_(engineering)) and busy-waits by using queuing and context switches. However, if the  time that is spent suspending a thread and then restoring it can be  proven to be always more than the time that must be waited for a thread  to become ready to run after being blocked in a particular situation,  then [spinlocks](https://en.wikipedia.org/wiki/Spinlock) are an acceptable solution (for that situation only).</u>

##### Bound on the mutual exclusion problem

​	One binary test&set register is sufficient to provide the deadlock-free solution to the mutual exclusion problem. But a solution built with a test&set register can possibly lead to the starvation of some processes which become caught in the trying section. <u>In fact, Ω ( 根号n )  distinct memory states are required to avoid lockout. To avoid unbounded waiting, n distinct memory states are required.</u>

##### Recoverable mutual exclusion

​	Most algorithms for mutual exclusion are designed with the assumption  that no failure occurs while a process is running inside the critical  section. However, in reality such failures may be commonplace. For  example, a sudden loss of power or faulty interconnect might cause a  process in a critical section to experience an unrecoverable error or  otherwise be unable to continue. <u>If such a failure occurs, conventional, non-failure-tolerant mutual exclusion algorithms may deadlock or  otherwise fail key liveness properties.</u> To deal with this problem,  several solutions using crash-recovery mechanisms have been proposed.

##### Types of mutual exclusion devices

The solutions explained above can be used to build the synchronization primitives below:

- [Locks](https://en.wikipedia.org/wiki/Lock_(computer_science)) (mutexes)
- [Readers–writer locks](https://en.wikipedia.org/wiki/Readers–writer_lock)
- [Recursive locks](https://en.wikipedia.org/wiki/Reentrant_mutex)
- [Semaphores](https://en.wikipedia.org/wiki/Semaphore_(programming))
- [Monitors](https://en.wikipedia.org/wiki/Monitor_(synchronization))
- [Message passing](https://en.wikipedia.org/wiki/Message_passing)
- [Tuple space](https://en.wikipedia.org/wiki/Tuple_space)

​	Many forms of mutual exclusion have side-effects. For example, classic [semaphores](https://en.wikipedia.org/wiki/Semaphore_(programming)) permit [deadlocks](https://en.wikipedia.org/wiki/Deadlock), in which one process gets a semaphore, another process gets a second  semaphore, and then both wait till the other semaphore to be released.  Other common side-effects include [starvation](https://en.wikipedia.org/wiki/Resource_starvation), in which a process never gets sufficient resources to run to completion; [priority inversion](https://en.wikipedia.org/wiki/Priority_inversion), in which a higher-priority thread waits for a lower-priority thread;  and high latency, in which response to interrupts is not prompt.

​	Much research is aimed at eliminating the above effects, often with the goal of guaranteeing [non-blocking progress](https://en.wikipedia.org/wiki/Non-blocking_synchronization). No perfect scheme is known. Blocking system calls used to sleep an entire process. Until such calls became [threadsafe](https://en.wikipedia.org/wiki/Thread_safety), there was no proper mechanism for sleeping a single thread within a process (see [polling](https://en.wikipedia.org/wiki/Polling_(computer_science))).[*[citation needed](https://en.wikipedia.org/wiki/Wikipedia:Citation_needed)*]

---

> [Futex--wiki](https://en.wikipedia.org/wiki/Futex)

#### Futex--wiki

​	**In [computing](https://en.wikipedia.org/wiki/Computing), a futex (short for "fast userspace [mutex](https://en.wikipedia.org/wiki/Mutual_exclusion)") is a [kernel](https://en.wikipedia.org/wiki/Kernel_(operating_system)) [system call](https://en.wikipedia.org/wiki/System_call) that [programmers](https://en.wikipedia.org/wiki/Programmer) can use to implement basic [locking](https://en.wikipedia.org/wiki/Lock_(computer)), or as a building block for higher-level locking abstractions such as [semaphores](https://en.wikipedia.org/wiki/Semaphore_(programming)) and [POSIX](https://en.wikipedia.org/wiki/POSIX) mutexes or [condition variables](https://en.wikipedia.org/wiki/Condition_variable).**

​	**A futex consists of a [kernelspace](https://en.wikipedia.org/wiki/Kernel_(computing)) *wait queue* that is attached to an [atomic](https://en.wikipedia.org/wiki/Atomic_operations) [integer](https://en.wikipedia.org/wiki/Integer) in [userspace](https://en.wikipedia.org/wiki/Userspace).**  Multiple [processes](https://en.wikipedia.org/wiki/Process_(computing)) or [threads](https://en.wikipedia.org/wiki/Thread_(computer_science)) operate on the integer entirely in userspace (using [atomic operations](https://en.wikipedia.org/wiki/Atomic_operation) to avoid interfering with one another), and only resort to relatively expensive [system calls](https://en.wikipedia.org/wiki/System_call) to request operations on the wait queue (for example to wake up waiting processes, or to put the current process on the wait queue).  **A  properly programmed futex-based lock will not use system calls except  when the lock is contended; since most operations do not require  arbitration between processes,  this will not happen in most cases.**

##### History

​	On [Linux](https://en.wikipedia.org/wiki/Linux), Hubertus Franke ([IBM](https://en.wikipedia.org/wiki/IBM) [Thomas J. Watson Research Center](https://en.wikipedia.org/wiki/Thomas_J._Watson_Research_Center)), Matthew Kirkwood, [Ingo Molnár](https://en.wikipedia.org/wiki/Ingo_Molnár) ([Red Hat](https://en.wikipedia.org/wiki/Red_Hat)) and [Rusty Russell](https://en.wikipedia.org/wiki/Rusty_Russell) ([IBM Linux Technology Center](https://en.wikipedia.org/wiki/IBM_Linux_Technology_Center)) originated the futex mechanism. Futexes appeared for the first time in  version 2.5.7 of the Linux kernel development series; the semantics  stabilized as of version 2.5.40, and **futexes have been part of the [Linux kernel mainline](https://en.wikipedia.org/wiki/Linux_kernel_mainline) since the December 2003 release of 2.6.x stable kernel series.**

​	In 2002 discussions took place on a proposal to make futexes accessible via the file system by creating a special node in `/dev` or `/proc`. However, [Linus Torvalds](https://en.wikipedia.org/wiki/Linus_Torvalds) strongly opposed this idea and rejected any related patches.[[1\]](https://en.wikipedia.org/wiki/Futex#cite_note-1)

​	<u>Futexes have been implemented in Microsoft Windows since Windows 8 or Windows Server 2012 under the name WaitOnAddress</u>.[[2\]](https://en.wikipedia.org/wiki/Futex#cite_note-2)

​	In 2013 Microsoft patented futexes and the patent was granted in 2014.[[3\]](https://en.wikipedia.org/wiki/Futex#cite_note-3)

​	<u>In May 2014 the [CVE](https://en.wikipedia.org/wiki/Common_Vulnerabilities_and_Exposures) system announced a vulnerability discovered in the Linux kernel's futex subsystem that allowed denial-of-service attacks or local privilege  escalation.</u>[[4\]](https://en.wikipedia.org/wiki/Futex#cite_note-4)[[5\]](https://en.wikipedia.org/wiki/Futex#cite_note-5)

​	In May 2015 the [Linux kernel](https://en.wikipedia.org/wiki/Kernel_(operating_system)) <u>introduced a deadlock bug via [Commit b0c29f79ecea](https://github.com/torvalds/linux/commit/b0c29f79ecea) that caused a hang in user applications</u>. The bug affected many  enterprise Linux distributions, including 3.x and 4.x kernels, and Red  Hat Enterprise Linux version 5, 6 and 7, SUSE Linux 12 and Amazon Linux.[[6\]](https://en.wikipedia.org/wiki/Futex#cite_note-6)

​	Futexes have been implemented in OpenBSD since 2016.[[7\]](https://en.wikipedia.org/wiki/Futex#cite_note-7)

​	**The futex mechanism is one of the core concepts of the Zircon kernel[[8\]](https://en.wikipedia.org/wiki/Futex#cite_note-8) in [Google](https://en.wikipedia.org/wiki/Google)'s [Fuchsia operating system](https://en.wikipedia.org/wiki/Google_Fuchsia) since at least April 2018.**[[9\]](https://en.wikipedia.org/wiki/Futex#cite_note-9)

##### Operations

​	**Futexes have two basic operations, `WAIT` and `WAKE`. A third operation called `REQUEUE` is available and functions as a more generic `WAKE` operation that can move threads between waiting queues. [10]**

+ `WAIT(addr, val)`
  
  ​	If the value stored at the address `addr` is `val`, puts the current thread to sleep.
  
+ `WAKE(addr, num)`
  
  ​	Wakes up `num` number of threads waiting on the address `addr`.
  
+ `CMP_REQUEUE(old_addr, new_addr, num_wake, num_move, val)`
  
  ​	If the value stored at the address `old_addr` is `val`, wakes `num_wake` threads waiting on the address `old_addr`, and enqueues `num_move` threads waiting on the address `old_addr` to now wait on the address `new_addr`. **This can be used to avoid the thundering herd problem on wake**.[11][12]

> [futex(2) — Linux manual page](https://www.man7.org/linux/man-pages/man2/futex.2.html) ==> Linux中futex的详细实现，代码手册。
>
> ```c
> #include <linux/futex.h>
> #include <sys/time.h>
> 
> int futex(int *uaddr, int futex_op, int val,
>           const struct timespec *timeout,   /* or: uint32_t val2 */
>           int *uaddr2, int val3);
> ```
>
> The `futex()` system call provides a method for waiting until a certain condition becomes true.  It is typically used as a blocking construct in the context of shared-memory synchronization.  **When using futexes, the majority of the synchronization operations are performed in user space**.  **A user-space program employs the `futex()` system call only when it is likely that the program has to block for a longer time until the condition becomes true.**  Other `futex()` operations can be used to wake any processes or threads waiting for a particular condition.
----




> [CRITICAL_SECTION的详细说明](https://www.cnblogs.com/crj8812/p/4499058.html) <= 详细，建议阅读
>
> [硬件同步机制中test-and-set指令和swap指令实现的详解](https://blog.csdn.net/weixin_42333573/article/details/103571740)

# 10. 信号量、管程，实现同步互斥

## 10.0 背景

### 10.0.1 并发时的同步互斥问题

并发问题：竞争条件（竟态条件）

- 多线程并发导致资源竞争

同步

- 多线程共享公共数据的协调执行
- 包括互斥与条件同步
- 互斥：在同一时间只有一个线程可以执行临界区

确保同步正确很难？

- 需要高层次的编程抽象（如：锁）
- 从底层硬件支持编译（提供一些原子操作的机械指令）

基本同步方法

![img](https://img-blog.csdn.net/20180423103651896?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0FsYXRlYmxvb21lcg==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)

> [十、信号量和管程](https://blog.csdn.net/Alatebloomer/article/details/80047067)

## 10.1 信号量

### 10.1.1 信号量概述

​	如果需要多个进程/线程对临界区同时操作，就不能借助单纯的`test-and-set`实现的锁机制了。因为锁机制，只能保证多个进程/线程中的一个能够访问临界区，而不能保证多个<small>（自己设定的数量，比如5个）</small>。

​	信号量机制，可以处理临界区需要多个进程/线程同时访问的情况（访问临界区时也是互斥，多个进程之间同步）。

信号量是一种抽象数据类型

- 一个整形 (sem)，两个原子操作
- `P()` ：sem减1，如果`sem<0`，等待，否则继续
- `V() `：sem加1，如果`sem<=0`，唤醒一个等待的P

信号量类似铁路

- 初始化2个资源控制的信号灯

  *（火车进入临界区之前执行P操作，退出临界区时执行V操作。）*

![img](https://img-blog.csdn.net/20180423104729960?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0FsYXRlYmxvb21lcg==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)

---

Dijkstra在20世纪60年代提出

+ V：Verhoog（荷兰语增加）
+ P：Prolaag（荷兰语简称"Probeer te Verlagen"，或尝试减少）

在早期的操作系统是主要的同步原语

+ 例如，原Unix
+ 现在很少用（但在计算机科学研究，还是非常重要）

---

信号量的特性

- 信号量是**整数**<small>(一般设置成一个大于0的整形)</small>

- 信号量是**被保护**的变量

  + 初始化完成后，唯一改变一个信号量的值的办法是通过`P()`和`V()`

  + 操作必须是原子

- **`P()` 能够阻塞，`V()`不会阻塞**

- 我们假定假定信号量是"公平的"

  + 没有线程被阻塞在`p()`仍然阻塞，如果`V()`被无限频繁调用（在同一个信号量）

  + 在实践中，FIFO经常被使用

两种类型信号量

- **二进制信号量**：可以是0或1

- **一般/计数信号量**：可取任何非负值

  *两者相互表现（给定一个可以实现另一个）*

信号量可以用在2个方面

- **互斥**
- **条件同步（调度约束——一个线程等待另一个线程的事件发生）**

> [十、信号量和管程](https://blog.csdn.net/Alatebloomer/article/details/80047067)
>
> [挂起状态--百度百科]([https://baike.baidu.com/item/%E6%8C%82%E8%B5%B7%E7%8A%B6%E6%80%81/9359976?fr=aladdin](https://baike.baidu.com/item/挂起状态/9359976?fr=aladdin)) <= 回顾一下
>
> 挂起（等待,阻塞）进程在操作系统中可以定义为暂时被淘汰出内存的进程，机器的资源是有限的，在资源不足的情况下，操作系统对在内存中的程序进行合理的安排，其中有的进程被暂时调离出内存，当条件允许的时候，会被操作系统再次调回内存，重新进入等待被执行的状态即就绪态，系统在超过一定的时间没有任何动作。

### 10.1.2 信号量使用

#### 用二进制信号量实现的互斥

![img](https://img-blog.csdn.net/20180423110055767?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0FsYXRlYmxvb21lcg==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)

```none
mutex = new Semaphore(1);

mutex->P();
...
Critical Section;
...
mutex->V();
```

*(二进制信号量机制主要用于两个进程之间的互斥，里面的`P()`和`V()`操作同样需要保证实现原子性，不然也是白瞎。)*

---

#### 用二进制信号量实现的调度约束

![img](https://img-blog.csdn.net/20180423110314743?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0FsYXRlYmxvb21lcg==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)

```none
condition = new Semaphore(0);

// Thread A
...
condition->P();
...

// Thread B
...
condition->V();
...
```

*(同步操作，初始化信号量=0，那么A执行P操作阻塞，必须等B执行完某些代码后，B执行了V操作之后，A线程才能继续往下执行)*

`P()`等待，`V()`发出信号。

---

#### 生产者-消费者问题

一个线程等待另一个线程处理事情

+ 比如生产东西或者消费东西
+ 互斥（锁机制），是不够的

*(二进制信号量用于2个进程/线程的互斥和同步还可以，但是复杂的or多进程/线程之间的互斥、同步，就需要计数信号量来实现了，即信号量的值不限于0和1之间切换。)*

例如：有界缓冲区的生产者-消费者问题

- 一个或多个**生产者**产生数据将数据放在一个缓冲区里
- 单个**消费者**从缓冲区取出数据

+ 在任何一个时间**只有一个**生产者或消费者可访问缓冲区

```none
Producer→Buffer→Consumer
```

 ![img](https://img-blog.csdn.net/20180423110614323?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0FsYXRlYmxvb21lcg==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)

正确性要求

- 在任何一个时间只能有一个线程操作缓冲区（互斥）
- 当缓冲区空，消费者必须等待生产者（调度/同步约束）

+ 当缓冲区满，生产者必须等待消费者（调度/同步约束）

每个约束用一个单独的信号量

- 二进制信号量互斥
- 一般信号量`fullBuffers`
- 一般信号量`emptyBuffers`

```cpp
Class BoundedBuffer{
    mutex = new Semaphore(1); // (1)
    fullBuffers = new Semaphore(0); // (1)
    emptyBuffers = new Semaphore(n); // (1)
}
BoundedBuffer::Deposit(c){
    emptyBuffers->P(); // (3)
    mutex->P(); // (2)
    Add c to the buffer; // (1)
    mutex->V(); // (2)
    fullBuffers->V(); // (3)
}
BoundedBuffer::Remove(c){
    fullBuffers->P(); // (4)
    mutex->P(); // (2)
    Remove c from buffer; // (1)
    mutex->V(); // (2)
    emptyBuffers->V(); // (4)
}
```

按照步骤编写和思考：

1. `Class BoundedBuffer`里，用于**所有类别**进程间**互斥**访问临界区的**二进制信号量**`mutex`初始化1；用于**不同类别**进程间**同步**临界区资源的**条件信号量**`fullBuffers`初始化0；而**条件信号量**`emptyBuffers`同样是用于不同类别进程同步临界区资源的，初始化n，表示该需要被同步的资源允许某种类别的操作最多n次。<small>（针对临界区`buffer`的PV操作，其实也能对生产者/消费者的同类进程/线程之间起到一定的互斥作用，但是它主要作用是不同类别的进程/线程同步，互斥的效果可以说是附带的。）</small>

   `BoundedBuffer::Deposit(c)`里的`Add c to buffer`，表示生产者，不断往缓存区放入数据。这里`buffer`就是不同类别的进程之间需要同步的资源。(即生产者生产buffer资源，而消费者消费buffer资源；这两个不相同类别的进程，通过`buufer`这一个需要同步的资源，产生了关系。)

   `BoundedBuffer::Remove(c)`同理，里面是表示消费者，不断从缓存区取出数据。通过`buffer`这一临界区资源，**不同类别**的进程/线程（生产者和消费者）产生了关系。`buffer`两方都能访问，所以需要保证`buffer`的修改是同步的，即消费者/生产者修改后，生产者/消费者能够同步的获取到`buffer`的最新状态。

2. `BoundedBuffer::Deposit(c)`和`BoundedBuffer::Remove(c)`都使用`mutex->P()`和`mutex->V()`来保证对buffer的修改，在**所有类别**的进程之间是互斥的。即不管是消费者还是生产者，同一时刻，只能有一个进程/线程能够对`buffer`进行访问。

3. `BoundedBuffer::Deposit(c)`里的`emptyBuffers->P()`保证**同类别的生产者**至多有m个（m<=n）能够有机会（因为还有`mutex->P()`把关，所以只能说有机会）访问临界区资源`buffer`。而`fullBuffers->V()`保证不同类别的，即消费者有机会访问临界区资源`buffer`（如果`fullBuffers`维护的信号量<=0，那么只有生产者执行V操作后，原本阻塞/等待or被挂起的消费者，才可能被唤醒or重新加载到内存；如果`fullBuffers`维护的信号量>0，那么无需生产者执行V操作，消费者本身就有机会访问临界区`buffer`。同样，消费者访问`buffer`还需要经过`mutex->P()`把关，所以只是有机会访问临界区，不是绝对。）

4. `BoundedBuffer::Remove(c)`里的`fullBuffers->P()`和`emptyBuffers->V()`和编写`BoundedBuffer::Deposit(c)`里的`emptyBuffers->P()`、`fullBuffers->V()`作用类似，很好推敲，不再赘述。

![img](https://img-blog.csdn.net/20180423111331529?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0FsYXRlYmxvb21lcg==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)

P、V操作的顺序有影响吗？

生产者和消费者中的V顺序是可以调换的。但是P操作不行。

​	<u>即`BoundedBuffer::Deposit(c)`中原本最后两行的`mutex->V(); fullBuffers->V();`可以调换顺序变成`fullBuffers->V(); mutex->V();`，因为二进制信号量和条件信号量本身和锁类似，不管先释放哪个锁，只要能保证两个锁都释放就没问题。</u>

​	<u>而`BoundedBuffer::Deposit(c)`中原本的`emptyBuffers->P(); mutex->P();`不能调换顺序成`mutex->P(); emptyBuffers->P();`（可能导致死锁等严重问题）。</u>

​	假设修改了`BoundedBuffer::Deposit(c)`里面的2个P操作，那么可能出现所有生产者执行到`mutex->V()`之后马上切换进程，最后导致`emptyBuffers`管理的信号量\=\=0。当导致`emptyBuffers`管理的信号量=\=0的生产者执行完`mutex->V()`后，假设接下去执行的是消费者进程/线程。如果此时`fullBuffers`管理的信号量\=\=0，那么这个消费者连一个`fullBuffers->P();`操作都没法往下执行，就进入阻塞/等待队列or被挂起了。之后从阻塞队列里取出进程/线程预执行时，除非取出的是之前执行完`mutex->V()`的生产者继续往下执行`fullBuffers->V()`，不然其他新创建的消费者进程/线程，在第一步P操作就阻塞/等待or被挂起了，因为此时`fullBuffers`各自管理的信号量都==0，连`fullBuffers->V()`都过不去；而如果执行的是其他新创建的生产者进程/线程，由于生产者P操作顺序对调了，那么生产者执行`mutex->P(); emptuBuffers->P();`，卡在`emptuBuffers->P();`，这个更严重，因为占有二进制信号量`mutex`没有释放。（如果有人不知道，还继续新建多个生产者or消费者进程/线程，那么很可能长时间处于这种死锁状态，导致CPU资源被浪费=>每次执行的生产者or消费者进程都卡在P操作无法继续往下）

> [十、信号量和管程](https://blog.csdn.net/Alatebloomer/article/details/80047067)

### 10.1.3 信号量机制的实现

使用硬件原语

+ 禁用中断
+ 原子指令（test-and-set）

类似锁

例如：使用"禁用中断"

```cpp
class Semaphore{
    int sem;
    WaitQueue q; // 等待队列
}
Semaphore::P(){
    Add this thread t to q;
    block(p);
}
Semaphore::V(){
    sem++;
    if(sem<=0){
        Remove a thread t from q;
        wakeup(t);
    }
}
```

*(同样需要保证指令具有原子性，不然进程/线程切换，同样不能保证互斥or同步)*

*（类似前面的Lock，同样需要依赖硬件支持，比如提供原子性的机械码指令。前面说过，纯软件以算法的形式实现软件的互斥、同步，开销大，而且需要进程/线程忙等，一直spin自旋判断是否满足某些条件，占用CPU资源）*

![img](https://img-blog.csdn.net/20180423112347685?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0FsYXRlYmxvb21lcg==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)

信号量的双用途

- 互斥和条件同步
- 但是等待条件是独立的互斥

读/开发代码比较困难

+ 程序员需要非常精通信号量

容易出错

+ 使用的信号量已经被另一个线程占用
+ 忘记释放信号量

不能够处理死锁问题

> [十、信号量和管程](https://blog.csdn.net/Alatebloomer/article/details/80047067)
>
> [PV操作原理和实现](https://blog.csdn.net/s2152637/article/details/102466060) <= 推荐，不错、简洁的文章

### 10.1.4 Linux信号量机制(优质网文转载)

> [【原创】Linux信号量机制分析](https://www.cnblogs.com/LoyenWang/p/12907230.html)
>
> 1. 读写信号量的特性可以与读写自旋锁进行类比（读者与读者并发、读者与写者互斥、写者与写者互斥），区别在于读写信号量可能会发生睡眠，进而带来进程切换的开销；
> 2. 为了优化读写信号量的性能，引入了`MCS锁`机制，进一步减少切换开销。第一个写者获取了锁后，第二个写者去获取时自旋等待，而读者去获取时则会进入睡眠；
> 3. 读写信号量的`count`值很关键，代表着读写信号量不同状态的切换，因此也决定了执行流程；
> 4. 读者或写者释放锁的时候，去唤醒等待列表中的任务，需要分情况处理。等待列表中可能存放的是读者与写者的组合，如果第一个任务是写者，则直接唤醒该写者，否则将唤醒排在前边的连续几个读者；

## 10.2 管程

### 10.2.1 管程-概述

*<small>(管程抽象程度比信号量高，这也意味着程序员用来解决互斥、同步问题，更简单了)</small>*

目的：分离互斥和条件同步的关注

什么是管程<small>(本来是高级语言Java针对并发编程而实现的，后面也引入到操作系统层面中)</small>

+ 一个锁：指定临界区<small>(所有要访问管程管理的共享变量的函数，都只能是在一个线程中)</small>

- 0或者多个条件变量：等待/通知信号量用于管理并发访问共享数据<small>(管程，包含了一堆的共享变量，以及对这些共享变量管理、操作的一系列函数的互斥同步模块。一些资源得不到满足的线程，被挂起，挂到条件变量上。)</small>

一般方法

- 收集在对象/模块中的相关共享数据
- 定义方法来访问共享数据

![img](https://img-blog.csdn.net/20180423133944700?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0FsYXRlYmxvb21lcg==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)

<small>互斥访问，没访问到共享资源的，进入等待队列。x、y表示访问不同资源时需要满足的不同条件。</small>

<small>进入管程，这个动作本身是互斥的。（也就是保证获取锁的动作是互斥的）</small>

> [十、信号量和管程](https://blog.csdn.net/Alatebloomer/article/details/80047067)

### 10.2.2 管程的实现

> [管程--百度百科]([https://baike.baidu.com/item/%E7%AE%A1%E7%A8%8B/10503922?fr=aladdin](https://baike.baidu.com/item/管程/10503922?fr=aladdin))
>
> [Java并发--lock锁详解](https://www.cnblogs.com/bsjl/p/7654618.html)

**Lock**

+ `Lock::Acquire()` - 等待直到锁可用，然后抢占锁
+ `Lock::Release()` - 释放锁，唤醒等待者（如果有）

*<small>(Lock确保管程里面的函数操作都是互斥的。Lock可以通过语言级的保障，不写也行，只要语言能够自动嵌入的话=>指自动帮你在汇编代码之类的天上原子性的机械码指令Lock操作之类的。不过就java来说，锁等机制的使用还是需要自己手动写的)</small>*

**Condition Variable**

+ 允许等待状态进入临界区
  + 允许处于等待（睡眠）的线程进入临界区
  + 某个时刻原子释放锁进入睡眠

*<small>(可以看出，条件变量本身实现互斥、同步的方式，和信号量机制类似)</small>*

**Wait() operation**

- 释放锁，睡眠，重新获得锁返回后

**Signal() operation (or broadcast() operation)**

- 唤醒等待者（或者所有等待者），如果有。

---

#### 条件变量的实现

+ 需要维持每个条件队列
+ 线程等待的条件等待`signal()`

```cpp
Class Condition{
    int numWaiting = 0;
    WaitQueue q;
}
Condition::Wait(lock){
    numWaiting++;
    Add this thread t to q;
    release(lock);
    schedule(); // need mutex
    require(lock);
}
Condition::Siginal(){
    if(numWaiting>0){
        Remove a thread t from q;
        wakeup(t); // need mutex
        numWaiting--;
    }
}
```

信号量机制和条件变量的差异：

+ 信号量的PV操作分别对应`signal--`和`signal++`，且`signal`本身就近似资源；而条件变量的Wait和Signal操作分别对应`numWaiting++`和`numWaiting--`，而`numWaiting`表示的是等待队列中的成员个数
+ 信号量机制中的P近似加锁，而V近似解锁；条件变量的`Wait()`先解锁后加锁，而`Signal()`不涉及锁操作

*（后续的介绍会得知，条件变量机制的`Wait()`使用前往往需要先加锁，所以`Wait()`里面先释放锁`release(lock)`，然后进入等待队列，让CPU重新调度，可能调度到自己也可能是别的进程/线程，等下次调度到自己的时候，正好`Wait()`里`schedule()`下一步执行的就是`require(lock)`，也就是自己重新获得锁。）*

*（`Signal()`中的`wakeup(t)`，是从上一步`Remove a thread t from q`即等待队列中获取一个线程，并让它被唤醒从等待状态编程就绪态（进入就绪队列），即之后有机会被CPU调度并继续`Wait(lock)`的下一步`require(lock)`。）*

t![img](https://img-blog.csdn.net/20180423134607390?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0FsYXRlYmxvb21lcg==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)

> [十、信号量和管程](https://blog.csdn.net/Alatebloomer/article/details/80047067)

### 10.2.3 管程机制——生产者-消费者问题

从生产者-消费者问题，比较信号量机制和管程条件变量的异同点。

```cpp
class BoundedBuffer{
    ...
    Lock lock; // (1)
    int count = 0; // (1)
    Condition notFull,notEmpty; // (1)
}
BoundedBuffer::Deposit(c){
    lock->Acquire(); // (2)
    while(count==n) // (3)
        notFull.Wait(&lock); // (3)
    Add c to the buffer; // (1)
    count++; // (1)
    notEmpty.Signal(); // (4)
    lock->Release(); // (2)
}
BoundedBuffer::Remove(c){
    lock->Acquire(); // (2)
    while(count==0) // (4)
        notEmpty.Wait(&lock); // (4)
    Remove c from buffer; // (1)
    count--; // (1)
    notFull.Signal(); // (3)
    lock->Release(); // (2)
}
```

按照步骤编写和思考：

1. `BoundedBuffer`即充当临界区资源，所以带有锁`Lock lock`。在`Deposit(c)`或`Remove(c)`中执行`lock->Acquire()`后，管程保证只有一个线程/进程能够继续往下执行该函数（`Deposit(c)`或`Remove(c)`）。`count=0`标记当前`BoundedBuffer`临界区中有几份可用资源（生产者生产资源，消费者消费资源）。`Condition notFull,notEmpty;`两个条件变量分别表示临界区中的资源"没有达到上限值"和"没有用完"。*（Lock操作需要硬件支持，即底层需要由硬件保证Lock操作的加锁、解锁具有原子性。比如底层Lock通过具有原子性的机械指令来实现等...）*

   `BoundedBuffer::Deposit(c)`，生产者需要生产资源，所以`Add c to the buffer; count++`，表示执行生产资源的代码，并且临界区中资源的剩余个数增加1个。

   `BoundedBuffer::Remove(c)`，消费者需要消费资源，所以`Remove c from buffer; count--`，表示执行消费资源的代码，并且临界区中资源的剩余个数减少1个。

2. `BoundedBuffer::Deposit(c)`和`BoundedBuffer::Remove(c)`的函数开头和结尾添加配对的`lock->Acquire();`和`lock->Release();`。根据管程的定义中，多个线程，只能有一个线程真正访问到管程管理的函数。所以不管有几个线程正想要执行`Deposit(c)`或`Remove(c)`，都只能有一个线程能够真正往下执行`Deposit(c)`或`Remove(c)`，即同一时刻下只能是某个生产者在生产资料or某个消费者在消费资料。因为这里`Lock lock`是类成员变量，所以`Deposit(c)`和`Remove(c)`中的`lock->Acquire()`加锁的是同一个锁。如果这里`Lock lock`是生产者和消费者函数里的局部变量，就无法保证每个线程里的生产者or消费者执行是互斥的，因为局部变量每个函数里的都不同，`局部变量的lock`是临时放入线程独自的栈中的，加锁也是白加。

   函数头的`lock->Acquire()`，对类成员变量`Lock lock`加锁，则同一时刻，只能有一个线程在执行`Deposit(c)`、`Remove(c)`其中一个函数。

   而函数尾部的`lock->Release()`，对类成员变量`lock`解锁，即接下去所有其他就绪的线程都有机会执行`lock->Acquire()`。

   成对的加锁和解锁，保证了`BoundedBUffer`里面的`Deposit(c)`和`Remove(c)`的函数执行具有互斥性，同一时刻只能存在一个线程正好在执行`Deposit(c)`或者正好在执行`Remove(c)`。

3. `BoundedBuffer::Deposit(c)`里的`while(count == n) notFull.Wait(&lock);`，回顾前面`10.2.2 管程的实现中的代码 Condition::Wait(lock)`，会发现`Wait(lock)`是先把当前线程加入等待队列，然后释放锁，让CPU重新调度进程/线程。所以这里如果临界区资源达到上限值`n`，生产者线程就暂时停止生产，释放锁，让消费者线程有机会得到调度（如果存在消费者线程的话）。如果生产者执行了`notFull.wait(&lock)`后，CPU再次调度到某个生产者，那么会重复`Deposit(c)`中的`lock->Acquire();`加锁操作和接下去的`notFull.Wait(&lock)`中的`Condition::Wait(lock)`内部的`release(lock)`释放锁。

   `BoundedBuffer::Remove(c)`里的`notFull.Signal()`，回顾前面的`Condition::Siginal()`，就知道`Signal()`会移除等待队列中的一个线程，唤醒它（让线程转为就绪态）。这样子如果等待队列中存在因为`notFull`条件而待在等待队列中的生产者线程，那么就使该等待队列中其中一个生产者线程进入就绪态（通常简单的等待队列以FIFO方式实现，也就是等待队列中位于队列首部的生产者线程会转变成就绪态，之后有机会被CPU调度）。

   `Deposit(c)`中的`notFull.Wait(&lock)`和`Remove(c)`中的`notFull.Signal()`是想对应的。可以说是因为设置了`Deposit(c)`中的`notFull.Wait(&lock)`，然后就需要配对地在`Remove(c)`中添加一行`notFull.Signal()`。

   条件变量的`Wait()`和`Singal()`不要求数量上一一对应，完全可以N个线程执行`Wait()`，而M个线程执行`Signal()`。

4. `BoundedBuffer::Remove(c)`里的`while(count == n) notEmpty.Wait(&lock);`同理对应`BoundedBuffer::Deposit(c)`里的`notEmpty.Signal();`。通常可以理解为是先设置了前者`Wait(lock)`，才会需要在另一个函数里配对设置`Signal()`。这里表达的意思即消费者如果临界区的资源个数不够了，就停止消费，直到有生产者又新生产资源后，才通知消费者可以继续消费了。其余细节逻辑和第3步添加的代码类似，不再赘述。

![img](https://img-blog.csdn.net/20180423135306829?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0FsYXRlYmxvb21lcg==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)

> [十、信号量和管程](https://blog.csdn.net/Alatebloomer/article/details/80047067)

### 10.2.4 管程条件变量的Signal两种实现执行方案

根据前面的`Condition::Signal()`代码可知，`Signal()`会唤醒一个等待队列中的线程，使其转变为就绪态。

这里就引出一个问题，即**用户态线程**执行`Signal()`后，什么时候阻塞该用户线程切换内核态，进行**内核态管理**的等待队列的线程唤醒和CPU重新选择进程/线程进行调度执行？

业界有两种方案：<small>（其中Hansen-style是最常见的实现，不管是常见的通用操作系统还是Java语言内部对管程的实现。）</small>

+ Hansen-style（most real OSes，or Java，Mesa）
+ Hoare-style（most textbooks）

```none
// Hansen-style

# T1
lock.acquire()
...
x.wait()
// T1 blocks
				// T2 starts
				# T2
				lock.acquire()
				...
				x.signal()
				...
				lock.release()
				// T2 finishes
// T1 resumes
# T1
...
lock.release()
```

```none
// Hoare-style

# T1
lock.acquire()
...
x.wait()
// T1 blocks
				// T2 starts
				# T2
				lock.acquire()
				...
				x.signal()
				// T2 blocks
// T1 resumes
# T1
...
lock.release()
// T1 finishes
				// T2 resumes
				...
				lock.release()
```

两种方案，**Hansen-style实现更简单；而Hoare-style实现较复杂，需要硬件配合进行复杂的线程控制实现(因为Signal()执行完后立即阻塞当前线程，并需要操作系统能保证CPU下一个调度的一定是Signal()从等待队列中唤醒的进入就绪态的线程)，但是相对更直观**。（Hansen-style使用更普遍，毕竟更容易实现。）

Hansen-style由于当前线程需要等到`lock.release()`执行后，其他就绪的or高优先级阻塞态的进程/线程才有机会占用CPU，不能保证下个被CPU调度的一定是`Signal()`所唤醒的那一个线程。因为在当前线程释放锁之后，被唤醒的线程和其他就绪态线程需要竞争抢占CPU。

而Hoare-style执行`Signal()`后就唤醒线程使其进入就绪态，并要求CPU接下去就调度这个被刚被唤醒的线程。之所以实现复杂，除了必须保证这个CPU执行顺序以外，还有就是原本执行`Signal()`的线程还没有执行`lock.release()`释放锁，即<u>操作系统和硬件需要复杂实现，来支持被唤醒的这个线程能够在唤醒它的那个线程所拥有的锁内继续执行代码</u>。

+ Hansen-style

  + Signal is only a "hint" that the condition may be true
  + Need to check again

  + Benefits
    + Efficient implementation

+ Hoare-style

  + Cleaner，good for proofs
  + When a condition variable is signaled，it does not change

  + But
    + Inefficient implementation

![img](https://img-blog.csdn.net/20180423140351291?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0FsYXRlYmxvb21lcg==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)

Hansen-style和Hoare-style，在前面生产者-消费者问题中，生产者`Deposit()`的不同代码实现方式：

```cpp
// Hansen-style (也就是我们前面代码所编写的方式)
Deposit(){
    lock->acquire();
    while(count==n){
        notFull.wait(&lock);
    }
    Add thing;
    count++;
    notEmpty.signal();
    lock->release();
}

// Hoare-style
Deposit(){
    lock->acquire();
    if(count==n){
        notFull.wait(&lock);
    }
    Add thing;
    count++;
    notEmpty.signal();
    lock->release();
}
```

很明显，Hasen-style和Hoare-style的`Deposit()`实现，只有一行区别，即判断`coun==n`前者用`while`，后者用`if`。

（因为Hasen-style没法保证消费者执行`notFull.signal()`唤醒的生产者线程一定是下一个被CPU调度的，进而没法保证下次CPU调度某个线程执行后，`count<n`仍然成立，因为可能下一个调度的线程，正好是某个就绪态的生产者，进而导致`count==n`，如果这之后CPU正好调度之前唤醒的生产者线程，那么它理应再次执行`notFull.wait(&lock)`，如果用`if`那么就直接往下了，而此时`count==n`，如果直接往下继续执行`count++`会导致资源数量溢出，可能触发某些异常）

（而Hoare-style保证`Signal()`唤醒的线程一定在下一次CPU调度中被选上，如果当前生产者线程本来就`count<n`那不进入`if`语句；如果当前`count==n`，并且有好几个生产者都先后被CPU调度，都执行了`notFull.wait(&lock)`，那么这些生产者线程都进入等待队列，需要等待消费者线程来唤醒。由于Hoare-style保证消费者的`Signal()`执行后马上阻塞自身线程，并保证刚唤醒的生产者线程一定下次就被CPU调度，假设等待队列采用FIFO实现，那么等待队列队首生产者线程一定先于其他不在等待队列中的就绪态生产者线程执行，所以此时一定是`count==n-1`，即被唤醒的生产者一定不会需要再次执行`notFull.wait(&lock)`，所以只用`if`语句来判断`count==n`就够了，用不着`while`）

<small>单核CPU要实现Hoare-style的管程机制就够复杂了，现在都是多核CPU了，就更别说多复杂了，毕竟`Lock`还需要考虑多核之间互斥，即可能需要给总线加锁。(x86平台上，为了保证硬件定下来的原子操作机械指令在多核CPU下仍然具有原子性，CPU提供了在指令执行期间对总线加锁的手段，这样同一总线上别的CPU核就暂时不能通过总线访问内存了，进而保证了这条指令在多处理器环境中的原子性。）</small>

+ **Hansen管程**<small>（操作系统实现通常都采用这种，Java语言也是用的这种）</small>

  + 条件变量释放仅是一个提示
  + 需要重新检查条件

  + 特点：**高效**

+ Hoare管程

  + 条件变量释放同时表示放弃管程访问
  + 释放后条件变量的状态可用

  + 特点：低效

![img](https://img-blog.csdn.net/20180423140409795?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0FsYXRlYmxvb21lcg==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)

> [十、信号量和管程](https://blog.csdn.net/Alatebloomer/article/details/80047067)

### 10.3 并发编程问题

### 10.3.0 引言、背景

通过前面的学习，我们知道，进程/线程间的同步、互斥，可以通过**信号量<small>(忙等自旋or非忙等阻塞)</small>or管程<small>（条件变量）</small>机制**来实现。

**信号量、条件变量、锁等都是高层抽象，需要硬件支持（禁用中断、原子指令、原子操作等），才能正常工作**。

而我们并发编程，就是用的高层抽象（信号量、锁、条件变量等）去实现，而忽略底层硬件的细节。

![img](https://img-blog.csdn.net/20180423141155183?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0FsYXRlYmxvb21lcg==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)

开发/调试并行程序很难

+ 非确定性的交叉指令

同步结构

- 锁：互斥
- 条件变量：有条件的同步
- 其他原语：信号量
- 怎样有效的使用这些结构？
  + 制定并遵循严格的程序设计风格/策略

> [十、信号量和管程](https://blog.csdn.net/Alatebloomer/article/details/80047067)

### 10.3.1 读者写者问题

1. 动机
   + 共享数据的访问

2. 两种类型使用者

   + 读者：不需要修改数据

   + 写者：读取和修改数据

3. 问题的约束

   + **允许同一时间有多个读者**，但在任何时候只有一个写者<small>*(因为读操作不会破坏数据原有的样子)*</small>

   + 当没有写者时读者才能访问数据

   + 当没有读者和写者时写者才能访问数据

   + 在任何时候只能有一个线程可以操作共享变量

4. 多个并发进程的数据集共享

   + 读者：只读数据集；他们不执行任何更新
   + 写者：可以读取和写入

5. 共享数据

   + 数据集
   + 信号量CountMutex初始化为1
   + 信号量WriteMutex初始化为1<small>*（用于写者互斥，保证同一时刻只能有一个写者）*</small>
   + 整数Rcount初始化为0<small>*(Rcount代表读者的数量，这里写者同一时刻只能有1个，所以不需要特地标识。从0开始，信号量机制，保证互斥)*</small>

---

#### 读者优先——信号量机制实现

```cpp
// Writer
sem_wait(WriteMutex); // (1)
write; // (1)
sem_post(WriteMutex); // (1)

// Reader
sem_wait(CountMutex); // (3)
 if(Rcount==0) // (2)
     sem_wait(WriteMutex); // (1)
 ++Rcount; // (2)
sem_post(CountMutex); // (3)
read; // (1)
sem_wait(Count_Mutex); // (3)
--Rcount; // (2)
if(Rcount==0) // (2)
    sem_post(WriteMutex); // (1)
sem_post(CountMutex); // (3)
```

这里`sem_wait`相当于P操作，而`sem_post`相当于V操作。

按照步骤编写和思考：

1. 因为"允许同一时间有多个读者，但是在任何时候只有一个写者"，所以`写者Writer`中`write;`前后用`sem_wait(WriteMutex);`和`sem_post(WriteMutex);`包裹，`WriteMutex`初始值1，保证写者线程互斥；

   因为"当没有写者时读者才能访问数据"，所以`读者Reader`的`read;`用`sem_wait(WriteMutex);`和`sem_post(WriteMutex);`包裹，表示一旦有读者读了，那么其他写者也就没法写了，读者和写者互斥。

2. `读者Reader`在对`WriteMutex`的PV操作前添加`if(Rcount==0)`，并在`read;`前后添加`++Rcount;`和`--Rcount;`，因为"允许同一时间有多个读者，但在任何时候只有一个写者"、"没有写者时读者才能访问数据"，所以当执行的是读者线程时，只有该读者时最后一个，即`--Rcount;`后`Rcount==0`，才执行对写者的V操作，即`if(Rcount == 0) sem_post(WriteMutex);`。而某一个时刻，正好等待队列中没有读者`Rcount==0`，且存在多个就绪态读者和多个就绪态写者线程抢占CPU，如果这时候读者抢占到了，那么由于"当没有读者和写者时写者才能访问数据"，所以`if(Rcount==0) sem_wait(WriteMutex);`，即多个就绪态读者只有第一个抢先于写者得到CPU的需要考虑`sem_wait(WriteMutex);`之后在该读者线程执行结束前，可以有多个其他就绪态读者线程也执行`read;`之前的几行代码。

   `++Rcount;`和`--Rcount`分别是读者线程`read;`读操作之前对当前读者数量计数+1或-1。需要注意的就是`++Rcount;`必须在`sem_wait(WriteMutex);`之后，因为只有确保同一时刻下写者没法执行(被读者执行了P操作)，读者才可以执行关于自己读者逻辑的操作哦。

3. 由于`Rcount`读者数量，相对所有读者线程来说，是共享资源，所有对`Rcount`的修改，必须用PV操作保证互斥。在`++Rcount;`和`--Rcount;`前后添加`sem_wait(CountMutex);`和`sem_post(CountMutex);`。这里需要注意的时，读者对`CountMutex`的PV操作其实连"对写者`WriteMutex`的PV操作"也包裹在内了，因为读者线程中对写者的PV操作，必须保证是当时某个时间段内系统中没有任何运行态的读者，而多个就绪态的读者和多个就绪态写者竞争CPU且最后是某个读者抢到CPU并为当时那个时间段内头一个进入运行态的读者，这个"第一个运行态的读者"才有资格对写者进行P操作；而只有这个时间段内"最后一个运行态的读者"进行`--Rcount;`使得`Rcount==0`时，才有资格对写者执行V操作，使得写者有机会得到CPU。

   在读者线程中，写者的PV操作和自身`Rcount`读者数量修改，对所有读者来说都是共享资源，所以需要用读者的PV操作保护，保证互斥。（某一时间段内可以有多个读者；只有没有读者运行时，写者才有可能运行）

大致概括：（1）通过`sem_wait(WriteMutex)`实现写者-写者、写者-读者互斥；（2）通过添加`if(Rcount==0)`、`++Rcount`和`--Rcount`完成多个读者的读操作，且确保读者优先于写者。（因为读者抢占CPU后，在读者执行完之前，允许继续来多个读者抢占CPU，而写者必须等到最后一个读者执行完后，才有机会继续和就绪态读者抢占CPU）；（3）通过`sem_wait(CountMutex)`实现读者-读者互斥，确保"读者数量修改"和"对写者PV操作"是读者之间互斥的。

*（写者因为同一时刻只允许出现一个，所以无需记录写者数量。）*

![img](https://img-blog.csdn.net/20180423150229678?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0FsYXRlYmxvb21lcg==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)

----

读者优先策略

- 只要有一个读者处于活动状态，后来的读者都会被接纳
- 如果读者源源不断地出现的话，那么写者就时钟处于阻塞状态（饥饿）

写者优先策略

- 一旦写者就绪，那么写者会尽可能快地执行写操作
- 如果写者源源不断地出现的话，那么读者就始终处于阻塞状态

----

#### 写者优先——管程机制实现

读方法、写方法，以及管程的状态变量、条件变量。

```cpp
# Basic structure: two methods
// 读方法
Database::Read(){
    Wait Util no writers;
    read database;
    check out - wake up waiting writers;
}
// 写方法
Database::Write(){
    Wait until no readers/writers;
    write database;
    check out - wake up waiting readers/writers;
}

# Monitor's State variables
// 管程的状态变量、条件变量
AR = 0;			// # of active readers
AW = 0;			// # of active writers
WR = 0;			// # of waiting readers
WW = 0;			// # of waiting writers
Condition okToRead; // 字面义，读者可以进行读操作了
Condition okToWrite; // 字面义，写者可以进行写操作了
Lock lock; // 前面说了，管程要确保只有一个管程管理的函数能真正访问到临界区。
```

用"管程"机制实现读者-写者问题的**写者优先**形式。

这里读者需要等待的写者有两种：

1. 正在进行写操作的写者
2. 在等待队列中等待要进行写操作的写者

只要存在上面其中一种写者，就轮不到读者占用CPU。

![img](https://img-blog.csdn.net/20180423152142420?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0FsYXRlYmxvb21lcg==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)

```cpp
// 完整的读者Reader逻辑
Public Database::Read(){
    // Wait until no writers;
    StartRead();
    read database;
    // check out -wake up waiting writers;
    DoneRead();
}
Private Database::StartRead(){
    lock.Acquire(); // (1)
    while((AW+WW)>0){ // (2)
        WR++; // (2)
            okToRead.wait(&lock); // (2)
        WR--; // (2)
    }
    AR++; // (2)
    lock.Release(); // (1)
}
Private Database::DoneRead(){
    lock.Acquire(); // (1)
    AR--; // (3)
    if(AR==0&&WW>0){ // (3)
        okToWrite.signal(); // (3)
    }
    lock.Release(); // (1)
}
```

按照步骤编写和思考：

1. 管程机制，要保证管程管理的函数都是互斥的，所以函数头和尾各自添加`lock.Acquire();`和`lock.Release();`。这里`lock`同样是成员变量，所以同一时刻只可能有一个线程成功执行`Database::StartRead()`或`Database::DoneRead()`中的`lock.Acquire();`。

2. `Database::StartRead()`在`lock.Release()`之前添加`AR++;`，表示读者将进行读操作了，所以活跃的读者数量+1。而这里实现的是写者优先，所以存在活跃的写者`AW`or等待的写者`WW`时，读者都必须等待。所以`while((AW+WW)>0){WR++;okToRead.wait(&lock);WR--;}`，直到有写者唤醒等待队列中的写者/读者线程，正好唤醒的是读者时，读者才进入就绪态竞争CPU资源。

   这里`okToRead.wait(&lock)`外层用`while`而不是`if`，因为前面说过java语言和常见操作系统采用`Hasen-style`实现`Signal()`的执行策略，所以当`(AW+WW)>0`为`ture`时，原本等待队列中的读者线程并不是马上被写者唤醒，而是需要等写者执行完`lock.Release()`后，才会真正唤醒等待队列中的某个读者，这时候该读者需要和其他就绪态的写者和读者竞争CPU资源，不能保证接下去一定是被唤醒的该读者占用到CPU并执行了`wait()`内部`schedule();`的下一步`lock.Acquire()`占用到锁。

3. `Database::DoneRead()`首先在获取锁后，就执行`AR--;`，因为此时读者已经执行完读操作，所以让"活跃的读者数量"-1。再者，需要考虑读者什么时候唤醒写者，前面信号量实现读者优先时就知道了，这里应该等到最后一个活跃的读者`AR`执行完操作并且当前等待队列中存在等待中的写者`WW`时，才需要唤醒写者。即`if(AR==0&&WW>0){okToWrite.signal();}`。这里唤醒逻辑外层用`if`而不是`while`，因为同一时刻只可能有一个函数获得`lock`锁，即这里只可能是该函数正在执行，这里要么之前等待队列中就存在等待的写者，要么之前就没有等待的写者（指`AR--;`执行完后的这个短时间内）。

![img](https://img-blog.csdn.net/201804231522010?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0FsYXRlYmxvb21lcg==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)

```cpp
// 完整的写者Writer逻辑
Public Database::Write(){
    // Wait until no readers/writers;
    StartWrite();
    write database;
    // check out -wake up waiting readers/writers;
    DoneWrite();
}
Private Database::StartWrite(){
    lock.Acquire(); // (1)
    while((AW+AR)>0){ // (2)
        WW++; // (2)
            okToWrite.wait(&lock); // (2)
        WW--; // (2)
    }
    AW++; // (2)
    lock.Release(); // (1)
}
Private Database::DoneWrite(){
    lock.Acquire(); // (1)
    AW--; // (3)
    if(WW>0){ // (3)
        okToWrite.signal(); // (3)
    }
    else if(WR>0){ // (3)
        okToRead.broadcast(); // (3)
    }
    lock.Release(); // (1)
}
```

按照步骤编写和思考：

1. `Database::StartWrite()`和`Database::DoneWrite()`同样因为管程机制，需要在函数开头结尾添加`lock.Acquire();`和`lock.Release();`保证同一时刻只可能有一个线程在其中一个函数中获取到`lock`锁，进而保证互斥。

2. `Database::StartWrite()`因为接下去要执行写操作了，所以在`lock.Release();`之前添加`AW++;`表示当前活跃的写者数量+1。而在这之前，写者需要判断什么时候能够被唤醒or执行`AW++;`。因为这里是写者优先，且同一时刻仅允许一个活跃的写者，所以需要等待没有活跃的写者or活跃的读者时，才能写。

   和前面读者的`Database::StartRead()`不同的是，读者需要等到完全没写者（活跃的写者or等待的写者）才能继续执行；而这里写者只等待活跃的写者or活跃的读者，不考虑任何等待队列中的线程。

3. `Database::DoneWrite()`的`AR--;`因为写者进行完写操作了，所以活跃的写者数量-1。而写者同样需要考虑什么时候唤醒等待队列中的线程。这里因为写者优先，所以先`if(WW>0){okToWrite.signal();}`，即先考虑是否有等待的写者，如果有，则唤醒一个（因为同一时刻只允许一个正在写的写者，所以只唤醒一个）；而接下去才`else if(WR>0){okToRead.broadcast();}`，即先考虑完是否有等待的写者后，再考虑是否有等待的读者。因为允许存在多个活跃的读者，所以这里用`broadcast()`，直接让当时等待队列中所有的读者都被唤醒，都有机会抢占CPU进而获得锁。

   这里体现写者优先的，主要是先`if(WW>0)`后`else if(WR>0)`，这样一定是等待的写者最先有机会被唤醒进而进入就绪态。

![img](https://img-blog.csdn.net/20180423153006780?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0FsYXRlYmxvb21lcg==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)

> [十、信号量和管程](https://blog.csdn.net/Alatebloomer/article/details/80047067)

### 10.3.2 哲学家就餐问题

#### 问题描述

（1965年由Dijkstra首先提出并解决）5个哲学家围绕一张圆桌而坐，桌子上放着5支叉子，每两个哲学家之间放一支；哲学家的动作包括思考和进餐，进餐时需要同时拿起他左边和右边的两支叉子，思考则同时将两支叉子放回原处。如何保证哲学家们的动作有序进行？如：不出现有人永远拿不到叉子。

![img](https://img-blog.csdn.net/20180423153920587?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0FsYXRlYmxvb21lcg==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)

共享数据

- Bowl of rice（data set）
- Semphore fork[5]  initialized to 1

`take fork(i)`：`P(fork[i])`    `put fork(i)`：`V(fork[i])`

---

下面是几种典型的**错误样例**（不正确or不合适）

+ 方案1：

  ```cpp
  #define N 5 // 哲学家个数
  void philosopher(int i) // 哲学家编号 0~4
      
      while(TRUE)
      {
          think(); // 哲学家在思考
          take_fork(i); // 去拿左边的叉子
          take_fork((i+1)%N); // 去拿右边的叉子
          eat(); // 吃面条...
          put_fork(i); // 放下左边的叉子
          put_fork((i+1)%N); // 放下右边的叉子
      }
  ```

  不正确，可能导致死锁。比如每个进程/线程都拿起左边叉子，那就没有人能拿到右边叉子了。

  **因为占用了别人想要的资源，同时又想要别人正占用的资源，死锁**。

+ 方案2

  ```cpp
  #define N 5 // 哲学家个数
  void philosopher(int i) // 哲学家编号 0~4
      
      while(TRUE)	// 去拿两把叉子
      {
          think(); // 哲学家在思考
          take_fork(i); // 去拿左边的叉子
          if(fork(i+1)%N){ // 判断右边叉子是否还在
              take_fork((i+1)%N); // 去拿右边的叉子
              break;
          }else{ // 右边叉子拿不到
              put_fork(i); // 放下左边的叉子
              wait_some_time() // 等待一会
          }
      }
  ```

  虽然对拿叉子的过程过了改进，但是仍不正确。还是每个人都拿起左边叉子，每次都这么干，就无限循环了

+ 方案3

  ```cpp
  #define N 5 // 哲学家个数
  void philosopher(int i) // 哲学家编号 0~4
      
      while(TRUE)
      {
         take_fork(i); // 去拿左边的叉子
         if(fork(i+1)%N){ // 右边叉子还在吗
             take_fork((i+1)%N); // 去拿右边的叉子
             break; // 两个叉子均到手
         }else{ // 右边叉子已不在
             put_fork(i); // 放下左边的叉子
             wait_random_time(); // 等待随机长时间
         }
      }
  ```

  等待时间随机变化。可行，但非完全之策。（有些哲学家运气不好，每次随机等待的时间都很长，那就饿死了。而且随机等待时间不能保证每个哲学家都在一定的时间限制范围内吃到饭）

+ 方案4

  ```cpp
  semaphore mutex	// 互斥信号量，初值1
  void philosopher(int i) // 哲学家编号i:0~4
      
      while(TRUE){
          think(); // 哲学家在思考
          P(mutex); // 进入临界区
            take_fork(i); // 去拿左边的叉子
            take_fork((i+1)%N); // 去拿右边的叉子
            eat(); // 吃面条中...
            put_fork(i); // 放下左边的叉子
            put_fork((i+1)%N); // 放入右边的叉子
          V(mutex); // 退出临界区
      }
  ```

  互斥访问。正确，但每次只允许人进餐。（二进制信号量，可以保证同一个时刻只有一个哲学家吃饭。但是我们知道5支叉子，同一时刻可以有2个人同时进餐）

  缺点：把就餐（而不是叉子）看成是必须互斥访问的临界资源，因此会造成（叉子）资源的浪费。从理论上说，如果有五把叉子，应允许两个不相邻的哲学家同时进餐。

----

#### 思考和代码实现

1. 指导原则：要么不拿，要么就拿两把叉子

   ```none
   S1 思考中...
   S2 进入饥饿状态;
   S3 如果左邻居或右邻居正在进餐,等待;否则转S4
   S4 拿起两把叉子;
   S5 吃面条...
   S6 放下左边的叉子;
   S7 放下右边的叉子;
   S8 新的循环又开始了,转S1
   ```

2. 指导原则：不能浪费CPU时间；进程间相互通信。

   ```none
   S1 思考中...
   S2 进入饥饿状态;
   S3 如果左邻居或右邻居正在进餐,进程进入阻塞态;否则转S4
   S4 拿起两把叉子;
   S5 吃面条...
   S6 放下左边的叉子,看看左邻居现在能否进餐(饥饿状态、两把叉子都在),若能则唤醒之;
   S7 放下右边的叉子,看看右邻居现在能否进餐(饥饿状态、两把叉子都在),若能则唤醒之;
   S8 新的循环又开始了,转S1
   ```

3. 程序如何编写

   1. 必须有数据结构，来描述每个哲学家的当前状态；
   2. 该状态是一个临界资源，各个哲学家对它的访问应该互斥地进行——进程互斥；
   3. 一个哲学家吃饱后，可能要唤醒它地左邻右舍，两者之间存在着同步关系——进程同步；

   ```cpp
   // 1. 必须有数据结构，来描述每个哲学家的当前状态；
   #define N 5           //哲学家个数
   #define LEFT (i)       //第i个哲学家的左邻居
   #define RIGHT (i+1)%N  //第i个哲学家的右邻居
   #define THINKING 0     //思考状态
   #define HUNGRY  1      //饥饿状态
   #define EATTING 2     //进餐状态
   int state[N];         //记录每个人的状态
   
   // 2. 该状态是一个临界资源，对它的访问应该互斥地进行
   semaphore mutex;         //互斥信号量,初值1
   
   // 3. 一个哲学家吃饱后，可能要唤醒邻居，存在同步关系
   semaphore s[N];   //同步信号量 ，初值0
   ```

   ```cpp
   // 函数phlosopher的定义(多线程下,哲学家操作互斥,无死锁,且不会有哲学家一直拿不到叉子而饿死)
   void philosopher(int i) // i的取值: 0~N-1
   {
       while(TRUE) // 封闭式循环
       {
           think(); // S1,思考中...
           take_forks(i); // S2~S4,拿到两把叉子或被阻塞
           eat(); // S5,吃面条中...
           put_forks(i); // S6~S7,把两把叉子放回原处,并且判断左邻右舍是否有机会吃饭,有则唤醒之
       }
   }
   // 函数take_forks的定义
   // 功能:要么拿到两把叉子,要么被阻塞起来。
   void take_forks(int i) // i的取值:0~N-1
   {
       P(mutex); // 进入临界区
       state[i] = HUNGRY; // 我饿了!
       test_take_left_right_forks(i); // 试图拿两把叉子
       V(mutex); // 退出临界区
       P(s[i]); // 没有叉子便阻塞
   }
   // 函数test_take_left_right_forks的定义
   void test_take_left_right_forks(int i) // i:0~N-1
   {
       if(state[i]==HUNGRY && // i:我自己,or其他人
          state[LEFT] != EATING &&
          state[RIGHT] != EATING) // 这里拿叉子判断，是根据哲学家的状态判断和标识的,不是单纯从叉子资源的角度去标记叉子
       {// 因为拿到单个叉子没有意义，我们只需判断是否能同时拿2叉子or干脆不拿叉子了
           state[i] = EATING; // 两把叉子到手
           V(s[i]); // 通知第i人可以吃饭了。初始值0，自己唤醒自己让自己进餐
       }
   }
   // 函数put_forks的定义
   // 功能: 把两把叉子在放回原子，并在需要的时候，去唤醒左邻右舍
   void put_forks(int i) // i的取值:0~N-1
   {
       P(mutex); // 进入临界区
         state[i] = THINKING; // 交出两把叉子
         test_take_left_right_forks(LEFT) // 看左邻局能否进餐
         test_take_left_right_forks(RIGHT) // 看右邻局能否进餐
       V(mutex); // 退出临界区
   }
   // 函数test_take_left_right_forks(int i)的V(s[i]);对应take_forks(int i)的P(s[i]);
   // think()同样需要PV保护临界资源,因为哲学家的状态修改是互斥操作。
   // eat()不需要互斥保护等,因为take_forks已经把需要互斥的"是否拿到叉子"的状态设置成"EATING"了,这里就普通执行操作即可
   void think(i)
   {
      P(mutex);
      state[i] = THINKING;
      V(mutex);
   }
   ```

   ----

   管程方式写法

   ```cpp
   // 1. 必须有数据结构，来描述每个哲学家的当前状态；
   #define N 5           //哲学家个数
   #define LEFT (i)       //第i个哲学家的左邻居
   #define RIGHT (i+1)%N  //第i个哲学家的右邻居
   #define THINKING 0     //思考状态
   #define HUNGRY  1      //饥饿状态
   #define EATTING 2     //进餐状态
   int state[N];         //记录每个人的状态
   
   Lock lock; // 全局锁,保证所有使用lock的管程管理的函数互斥执行
   Condition canEat[N]; // 是否满足能吃饭的条件(换句话说,就是拿到了两个叉子) 
   void philosopher(i)
   {
        while(TRUE) // 封闭式循环
       {
           think(); // S1,思考中...
           take_forks(i); // S2~S4,拿到两把叉子或被阻塞
           eat(); // S5,吃面条中...
           put_forks(i); // S6~S7,把两把叉子放回原处,并且判断左邻右舍是否有机会吃饭,有则唤醒之
       }
   }
   void take_forks(i)
   {
      lock.Acquire();
        state[i] = HUNGRY;    //代表当前哲学家正在等待筷子，处于阻塞状态
        test_take_left_right_forks(i);   //尝试是否能拿到叉子
        while(state[i] != EATTING) // 如果没能拿到叉子,就进入等待队列
            canEat[i].wait(&lock);
      lock.Release();
    
   }
   void test_take_left_right_forks(i)
   {
      if(state[i] == HUNGRY && state[LEFT] != EATTING && state[RIGHT] != EATTING)
      {
          state[i] = EATTING;	//左右都没在进餐,当前哲学家拿到叉子
          canEat[i].signal();	//唤醒指定的i,其有机会抢占CPU并拿到lock
      }
    
   }
   void put_forks(i)
   {
     lock.Acquire();
       state[i] = THINKING;  //表示放下两叉子,重新思考状态
       test_take_left_right_forks(LEFT);
       test_take_left_right_forks(RIGHT); // 判断左和右是否有机会拿叉
     lock.Release();
   }
   void think(i)
   {
      lock.Acquire();
      state[i] = THINKING; // 哲学家的状态是临界资源，需要加锁
      lock.Release();
   }
   ```

> [十、信号量和管程](https://blog.csdn.net/Alatebloomer/article/details/80047067)

# 11. 死锁、进程间通信(IPC)

## 11.1 死锁问题

### 11.1.1 死锁问题-概述

+ 流量只在一个方向
+ 桥的每个部分可以看作一个资源
+ 如果死锁，可能通过一辆车倒退后可以解决（抢占资源和回滚）
+ 如果发生死锁，可能几辆车都必须倒退
+ 可能发生饥饿

![img](https://img-blog.csdn.net/20180424154039650?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0FsYXRlYmxvb21lcg==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)

一组阻塞的进程持有一种资源等待获取另一个进程所占有的一个资源。

例子：系统有2个磁带驱动器，P1和P2各有一个，都需要另一个。

> [十一、死锁与进程间通信](https://blog.csdn.net/Alatebloomer/article/details/80065466)

---

> [死锁--百度百科]([https://baike.baidu.com/item/%E6%AD%BB%E9%94%81/2196938?fr=aladdin](https://baike.baidu.com/item/死锁/2196938?fr=aladdin)) <= 下方内容都来自百度百科，删了部分
>
> 死锁是指两个或两个以上的进程在执行过程中，由于竞争资源或者由于彼此通信而造成的一种阻塞的现象，若无外力作用，它们都将无法推进下去。此时称系统处于死锁状态或系统产生了死锁，这些**永远在互相等待的进程称为死锁进程**。

![img](https://bkimg.cdn.bcebos.com/pic/810a19d8bc3eb1354547b744a51ea8d3fc1f4448?x-bce-process=image/watermark,image_d2F0ZXIvYmFpa2U4MA==,g_7,xp_5,yp_5)

1. 概述-死锁和相关概念

   ​	**死锁的规范定义：集合中的每一个进程都在等待只能由本集合中的其他进程才能引发的事件，那么该组进程是死锁的。**

   ​	*<small>一种情形，此时执行程序中两个或多个进[程](https://baike.baidu.com/item/程)发生永久堵塞（等待），每个进程都在等待被其他进程占用并堵塞了的资源。例如，如果进程A锁住了记录1并等待记录2，而进程B锁住了记录2并等待记录1，这样两个进程就发生了死锁现象。</small>*

   ![img](https://bkimg.cdn.bcebos.com/pic/8644ebf81a4c510fd4ec97696059252dd42aa59c?x-bce-process=image/watermark,image_d2F0ZXIvYmFpa2UxMTY=,g_7,xp_5,yp_5)

   ​	*<small>计算机系统中，如果系统的资源分配策略不当，更常见的可能是程序员写的程序有错误等，则会导致进程因[竞争资源](https://baike.baidu.com/item/竞争资源)不当而产生死锁的现象。</small>*

   ​	在两个或多个任务中，如果每个任务锁定了其他任务试图锁定的资源，此时会造成这些任务永久阻塞，从而出现死锁。例如：[事务](https://baike.baidu.com/item/事务)A 获取了行 1 的[共享锁](https://baike.baidu.com/item/共享锁)。事务 B 获取了行 2 的共享锁。

   ​	[排他锁](https://baike.baidu.com/item/排他锁)，等待事务 B 完成并释放其对行 2 持有的共享锁之前被阻塞。

   ​	排他锁，等待事务 A 完成并释放其对行 1 持有的共享锁之前被阻塞。

   ​	事务 B 完成之后事务 A 才能完成，但是事务 B 由事务 A 阻塞。<u>该条件也称为循环依赖关系：事务 A 依赖于事务 B，事务 B 通过对事务 A 的依赖关系关闭循环</u>。

   ​	**除非某个外部进程断开死锁，否则死锁中的两个事务都将无限期等待下去**。<u>Microsoft SQL Server 数据库引擎死锁监视器定期检查陷入死锁的任务。如果监视器检测到循环依赖关系，将选择其中一个任务作为牺牲品，然后终止其[事务](https://baike.baidu.com/item/事务)并提示错误。这样，其他任务就可以完成其事务</u>。对于事务以错误终止的应用程序，它还可以重试该事务，但通常要等到与它一起陷入死锁的其他事务完成后执行。

   ![img](https://bkimg.cdn.bcebos.com/pic/d4628535e5dde711d9f93685a7efce1b9d16619d?x-bce-process=image/watermark,image_d2F0ZXIvYmFpa2U5Mg==,g_7,xp_5,yp_5)

   ​	*<small><u>在应用程序中使用特定编码约定可以减少应用程序导致死锁的机会</u>。有关详细信息，请参阅将死锁减至最少。</small>*

   ​	**死锁经常与正常阻塞混淆**。[事务](https://baike.baidu.com/item/事务)请求被其他事务锁定的资源的锁时，发出请求的事务一直等到该锁被释放。默认情况下，除非设置了 LOCK_TIMEOUT，否则 SQL Server 事务不会超时。因为发出请求的事务未执行任何操作来阻塞拥有锁的事务，所以该事务是被阻塞，而不是陷入了死锁。最后，拥有锁的事务将完成并释放锁，然后发出请求底事务将获取锁并继续执行。

   ​	<small>不只是[关系数据库管理系统](https://baike.baidu.com/item/关系数据库管理系统)，任何多进[程](https://baike.baidu.com/item/程)系统上都会发生死锁，并且对于[数据库对象](https://baike.baidu.com/item/数据库对象)的锁之外的资源也会发生死锁。例如，多进程操作系统中的一个进程要获取一个或多个资源（例如，内存块）。如果要获取的资源当前为另一进程所拥有，则第一个进程可能必须等待拥有进程释放目标资源。这就是说，对于该特定资源，等待进[程](https://baike.baidu.com/item/程)依赖于拥有进程。在数据库引擎实例中，当获取非数据库资源（例如，内存或进程）时，会话会死锁。</small>

   ![img](https://bkimg.cdn.bcebos.com/pic/3801213fb80e7bec6bf876922f2eb9389b506ba6?x-bce-process=image/watermark,image_d2F0ZXIvYmFpa2U4MA==,g_7,xp_5,yp_5)

2. **产生条件**

   虽然进程在运行过程中，可能发生死锁，但死锁的发生也必须具备一定的条件，死锁的发生必须具备以下四个[必要条件](https://baike.baidu.com/item/必要条件)。

   1. **互斥条件**：指进程对所分配到的资源进行**排它性**使用，即在一段时间内某资源只由一个进程占用。如果此时还有其它进程请求资源，则请求者只能等待，直至占有资源的进程用毕释放。

   2. **请求和保持条件**：指进程已经保持至少一个资源，但又提出了新的资源请求，而该资源已被其它进程占有，此时请求进程阻塞，但又对自己已获得的其它资源保持不放。

   3. **不剥夺条件**：指进程已获得的资源，在未使用完之前，不能被剥夺，只能在使用完时由自己释放。

   4. **环路等待条件**：指在发生死锁时，必然存在一个进程——资源的环形链，即进程集合{P0，P1，P2，···，Pn}中的P0正在等待一个P1占用的资源；P1正在等待P2占用的资源，……，Pn正在等待已被P0占用的资源。

3. 产生原因

   + 竞争资源引起进程死锁

     ​	当系统中供多个进程共享的资源如打印机、公用队列的等，其数目不足以满足诸进程的需要时，会引起诸进程对资源的竞争而产生死锁。

   + **可剥夺资源和不可剥夺资源**

     ​	系统中的资源可以分为两类，一类是可剥夺资源，是指某进程在获得这类资源后，该资源可以再被其他进程或系统剥夺。例如，优先权高的进程可以剥夺优先权低的进程的[处理机](https://baike.baidu.com/item/处理机)。又如，内存区可由[存储器管理](https://baike.baidu.com/item/存储器管理)程序，把一个进程从一个存储区移到另一个存储区，此即剥夺了该进程原来占有的存储区，甚至可将一进程从内存调到外存上，可见，**[CPU](https://baike.baidu.com/item/CPU)和[主存](https://baike.baidu.com/item/主存)均属于可剥夺性资源**。	另一类资源是<u>**不可剥夺资源，当系统把这类资源分配给某进程后，再不能强行收回，只能在进程用完后自行释放**，如[磁带机](https://baike.baidu.com/item/磁带机)、打印机等。</u>

   + **竞争不可剥夺资源**

     ​	**在系统中所配置的不可剥夺资源，由于它们的数量不能满足诸进程运行的需要，会使进程在运行过程中，因争夺这些资源而陷于僵局**。例如，系统中只有一台打印机R1和一台[磁带机](https://baike.baidu.com/item/磁带机)R2，可供进程P1和P2共享。假定PI已占用了打印机R1，P2已占用了磁带机R2，若P2继续要求打印机R1，P2将阻塞；P1若又要求磁带机，P1也将阻塞。于是，在P1和P2之间就形成了僵局，两个进程都在等待对方释放自己所需要的资源，但是它们又都因不能继续获得自己所需要的资源而不能继续推进，从而也不能释放自己所占有的资源，以致进入死锁状态。

   + **竞争临时资源**

     ​	上面所说的打印机资源属于可顺序重复使用型资源，称为**永久资源**。还有一种所谓的<u>**临时资源**，这是指由一个进程产生，被另一个进程使用，短时间后便无用的资源，故也称为消耗性资源</u>，如**[硬件中断](https://baike.baidu.com/item/硬件中断)、信号、消息、[缓冲区](https://baike.baidu.com/item/缓冲区)内的消息等**，它也可能引起死锁。例如，SI，S2，S3是临时性资源，进程P1产生消息S1，又要求从P3接收消息S3；进程P3产生消息S3，又要求从进程P2处接收消息S2；进程P2产生消息S2，又要求从P1处接收产生的消息S1。如果消息通信按如下顺序进行：

     P1: ···Relese（S1）；Request（S3）； ···

     P2: ···Relese（S2）；Request（S1）； ···

     P3: ···Relese（S3）；Request（S2）； ···

     并不可能发生死锁。但若改成下述的运行顺序：

     P1: ···Request（S3）；Relese（S1）；···

     P2: ···Request（S1）；Relese（S2）； ···

     P3: ···Request（S2）；Relese（S3）； ···

     则可能发生死锁。

     2.进程推进顺序不当引起死锁

     由于进程在运行中具有[异步性](https://baike.baidu.com/item/异步性)特征，这可能使P1和P2两个进程按下述两种顺序向前推进。

     **1）** 进程推进顺序合法

     ​	当进程P1和P2并发执行时，如果按照下述顺序推进：P1：Request（R1）； P1：Request（R2）； P1: Relese（R1）；P1: Relese（R2）； P2：Request（R2）； P2：Request（R1）； P2: Relese（R2）；P2: Relese（R1）；这两个进程便可顺利完成，这种不会引起[进程死锁](https://baike.baidu.com/item/进程死锁)的推进顺序是合法的。

     **2）** [进程推进顺序非法](https://baike.baidu.com/item/进程推进顺序非法)

     ​	若P1保持了资源R1,P2保持了资源R2，系统处于不安全状态，因为这两个进程再向前推进，便可能发生死锁。例如，当P1运行到P1：Request（R2）时，将因R2已被P2占用而阻塞；当P2运行到P2：Request（R1）时，也将因R1已被P1占用而阻塞，于是发生进程死锁。

4. 预防

   ​	理解了死锁的原因，尤其是产生死锁的四个[必要条件](https://baike.baidu.com/item/必要条件)，就可以最大可能地避免、预防和解除死锁。只要打破四个必要条件之一就能有效预防死锁的发生：

   + 打破互斥条件：改造独占性资源为虚拟资源，大部分资源已无法改造。
   + 打破不可抢占条件：当一进程占有一独占性资源后又申请一独占性资源而无法满足，则退出原占有的资源。
   + **打破占有且申请条件：采用资源预先分配策略，即进程运行前申请全部资源，满足则运行，不然就等待，这样就不会占有且申请。**
   + 打破循环等待条件：实现资源有序分配策略，对所有设备实现分类编号，所有进程只能采用按序号递增的形式申请资源。

   **有序资源分配法**

   这种算法资源按某种规则系统中的所有资源统一编号（例如打印机为1、[磁带机](https://baike.baidu.com/item/磁带机)为2、磁盘为3、等等），申请时必须以上升的次序。系统要求申请进程：

   1. 对它所必须使用的而且属于同一类的所有资源，必须一次申请完；

   2. 在申请不同类资源时，必须按各类设备的编号依次申请。例如：进程PA，使用资源的顺序是R1，R2； 进程PB，使用资源的顺序是R2，R1；若采用动态分配有可能形成环路条件，造成死锁。

   采用有序资源分配法：R1的编号为1，R2的编号为2；

   PA：申请次序应是：R1，R2

   PB：申请次序应是：R1，R2

   这样就破坏了环路条件，避免了死锁的发生

   **银行家算法**<small>（这个百度感觉说的不是很好，就删了往下的内容了）</small>

5. 解决方法

   ​	在系统中已经出现死锁后，应该及时检测到死锁的发生，并采取适当的措施来解除死锁。

   + 死锁预防

     ​	这是一种较简单和直观的事先预防的方法。方法是通过设置某些限制条件，去破坏产生死锁的四个[必要条件](https://baike.baidu.com/item/必要条件)中的一个或者几个，来预防发生死锁。预防死锁是一种较易实现的方法，已被广泛使用。**但是由于所施加的限制条件往往太严格，可能会导致系统资源利用率和系统吞吐量降低**。

   + 死锁\避免

     ​	系统对进程发出的每一个系统能够满足的资源申请进行动态检查，并根据检查结果决定是否分配资源；**如果分配后系统可能发生死锁，则不予分配，否则予以分配**。这是一种保证系统不进入死锁状态的动态策略。

   + 死锁检测和解除

     ​	先检测：这种方法并不须事先采取任何限制性措施，也不必检查系统是否已经进入不安全区，此方法**允许系统在运行过程中发生死锁**。但可通过系统所设置的检测机构，及时地检测出死锁的发生，并精确地确定与死锁有关的进程和资源。检测方法包括定时检测、效率低时检测、进程等待时检测等。

     ​	然后解除死锁：采取适当措施，**从系统中将已发生的死锁清除掉**<small>（一般就是简单粗暴的kill进程）</small>。

     这是与检测死锁相配套的一种措施。当检测到系统中已发生死锁时，须将进程从死锁状态中解脱出来。常用的实施方法是撤销或挂起一些进程，以便回收一些资源，再将这些资源分配给已处于[阻塞状态](https://baike.baidu.com/item/阻塞状态)的进程，使之转为[就绪状态](https://baike.baidu.com/item/就绪状态)，以继续运行<u>。死锁的检测和解除措施，有可能使系统获得较好的资源利用率和吞吐量，但在实现上**难度最大**</u>。

6. 排除方法*<small>（感觉百度百科里面的这部分内容写得不是很好，可看可不看吧）</small>*

   1. 撤消陷于死锁的全部进程；

   2. 逐个撤消陷于死锁的进程，直到死锁不存在；

   3. 从陷于死锁的进程中逐个强迫放弃所占用的资源，直至死锁消失。

   4. 从另外一些进程那里强行剥夺足够数量的资源分配给死锁进程，以解除死锁状态

   + **计算机网络的死锁**

     ​	死锁是网络中最容易发生的故障之一，即使在网络负荷不很重时也会发生。**死锁发生时，一组[节点](https://baike.baidu.com/item/节点)由于没有空闲[缓冲区](https://baike.baidu.com/item/缓冲区)而无法接收和转发分组，节点之间相互等待，既不能接收分组也不能转发分组，并一直保持这一僵局，严重时甚至导致整个网络的瘫痪。**<small>此时，只能靠人工干预来重新启动网络，解除死锁。但重新启动后并未消除引起死锁的隐患，所以可能再次发生死锁。死锁是由于控制技术方面的某些缺陷所引起的，起因通常难以捉摸、难以发现，即使发现，也常常不能立即修复。因此，在各层协议中都必须考虑如何避免死锁的问题。</small>

   + 存储转发死锁及其防止

     ​	**最常见的死锁是发生在两个[节点](https://baike.baidu.com/item/节点)之间的[直接存储转发死锁](https://baike.baidu.com/item/直接存储转发死锁)**。例如，A节点的所有[缓冲区](https://baike.baidu.com/item/缓冲区)装满了等待输出到B节点的分组，而B节点的所有缓冲区也全部装满了等待输出到A节点的分组；此时，A节点不能从B节点接收分组，B节点也不能从A节点接收分组，从而造成两节点间的死锁。这种情况也可能发生在一组节点之间，例如，A节点企图向B节点发送分组、B节点企图向C节点发送分组、而C节点又企图向A节点发送分组，但此时每个节点都无空闲缓冲区用于接收分组，这种情形称做[间接存储转发死锁](https://baike.baidu.com/item/间接存储转发死锁)。当一个节点处于死锁状态时，所有与之相连的链路将被完全拥塞。

     ​	<small>*(这个预防方法感觉纯文字描述，也不是很直观，就干脆删了。此处原本是"存储转发死锁的防止方法")*</small>

   + 重装死锁及其防止

     ​	死锁中比较严重的情况是重装死锁。假设发给一个端系统的[报文](https://baike.baidu.com/item/报文)很长，被源节点拆成若干个分组发送，目的节点要将所有具有相同编号的分组重新装配成报文递交给目的端系统，若目的节点用于重装报文的[缓冲区](https://baike.baidu.com/item/缓冲区)空间有限，而且它无法知道正在接收的报文究竟被拆成多少个分组，此时，就可能发生严重的问题：为了接收更多的分组，该目的节点用完了它的缓冲空间，但它又不能将尚未拼装完整的报文递送给目的端系统，而邻节点仍在不断地向它传送分组，但它却无法接收。这样，经过多次尝试后，邻节点就会绕道从其它途径再向该目的节点传送分组，但该目的节点已被死锁，其周边区域也由此发生了拥塞。

## 11.2 系统模型

### 11.2.1 资源

#### 概念

- 资源类型R1, R2, . . .,Rm

  ​	CPU cycles（CPU周期、机械周期）、memory space、I/O devices

- 每个资源类型Ri有Wi个实例

+ 每个进程使用资源如下：
  + request/get ← free resource
  + used/hold ← requested/used resource
  + release ← free resource

申请了且能拿到的资源，其原本处于"空闲状态"；

被我们拿到了的资源，处于"占用状态"；

我们用完又释放了的资源，处于"空闲状态"；

*<small>(有些资源具有互斥性，一旦被某进程占用，其他进程就无法占用。如果资源本身不要求互斥访问，你可以用，我也可以用，那么就不会出现死锁。比如只是进行不加锁的读，只读不修改。占用资源时应该设置一定期限，不该让某些资源被某进程永久占用，进程用完共享资源后必须释放资源。)</small>*

---

#### 资源分类

1. 可重复使用的资源
   + 在一个时间只能一个进程使用且不能被删除
   + 进程获得资源，后来释放由其他进程重用
   + **处理器，I/O通道，主和副存储器，设备和数据结构，如文件，数据库和信号量**
   + 如果每个进程拥有一个资源并请求其他资源，死锁可能发生
2. 使用资源
   + 创建和销毁
   + **在I/O缓存区的中断，信号，消息，信息**
   + 如果接收消息阻塞可能会发生死锁
   + 可能少见的组合事件会引起死锁

*（一般设计让获取不到资源的线程进入睡眠/等待状态，等到能够获取资源后在被唤醒。其实唤醒后也不一定就能抢到资源，只不过是有资源能用了就通知一下原本等待的线程而已。）*

---

#### 资源分配图：一组顶点V和边E的集合

+ V有两种类型：
  + P = {P1，P2，...，Pn}，集合包括系统中的所有进程。
  + R = {R1，R2，...，Rm}，集合包括系统中的所有资源类型。
+ requesting/claiming edge - directed edge Pi → Rj
+ assignment/holding edge - directed edge Rj→Pi

![img](https://img-blog.csdn.net/20180424161931450?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0FsYXRlYmxvb21lcg==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)

![img](https://img-blog.csdn.net/20180424162339103?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0FsYXRlYmxvb21lcg==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)![img](https://img-blog.csdn.net/20180424162358672?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0FsYXRlYmxvb21lcg==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)

![img](https://img-blog.csdn.net/20180424162808875?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0FsYXRlYmxvb21lcg==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)

基本情况：

如果图中不包含循环→没有死锁

如果图中包含循环

- 如果每个资源类只有一个实例，那么死锁
- 如果每个资源类有几个实例，可能死锁

> [十一、死锁与进程间通信 ](https://blog.csdn.net/Alatebloomer/article/details/80065466) <= 这部分内容基本都在图上

## 11.3 死锁特征

### 11.3.1 死锁的必要条件

死锁**可能**出现，如果四个条件同时成立

- 互斥：在一个时间只能有一个进程使用资源。
- 持有并等待：进程保持至少一个资源正在等待获取其他进程持有的额外资源。
- 无抢占：一个资源只能被进程自愿释放，进程已经完成了它的任务之后。
- 循环等待：存在等待进程集合{P0，P1，...，PN} ，P0正在等待P1所占用的资源，P1 正在等待P2占用的资源，...，PN-1在等待PN所占用资源，PN正在等待P0所占用的资源

*（必要条件，也就是产生死锁的话，一定满足这4个条件。但是满足这4个条件的情况，不一定就死锁。）*

![img](https://img-blog.csdn.net/20180424163728962?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0FsYXRlYmxvb21lcg==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)

> [十一、死锁与进程间通信](https://blog.csdn.net/Alatebloomer/article/details/80065466) <= 最后那张图挺重要的，没图没灵魂

## 11.4 死锁处理方法

### 11.4.0 常见的四种方案

+ 死锁预防(Deadlock Prevention)：设置严格规则确保系统不会进入死锁状态

+ 死锁避免(Deadlock Avoidance)：拒绝可能导致死锁的资源请求

+ 死锁检测(Deadlock Detection)：通过算法检测进程中是否有死锁的进程
+ 死锁恢复(Recovery from Deadlock)：通常和死锁检测搭配使用，尝试重新执行死锁的进程或将其杀死

*(由于操作系统如果要实现死锁预防、避免、检测的开销都挺大的，现代操作系统一般采用鸵鸟算法，忽略死锁，假装系统中从来没有发生死锁。所以我们开发时需要尽量避免死锁，因为出现了，我们也不能寄希望于操作系统上，往往需要我们自己手动kill死锁的进程，然后从代码中分析可能是哪些地方出问题了。)*

> [鸵鸟算法--百度百科]([https://baike.baidu.com/item/%E9%B8%B5%E9%B8%9F%E7%AE%97%E6%B3%95/4342932?fr=aladdin](https://baike.baidu.com/item/鸵鸟算法/4342932?fr=aladdin))
>
> 鸵鸟算法，一种计算机操作系统算法，用于**当[死锁](https://baike.baidu.com/item/死锁/2196938)真正发生且影响系统正常运行时，手动干预—重新启动**。
>
> 鸵鸟算法的**使用前提是，问题出现的[概率](https://baike.baidu.com/item/概率)很低**。*<small>(市场级的操作系统和固件一般是不会出现死锁的，一般出现死锁都是普通程序员开发的锅）</small>*
>
> **大多数操作系统，包括[UNIX](https://baike.baidu.com/item/UNIX)，[LINUX](https://baike.baidu.com/item/LINUX)和[windows](https://baike.baidu.com/item/windows)，处理[死锁](https://baike.baidu.com/item/死锁)问题的办法仅仅是忽略它。**其假设前提是大多数用户宁可在极偶然的情况下发生[死锁](https://baike.baidu.com/item/死锁)也不愿接受因为死锁解决算法带来的性能上的损失。因为<u>解决[死锁](https://baike.baidu.com/item/死锁)的问题，通常代价很大</u>。鸵鸟算法的实质：出现死锁的概率很小，并且出现之后处理死锁会花费很大的代价，还不如不做处理，[OS](https://baike.baidu.com/item/OS/688)中这种置之不理的策略称之为鸵鸟算法。所以鸵鸟算法，是平衡性能和复杂性而选择的一种方法。

### 11.4.1 死锁预防(Deadlock Prevention)

限制申请方式

+ 互斥 - 共享资源不是必须的，必须占用非共享资源。

+ 占用并等待 - 必须保证当一个进程请求资源时，它不持有任何其他资源。

  + 需要进程请求并分配其所有资源，它开始执行之前或允许进程请求资源仅当进程没有资源。
  + **资源利用率低；可能发生饥饿**。

  *<small>（也就是类似哲学家用餐，要么需要的资源叉子全拿到，要么一个都不占用）</small>*

+ 非抢占*<small>（比较强势，直接kill占用自己需要的资源的进程，自己拿来用）</small>*
  + 如果进程占有某些资源，并请求其他不能被立即分配的资源，则释放当前已占有资源
  + 被抢占资源添加到资源列表中
  + 只有当它能够获得旧的资源以及它请求新的资源，进程可以得到执行

+ 循环等待 - 对资源类型进行排序，要求进程按顺序进行申请

  *<small>（在传统的通用操作系统，循环等待用得不多。在嵌入式系统中用的比较多。因为嵌入式操作系统中，资源得类型有限，可以针对一些特殊的情况来设计，从而保证死锁不会发生。）</small>*

### 11.4.2 死锁避免(Deadlock Avoidance)

#### 死锁避免-概述

**需要系统具有一些额外的先验信息提供**。

- 最简单和最有效的模式是要求每个进程声明它可能需要的每个类型资源的<u>最大数目</u>
- 资源的分配状态是通过限定<u>提供</u>与<u>分配</u>的资源数目，和进程的<u>最大</u>需求
- 死锁避免算法要**<u>动态检查</u>资源的分配状态，以确保永远不会有一个环形等待状态**

----

#### 安全状态和安全序列

+ 当一个进程请求可用资源，系统必须判断立即分配是否能使系统处于安全状态。

+ **系统处于安全状态是指：针对所有进程，存在安全序列**

+ 序列<P1，P2，...，PN>是安全的：针对每个`Pi`，`Pi`要求的资源能够由当前可用资源+所有`Pj `持有资源来满足，其中`j<i`。

  + 如果`Pi`资源的需求不是立即可用，那么`Pi`可以等到所有`Pj`完成。

  + 当`Pi`完成后，P<sub>i+1</sub>可得到所需要的资源，执行，返回所分配的资源，并终止。

  + 用同样的方法，P<sub>i+2</sub>，P<sub>i+3</sub>，...，P<sub>n</sub>都能获得其所需的资源

#### 安全状态与死锁的关系

- 如果系统处于**安全状态→无死锁**
- 如果系统处于**不安全状态→可能死锁**
- **避免死锁：确保系统永远不会进入不安全状态** *<small>(让系统进程调用和资源占用的情况，永远不成环</small>*

![img](https://img-blog.csdn.net/20180424172109180?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0FsYXRlYmxvb21lcg==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)

---

#### 银行家算法（Banker's Algorithm）

​	银行家算法（Banker's Algorithm）是一个避免死锁产生的著名算法，是由Dijkstra在1965年为T.H.E系统设计的一种避免死锁产生的算法。它以银行借贷系统的分配策略为基础，判断并保证系统的安全运行。

背景：

​	在银行系统中，客户完成项目需要申请货款的数量是有限的，每个客户在第一次申请货款时要声明完成该项目所需的最大资金量，在满足所有货款要求并完成项目时，客户应及时归还。

​	银行家在客户申请的货款数量不超过自己拥有的最大值时，都应尽量满足客户的需求。

​	在这种描述中，银行家好比操作系统，资金就是资源，客户就相当于要申请资源的进程。

前提条件

+ 多个实例。
+ 每个进程都必须最大限度地利用资源。
+ 当一个进程请求资源，就不得不等待。
+ 当一个进程获得所有的资源就必须在一段有限的时间释放它们。

​	基于上述前提条件，银行家算法通过尝试寻找允许每个进程获得的最大资源并结束（把资源返还给系统）的进程的请求的一个理想执行时序，来决定一个状态是否是安全的。

​	不存在这满足要求的执行时序的状态都是不安全的。

---

#### 银行家算法数据结构

n = 进程数量, m = 资源类型数量

+ Max（总需求量）：n\*m矩阵。如果`Max[i,j]=k`，表示进程P<sub>i</sub>最多请求资源类型R<sub>j</sub>的k个实例。

+ Available（剩余空闲量）：长度为m的向量。如果`Available[j]=k`，有k个类型R<sub>j</sub>的资源实例可用。

+ Allocation（已分配量）：n\*m矩阵。如果`Allocation[i,j]=k`，则P<sub>i</sub>当前分配了k个R<sub>j</sub>的实例。

+ Need（未来需求量）：n\*m矩阵。如果`Need[i,j]=k`，则P<sub>i</sub>可能需要至少k个R<sub>j</sub>实例完成任务。

  `Need[i,j]=Max[i,j]-Allocation[i,j]`

![img](https://img-blog.csdn.net/20180424173009808?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0FsYXRlYmxvb21lcg==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)

Safety State Estimating Algorithm（安全状态判别算法）

1. Work和Finish分别是长度为m和n的向量。

   初始化：

   ```cpp
   Work=Availvable // 当前资源剩余空闲量
   Finish[i] = false for i - 1,2,...n // 线程i没结束
   ```

2. 找这样的`i`： // 接下来找出Need比Work小的进程`i`

   + `Finish[i] = false`
   + Need<sub>i</sub>≤Work

   没有找到这样的`i`，转4

   *<small>(找到的话，就是资源剩余的够这个进程需求的量，那么这个进程就可以执行了)</small>*

3. Work = Work + Allocation<sub>i</sub>

   `Finish[i] = true` // 进程`i`的资源需求量小于当前剩余空间资源量，所以配置给它再回收

   转2.

   *<small>(有进程执行完了，其就释放自己占用的资源。如果2~3一直循环，然后所有进程都Finish[i]==true了，那么就说明存在安全序列，使得所有进程都能申请到资源并成功执行)</small>*

4. If Finish[`i`] == true for all `i`，then the system is in a safe state。// 所有进程的Finish为True，表明系统处于安全状态

   *<small>（如果执行到4时，并没有所有Finish[i]都==true，那么说明给进程`i`分配资源并运行，系统会出现唤醒等待，进入不安全状态，那么系统就不该给这个进程`i`分配资源）</small>*

![img](https://img-blog.csdn.net/20180424173249717?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0FsYXRlYmxvb21lcg==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)

Banker's Algorithm

Initial：Request = request vector for process P<sub>i</sub>，If Request<sub>i</sub>[j] = k then process P<sub>i</sub> wants k instances of resource type R<sub>j</sub>.

While:

1. 如果Request<sub>i</sub>≤Need<sub>i</sub>转到步骤2。否则，提出错误条件，因为进程已经超过了其最大要求。
2. 如果Request<sub>i</sub>≤Available，转到步骤3。否则P<sub>i</sub>必须等待，因为资源不可用。
3. 通过修改状态来分配请求资源给P<sub>i</sub> // 生成一个需要判断状态是否安全的资源分配环境

Available = Available - Request;

Allocation<sub>i</sub> = Allocation<sub>i</sub> + Request<sub>i</sub>;

Need<sub>i</sub> = Need<sub>i</sub> - Request<sub>i</sub>;

CALL Safety State Estimating Alforithm

+ 如果返回safe，将资源分配给P<sub>i</sub>
+ 如果返回unsafe，P<sub>i</sub>必须等待，旧的资源分配状态被恢复

![img](https://img-blog.csdn.net/20180424173947213?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0FsYXRlYmxvb21lcg==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)



银行家算法的安全状态判断示例组图

![img](https://img-blog.csdn.net/20180424175336590?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0FsYXRlYmxvb21lcg==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)

![img](https://img-blog.csdn.net/201804241902153?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0FsYXRlYmxvb21lcg==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)

![img](https://img-blog.csdn.net/20180424190227866?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0FsYXRlYmxvb21lcg==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)

![img](https://img-blog.csdn.net/20180424190245995?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0FsYXRlYmxvb21lcg==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)

![img](https://img-blog.csdn.net/20180424191124419?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0FsYXRlYmxvb21lcg==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)

> [十一、死锁与进程间通信](https://blog.csdn.net/Alatebloomer/article/details/80065466) <== 这部分图太多了，直接看这个文章就完事了

### 11.4.3 死锁检测(Deadlock Detection)

+ 允许系统进入死锁状态
+ 死锁检测算法检测
+ 恢复机制

*<small>(死锁检测的条件又放宽了一点，就是允许死锁。如果系统检测发现死锁，那就进入恢复机制，没死锁，那就继续运行)</small>*

*<small>(系统每过一段时间检测是否存在死锁进程。而不是每次进程发起请求就检测一次，比前面那些更宽松。)</small>*

*<small>(周期性检查，判断是否存在死锁进程，有则采取恢复措施，无则继续运行。)</small>*

---

如果进程等待图存在环，则认为可能存在死锁。

该算法检测时，无需保留资源结点；仅考虑各个结点之间形成的间接依赖关系。

![img](https://img-blog.csdn.net/20180424193107717?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0FsYXRlYmxvb21lcg==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)

---

使用银行家算法进行死锁检测（找不到安全序列，就认为可能存在死锁）

*<small>（需要大量图，不展开介绍了。建议直接看链接的博文）。其实这些基本都用不上，毕竟这些机制都太占系统资源了。</small>*

![img](https://img-blog.csdn.net/20180424193210569?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0FsYXRlYmxvb21lcg==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)

![img](https://img-blog.csdn.net/20180424193239406?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0FsYXRlYmxvb21lcg==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)

![img](https://img-blog.csdn.net/20180424193401193?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0FsYXRlYmxvb21lcg==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)

![img](https://img-blog.csdn.net/20180424193744928?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0FsYXRlYmxvb21lcg==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)

---

#### 死锁检测算法的使用

何时、使用什么样的频率来检测依赖于：

- 死锁多久可能会发生？
- 多少进程需要被回滚？

如果检测算法多次被调用，有可能是资源图有多个循环，所以我们无法分辨出多个可能死锁的进程中的哪些"造成"死锁。

*<small>(首先，死锁检测算法本身计算开销大，很难把握什么时候要执行检测or多久执行一次；再者，真的检测到死锁了之后，又该如何处理？如果直接kill进程，那要kill哪些？因为进程哪些重要哪些次要，这个操作系统本身难以把握。)</small>*

*<small>（因为检测算法很难把握分寸，所以实际一般顶多只用在开发环境，生产环境、实际操作系统中一般用不上。）</small>*

> [十一、死锁与进程间通信](https://blog.csdn.net/Alatebloomer/article/details/80065466) <= 图文并茂，推荐。<small>(虽然这次这个其实好像也不是很全，感觉有稍微漏了一点内容，但是无伤大雅。这位博主学的课的内容和我学的应该没什么太大区别，就是PPT样式不一样罢了。)</small>

### 11.4.4 死锁恢复(Recovery from Deadlock)

*<small>(死锁恢复，其实就是通过kill进程来实现的)</small>*

+ 终止所有的死锁进程

+ 在一个时间内终止一个进程直到死锁消除

+ 终止进程的顺序应该是
  + 进程的优先级
  + 进程运行了多久以及需要多少时间才能完成
  + 进程占用的资源
  + 进程完成需要的资源
  + 多少进程需要被终止
  + 进程是交互还是批处理

*<small>(不管怎么样，kill进程大多存在一定不合理性。不过死锁本身就是很少出现的现象，死锁恢复根式很少会启用的机制了。)</small>*

*（要是电脑内存爆满，就能体验到了。各种浏览器程序奔溃什么的...特别是java当时初学分布式，就会发现8G内存根本不够用呀。于是就多加了8G。）*

*<small>(抛开硬件不说，曾再网上别人博客看到说工作五六年了，就只真碰上一次死锁的情况，不然都没碰到过。)</small>*

---

死锁恢复，抢占别的进程当时所占用的资源，强制kill某些进程。

+ 选择一个受害者 - 最小成本。

+ 回滚 - 返回到一些安全状态，重启进程到安全状态

+ 饥饿 - 同一进程可能一直被选作受害者，包括回滚的数量

*（前面提到的四种处理死锁的机制：死锁预防、死锁避免、死锁检测、死锁恢复，这些其实在现代操作系统很少用。现代操作系统大多采用"鸵鸟算法"，就是当作死锁压根不存在<=即平时不去检测，等到真出现了，再执行死锁恢复机制<=基本就是强行kill进程。）*

## 11.5 进程间通信(IPC)

### 11.5.0 概述

> [ipc （进程间通信）--百度百科](https://baike.baidu.com/item/ipc/19486140#viewPageContent) <= 下方内容来自百度百科。
>
> 进程间通信(IPC*,*Inter-Process Communication*)*指至少两个进程或线程间传送数据或信号的一些技术或方法。

​	**进程间通信**（**IPC**，*Inter-Process Communication*），指至少两个进程或线程间传送数据或信号的一些技术或方法。进程是计算机系统分配资源的最小单位(进程是分配资源最小的单位，而线程是调度的最小单位，线程共用进程资源)。每个进程都有自己的一部分独立的系统资源，彼此是隔离的。为了能使不同的进程互相访问资源并进行协调工作，才有了进程间通信。<small>举一个典型的例子，使用进程间通信的两个应用可以被分类为客户端和服务器，客户端进程请求数据，服务端回复客户端的数据请求。有一些应用本身既是服务器又是客户端，这在分布式计算中，时常可以见到。这些进程可以运行在同一计算机上或网络连接的不同计算机上。</small>

​	进程间通信技术包括**消息传递**、**同步**、**共享内存**和**远程过程调用**<small>（RPC，微服务、分布式中的常见名词）</small>。<u>IPC是一种标准的Unix通信机制</u>。

使用IPC 的理由：

- 信息共享：Web服务器，通过网页浏览器使用进程间通信来共享web文件（网页等）和多媒体；
- 加速：维基百科使用通过进程间通信进行交流的多服务器来满足用户的请求；
- 模块化；
- 私有权分离。

与直接共享内存地址空间的多线程编程相比，IPC的缺点：

- 采用了某种形式的内核开销，降低了性能;
- **几乎大部分IPC都不是程序设计的自然扩展，往往会大大地增加程序的复杂度**。

| 方法               | 提供方（操作系统或其他环境）                         |
| ------------------ | ---------------------------------------------------- |
| 文件               | 多数操作系统                                         |
| 信号               | 多数操作系统                                         |
| Berkeley套接字     | 多数操作系统                                         |
| 消息队列           | 多数操作系统                                         |
| 管道               | 所有的POSIX系统，Windows                             |
| 命名管道           | 所有的POSIX系统，Windows                             |
| 信号量             | 所有的POSIX系统，Windows                             |
| 共享内存           | 所有的POSIX系统，Windows                             |
| Message Passing    | 用于MPI规范，Java RMI，CORBA，MSMQ，MailSlot以及其他 |
| Memory-Mapped File | 所有的POSIX系统，Windows                             |

---

#### 概述

+ 进程通信的机制及同步
+ 不使用共享变量的进程通信

*<small>(回顾前面，操作系统保证进程间的地址空间独立，每个进程PCB，即`task_struct`拥有自己的`mm_struct`。进程之间地址空间独立，互不相干。)</small>*

IPC facility提供2个基本操作

- send(message) - 消息大小固定或者可变
- receive(message)

如果P和Q想通信，需要：

- 在它们之间建立通信链路
- 通过send/receive交换消息

*<small>(进程间的通信，可以通过物理资源（内存等)or逻辑链路的管道等来实现）</small>*

通信链路的实现

- 物理 (如，共享内存，硬件总线）
- 逻辑 (如，逻辑属性）

----

#### 直接通信和间接通信

![img](https://img-blog.csdn.net/20180424200552899?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0FsYXRlYmxvb21lcg==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)

*（间接通信，进程A把消息发给内核Kernel，Kernel再转发消息给进程B；直接通信，进程A放置消息到共享内存区域，而进程B从监听的共享内存区域直接获取消息。）*

1. 直接通信

   + 进程必须正确的命名对方：*<small>(比如PID进程号)</small>*

     + send (P, message) – 发送信息到进程P

     + receive(Q,message) – 从进程Q接受消息

   + 通信链路的属性

     + 自动建立链路 *<small>(一般需要操作系统支持，由操作系统帮我们完成进程之间的链路建立，因为操作系统可以i访问所有的内存。不靠操作系统，那基本做不到链路的建立，毕竟用户只能申请用户态内存，而操作系统分配的用户态空间又是彼此独立，最后对应的物理地址不会重叠。)</small>*

     + 一条链路恰好对应一对通信进程

     + 每对进程之间只有一个链接存在

     + 链接可以是单向的，但通常为双向的

2. 间接通信 *<small>(相对简单，两者协商一个把数据放到X中，另一个从X中读取即可)</small>*

   + 定向从消息队列接收消息

     + 每个消息队列都有一个唯一的ID

     + 只有它们**共享了一个消息队列，进程才能够通信**

   + 通信链路的属性 *<small>(这个通信用的中间结点，一般是内核中共享的一个资源)</small>*

     + 只有共享了相同消息队列的进程，才建立链路

     + 链接可以与许多进程相关联

     + 每对进程可以共享多个通信链路
     + 连接可以是单向或双向

   + 操作

     + 创建一个新的消息队列
     + 通过消息队列发送和接收消息
     + 销毁消息队列

   + 原语的定义如下

     + send(A,message) – 发送消息到队列A
     + receive(A,message) – 从队列A接受消息

*（进程间通信采用"直接通信"or"间接通信"，需要根据实际应用场景来选择。比如TCP网络通信<=内核包装了TCP/IP协议栈实现，直接通信；而分布式项目常用消息队列来异步处理数据<=间接通信）*

*(通过发送/接收的途径/路径，判断直接通信和间接通信。)*

---

#### 阻塞与非阻塞

+ 消息传递可以是阻塞或非阻塞

+ **阻塞被认为是同步的**

  + **Blocking send** has the sender block until the message is received

  + **Blocking receive** has the receiver block until a message is available

+ **非阻塞被认为是异步的**
  + **Non-blocking send** has the sender send the message and continue
  + **Non-blocking receive** has the receiver receive a valid message or null

*（阻塞和非阻塞，阻塞可以理解为一系列操作必须完整执行完or抛出异常才能结束；而非阻塞通常可能执行完一系列操作也可能不执行完，取决于中间过程是否需要等待，如果需要等待，那么直接返回结果or异常，反之则持续执行直到结束并获得结果。比如"请求"-"执行"-"等待成功信号or异常信号"，阻塞必须完成整个流程，如果当前"执行"的条件不满足，那么阻塞等待直到条件满足并获取最后的"成功信号or异常信号"；而"非阻塞"，在中间操作有条件无法满足需要阻塞等待时，直接返回结果or异常，取消等待过程，比如"执行"时条件不足，这时候不会为了之后能继续执行而阻塞等待条件被满足，而是直接返回给应用结果"条件不足，无法继续"之类的结果。）*

*(阻塞被认为是同步的，很好理解。因为同步表示某些操作/状态必须按照一定顺序执行/变化。阻塞发送，需要等待消息确实被目标接收到后，才能取消阻塞；阻塞接收，需要等待确实收到可用信息后，才能取消阻塞。可以把发送看成"请求发送"-"发送"-"发送成功"的同步操作拆解，而接收可以看成"请求接收"-"接收"-"接收结束"的同步操作拆解)*

*(非阻塞被认为是异步的，这也很好理解。因为异步可以表示某些操作/状态具体执行顺序我们不关心，只需要得到最后的结果即可。非阻塞发送，提出发送的请求并完成发送动作后直接获取结果或异常，不去阻塞等待目标是否接收成功；非阻塞接收，提出接收的请求后，如果正好有数据可以接受就接受，否则直接返回空信息，不会傻等待直到有数据。这里可以如果拆解发送为"请求发送"-"发送"-"发送成功"，那么只要中间任何一个动作发生阻塞，直接返回结果or异常给进程继续往下执行而不等待，这点和异步类似。异步不关心中间过程具体什么时候被执行和被完成，只关心最后的结果（返回完整期望得到的信息or不希望得到的异常），而阻塞则是只要执行过程需要被阻塞就直接返回结果。）*

---

#### 通信链路缓冲

*（缓存用来提高效率；一般用在发送方法发送和接受方接受速度不匹配的情况下。使用缓存，必须考虑如何选择合适的大小。）*

队列的消息被附加到链路；可以是以下3种缓冲方式之一：

1. 0 容量 - 0 messages

   发送方必须等待接受方（rendezvous）

2. 有限容量 - n messages的有限长度

   发送方必须等待，如果队列满

3. 无限容量 - 无限长度

   发送方不需要等待

*（0容量可以类比同步（阻塞）发送方式，就是发送方确定接受方已经收到数据后，才能继续往下执行。因为0缓存，提前停止阻塞可能导致接受方数据丢失。）*

*（一般用的缓存，不管是操作系统内部的缓存还是应用层开发的缓存，都是有限容量的。只要缓存没填满，发送方就可以一直往缓存发送数据；而接受方只要缓存不为空，就可以一直从缓存取数据。）*

*（无限容量的缓存，没想到有什么使用场景，或者说根本办不到。一般用在场景设想中，实际不可能这么做。或者说就算实现上欺骗应用层让其以为有无限缓存，一直往缓存写不停，那底层硬件也不一定跟得上处理，很难找到平衡点。）*

> [十一、死锁与进程间通信](https://blog.csdn.net/Alatebloomer/article/details/80065466) <= 这次就上面那张图

### 11.5.1 信号

#### 概述

+ Signal（信号）

  + **软件中断通知事件处理**

  + Examples：`SIGFPE`，`SIGKILL`，`SIGUSR1`，`SIGSTOP`,`SIGCONT`等

+ 接收到信号时会发生什么

  + Catch：指定信号处理函数被调用

  + Ignore：依靠操作系统的默认操作
    + Excample：`Abort`，`memory dump`，`suspend` or `resume process`
  + Mask：闭塞信号因此不会传送
    + 可能是暂时的（当处理同样类型的信号）

+ 不足
  + **不能传输要交换的任何数据**

*（Signal信号，就是1bit或者少量信息，一般用来提示应用程序一些信息，而不是用来进程间通信的。因为数据量太少，且这些信号一般都是只能用操作系统定死的信号，不灵活。）*

*（**应用程序接收到来自操作系统的信号，一般要么直接停止运行，要么执行预先设置好的中断处理函数。也可能直接忽视信号，不做处理。**）*

*（**一般信号被应用程序的中断处理函数执行结束后，把CPU使用权返回给之前执行中断操作的进程。**也就是给操作系统信号，然后操作系统又转告这个进程去处理，处理完又把CPU返回给之前发出信号的那个进程。）*

---

#### 信号机制的实现

![img](https://img-blog.csdn.net/20180424204414915?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0FsYXRlYmxvb21lcg==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)

1. Proccess X with Signal handles，register handles.

   ```none
   //instruction set
   System call interface
   {read(),write(),sigaltstack()...}
   (restartable system calls)
   ```

2. kernel deliver signal, dispatch to handler

3. Process X -> signal handler stack

​	应用程序需要事先事先对应的系统调用接口的处理函数，注册到操作系统中。之后操作系统接收到信号时，就从注册列表查找这个进程，内核态下修改用户态进程的执行堆栈，把程序事先注册的中断处理函数入口写到函数调用栈顶部，之后再把CPU运行权交给用户态程序。那么程序就无感知地自动调用完之前注册的中断处理函数，然后继续上次PC程序计数器指向的位置往下运行。

​	这种方式看着挺奇怪的，我们一般也不这么用，因为和黑客的木马运作方式还挺像的，只不过我们是用在好的方面上。

示例：

```c
#include <stdio.h>
#include <signal.h>
void sigproc()
{
    signal(SiGINT, sigproc);
    /* NOTE some versions of UNIX will reset
     * signal to default after each call. So for
     * portability reset signal each time */
    prinft("you have pressed ctrl-c disabled \n");
}
void quitproc()
{
    printf("ctrl-\\ presed to quit \n"); /* this is "ctrl" & "\" */
    exit(0); /* normal exit status */
}
main()
{
    signal(SIGINT,sigproc); 	/* DEFAULT ACTION: term */
    signal(SIGQUIT,quitproc);   /* DEFAULT ACTION: term */
    printf("ctrl-c disabled use ctrl-\\ to quit\n");
    
    for(;;);
}
```

![img](https://img-blog.csdn.net/20180424210824414?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0FsYXRlYmxvb21lcg==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)

> [十一、死锁与进程间通信](https://blog.csdn.net/Alatebloomer/article/details/80065466)
>
> [信号SIGINT](https://blog.csdn.net/weixin_42377147/article/details/90475758)
>
> [C语言库signal.h操作](https://blog.csdn.net/u010842019/article/details/53115232)
>
> [C 标准库 - <signal.h>](https://www.runoob.com/cprogramming/c-standard-library-signal-h.html)

### 11.5.2 管道

*<small>(管道，用于数据交换)</small>*

*（管道机制，早期计算机科学家们想，能不能多个进程互相协作完成更复杂的功能。就想说能不能把某个进程的输出当作另一个进程的输入，中间就是A输出到文件，B又从文件中读取数据之类的。）*

​	**管道需要父进程给子进程之间建立好管道关系**。如果进程之间不具备父子关系，管道就没法建立。

​	**管道里传输信息的形式是字节流**，没有直观的结构化表现形式。*<small>（当然如果自己对字节流有采取特殊编码、解码方式，也不是不行，只不过会更加复杂。比如双方都协商用protobuf二进制数据传输协议=>Netty中TCP开发的可选方案）</small>*

- 子进程从父进程继承**文件描述符**
  + file descriptor 0 stdin, 1 stdout, 2 stderr

+ 进程不知道（或不关心！）从键盘，文件，程序读取或写入到终端，文件，程序

  `% ls | more`

![img](https://img-blog.csdn.net/20180424211522239?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0FsYXRlYmxvb21lcg==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)

​	*（`shell`进程，新建`ls`进程和`more`进程，并且申请让操作系统把`ls`的输出`stdout`变成`more`的输入。这里键盘输入到`ls`，`ls`输出内容到`buffer`(操作系统建立的管道），`more`又从`buffer`管道中获取输入，最后`more`把输出打印到屏幕上。）*

​	*（对于`ls`和`more`来说，`ls`不知道自己输出`stdout`内容输入到了管道`buffer`，`more`也无感知自己的输入`stdin`变成了管道`buffer`。<u>这建立管道的操作，由它们的**父进程**协调操作</u>。）*

shell：

+ 创建管道
+ 为`ls`创建一个进程，设置`stdout`为管道写端
+ 为`more`创建一个进程，设置`stdin`为管道读端

> [十一、死锁与进程间通信](https://blog.csdn.net/Alatebloomer/article/details/80065466)
>
> **与管道相关的系统调用**
>
> - 读管道：`read(fd,buffer, nbytes)`，`scanf()`是基于它实现的
>
> - 写管道：`write(fd,buffer, nbytes)`，`printf()`是基于它实现的
>
> - 创建管道：`pipe(rgfd)`
>
>   `rgfd`是2个文件描述符组成的数组
>
>   `rgfd[0]`是读文件描述符，`rgfd[1]`是写文件描述符
>
> 下面3篇基本就是具体的代码+介绍了。
>
> [linux c语言之pipe（）函数](https://blog.csdn.net/u010709783/article/details/78064747/)
>
> [linux——管道详解](https://blog.csdn.net/qq_38646470/article/details/79564392)
>
> [linux管道pipe详解](https://blog.csdn.net/qq_42914528/article/details/82023408)

### 11.5.3 消息队列

​	消息队列可以让不具备父子关系的进程相互交换数据。并且消息队列里传输的数据可以不是字节流，而是结构化的消息数据。(数据的可读性还是挺重要的。要是开发时性能要求不严格，那应该保证数据具有可读性，采用消息队列更直观。)

+ 消息队列按FIFO来管理消息
  + Message：作为一个字节序列存储
  + Message Queues：消息数组
  + FIFO & FILO configuration

![img](https://img-blog.csdn.net/2018042421264765?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0FsYXRlYmxvb21lcg==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)

![img](https://img-blog.csdn.net/20180424213110481?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0FsYXRlYmxvb21lcg==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)

> [十一、死锁与进程间通信](https://blog.csdn.net/Alatebloomer/article/details/80065466)
>
> 消息队列对应的系统调用
>
> + `msgget(key,flags)` 	获取消息队列标识
> + `msgsnd(QID,buf,size,flags)`	发送消息
> + `msgrcv(QID,buf,size,type,flags)`	接收消息
> + `msgctl(...)`	消息队列控制
>
> [Linux消息队列编程（简单应用）](https://blog.csdn.net/qq_27664167/article/details/81712887)
>
> [Linux系统编程—消息队列](https://www.jianshu.com/p/7e3045cf1ab8) <= 推荐，代码+图。

### 11.5.4 共享内存

前面的管道、消息队列，都可以理解为进程之间的"间接通信"方式。

共享内存属于"直接通信"方式。

*（共享内存，即两个进程的页表中，有部分页表映射到相同的物理空间上。这样两个进程共享同一块内存空间，一个往里写数据，另一个直接从里读数据，**不需要经过内核态转换，因为共享的是用户内存空间**。）*

+ 进程
  + 每个进程都有私有内存地址空间

  + **在每个地址空间内，明确地设置了共享内存段**

+ 优点
  + 快速、方便地共享数据

+ 不足
  + 必须同步数据访问

*（相对前面的"间接通信"，共享内存是进程间传递数据最快的方式，因为各自只要面对共享内存操作就好了。间接通信，经常需要经过内核态、用户态切换，那么数据就需要用户态拷贝到内核态，内核态再拷贝到用户态内存区域。<=如果使用的是操作系统提供的接口的话。）*

*（需要进程对临界资源（共享内存）的读写是互斥的，避免数据出现"异常"。）*

---

+ 最快的方法
+ 一个进程写另一个进程立即可见
+ 没有系统调用干预
+ 没有数据复制
+ **不提供同步**
  + 由程序员提供同步

![img](https://img-blog.csdn.net/2018042421384078?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0FsYXRlYmxvb21lcg==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)

*（共享内存实现，即操作系统内部的页表把两个进程各自用到的某一块虚拟页号映射到相同的物理页帧，那么看上去不相干的虚拟地址实际就映射到相同的物理地址上了。需要注意的是，编写程序需要实现同步、互斥操作，避免共享内存区域数据出错。）*

![img](https://img-blog.csdn.net/20180424214013418?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0FsYXRlYmxvb21lcg==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)

> [十一、死锁与进程间通信](https://blog.csdn.net/Alatebloomer/article/details/80065466)
>
> 共享内存相关的系统调用：
>
> + `shmget(ket,size,flags)`
>
>   创建共享段
>
> + `shmat(shmid,*shmaddr,flags)`
>
>   把共享段映射到进程地址空间
>
> + `shmdt(*shmaddr)`
>
>   取消共享段到进程地址空间的映射
>
> + `shmctl(...)`
>
>   共享段控制
>
> + 需要信号量等机制协调共享内存的访问冲突
>
> [linux中shmget函数](https://blog.csdn.net/qq_33573235/article/details/79169624)
>
> [Linux下进程间通信方式——共享内存](https://www.cnblogs.com/wuyepeng/p/9748889.html)
>
> [进程间通信之共享内存](https://blog.csdn.net/jiaomubai/article/details/99683686)
>
> [Linux进程间通信——使用共享内存](https://blog.csdn.net/ljianhui/article/details/10253345)
>
> 1. 优点：我们可以看到使用共享内存进行进程间的通信真的是非常方便，而且函数的接口也简单，数据的共享还使进程间的数据不用传送，而是直接访问内存，也加快了程序的效率。同时，它也不像匿名管道那样要求通信的进程有一定的父子关系。
>2. 缺点：**共享内存没有提供同步的机制**，这使得我们在使用共享内存进行进程间通信时，往往要借助其他的手段来进行进程间的同步工作。

### 11.5.5 Linux信号量机制(优质文章转载)

> [Linux 信号列表](https://blog.csdn.net/tennysonsky/article/details/46010505)
>
> [信号 （LINUX信号机制）--百度百科](https://baike.baidu.com/item/%E4%BF%A1%E5%8F%B7/7927794?fr=aladdin](https://baike.baidu.com/item/信号/7927794?fr=aladdin))
>
> 以下内容整合，百度百科、CSDN文章。

​	在计算机科学中，信号是Unix、类Unix以及其他POSIX兼容的操作系统中进程间通讯的一种有限制的方式。它是一种**异步**的通知机制，用来提醒进程一个事件已经发生。当一个信号发送给一个进程，操作系统中断了进程正常的控制流程，此时，任何非[原子操作](https://baike.baidu.com/item/原子操作/1880992)都将被中断。<u>如果进程定义了信号的处理函数，那么它将被执行，否则就执行默认的处理函数</u>。

#### 基本概念

​	**软中断信号**（signal，简称为信号）用来通知进程发生了异步事件。进程之间可以互相通过**系统调用[kill](https://baike.baidu.com/item/kill/15893360)**发送软中断信号。内核也可以因为内部事件而给进程发送信号，通知进程发生了某个事件。注意，**信号只是用来通知某进程发生了什么事件，并不给该进程传递任何数据。**
　收到信号的进程对各种信号有不同的处理方法。处理方法可以分为三类：

+ 第一种是类似中断的处理程序，对于需要处理的信号，进程可以指定处理函数，由该函数来处理。

+ 第二种方法是，忽略某个信号，对该信号不做任何处理，就象未发生过一样。

+ 第三种方法是，对该信号的处理保留系统的[默认值](https://baike.baidu.com/item/默认值)，**对大部分的信号的缺省操作是使得进程终止**。此方法为缺省操作。**进程通过系统调用signal来指定进程对某个信号的处理行为**。

在进程表的表项中有一个**软中断信号域**，该域中每一位对应一个信号，当有信号发送给进程时，对应其位置位。由此可以看出，进程对不同的信号可以同时保留，但**对于同一个信号，进程并不知道在处理之前来过多少个**。

#### 信号类型

发出信号的原因很多，这里按发出信号的原因简单分类，以了解各种信号：

1. 与**进程终止**相关的信号。当进程退出，或者子进程终止时，发出这类信号。
2. 与**进程例外事件**相关的信号。如进程越界，或企图写一个只读的内存区域（如程序正文区），或执行一个特权指令及其他各种硬件错误。
3. 与在**系统调用期间遇到不可恢复条件**相关的信号。如执行系统调用[exec](https://baike.baidu.com/item/exec/9077756)时，原有资源已经释放，而系统资源又已经耗尽。
4. 与执行**系统调用时遇到非预测错误条件相关**的信号。如执行一个并不存在的系统调用。
5. 在**用户态下的进程**发出的信号。如进程调用系统调用kill向其他进程发送信号。
6. **与终端交互**相关的信号。如用户关闭一个终端，或按下break键等情况。
7. **跟踪进程执行**的信号。

在 Linux 下，每个信号的名字都以字符`SIG`开头，每个信号和一个数字编码相对应。

要想查看这些信号和编码的对应关系，可使用命令：`kill -l`

```shell
 1) SIGHUP	 2) SIGINT	 3) SIGQUIT	 4) SIGILL	 5) SIGTRAP
 6) SIGABRT	 7) SIGBUS	 8) SIGFPE	 9) SIGKILL	10) SIGUSR1
11) SIGSEGV	12) SIGUSR2	13) SIGPIPE	14) SIGALRM	15) SIGTERM
16) SIGSTKFLT	17) SIGCHLD	18) SIGCONT	19) SIGSTOP	20) SIGTSTP
21) SIGTTIN	22) SIGTTOU	23) SIGURG	24) SIGXCPU	25) SIGXFSZ
26) SIGVTALRM	27) SIGPROF	28) SIGWINCH	29) SIGIO	30) SIGPWR
31) SIGSYS	34) SIGRTMIN	35) SIGRTMIN+1	36) SIGRTMIN+2	37) SIGRTMIN+3
38) SIGRTMIN+4	39) SIGRTMIN+5	40) SIGRTMIN+6	41) SIGRTMIN+7	42) SIGRTMIN+8
43) SIGRTMIN+9	44) SIGRTMIN+10	45) SIGRTMIN+11	46) SIGRTMIN+12	47) SIGRTMIN+13
48) SIGRTMIN+14	49) SIGRTMIN+15	50) SIGRTMAX-14	51) SIGRTMAX-13	52) SIGRTMAX-12
53) SIGRTMAX-11	54) SIGRTMAX-10	55) SIGRTMAX-9	56) SIGRTMAX-8	57) SIGRTMAX-7
58) SIGRTMAX-6	59) SIGRTMAX-5	60) SIGRTMAX-4	61) SIGRTMAX-3	62) SIGRTMAX-2
63) SIGRTMAX-1	64) SIGRTMAX
```

这里不对各种信号夸夸其谈，需要查字典的，建议直接[信号(Linux信号机制)--百度百科]([https://baike.baidu.com/item/%E4%BF%A1%E5%8F%B7/7927794?fr=aladdin](https://baike.baidu.com/item/%E4%BF%A1%E5%8F%B7/7927794?fr=aladdin)](https://baike.baidu.com/item/信号/7927794?fr=aladdin](https://baike.baidu.com/item/信号/7927794?fr=aladdin)))

#### 内核对信号的基本处理方法

​	<small>内核给一个进程发送软中断信号的方法，是在进程所在的进程表项的信号域设置对应于该信号的位。这里要补充的是，如果信号发送给一个正在睡眠的进程，那么要看 该进程进入睡眠的优先级，如果进程睡眠在可被中断的优先级上，则唤醒进程；否则仅设置进程表中信号域相应的位，而不唤醒进程。这一点比较重要，因为**进程检查是否收到信号的时机是：一个进程在即将从内核态返回到用户态时；或者，在一个进程要进入或离开一个适当的低调度优先级睡眠状态时。**</small>

​	**内核处理一个进程收到的信号**的时机是在一个**进程从内核态返回用户态时**。所以，当一个进程在内核态下运行时，**软中断信号**并不立即起作用，要等到将返回用户态时才处理。**<u>进程只有处理完信号才会返回用户态，进程在用户态下不会有未处理完的信号</u>**。

​	内核处理一个进程收到的**软中断信号是在该进程的上下文中**，因此，<u>进程必须处于运行状态</u>。

​	前面介绍概念的时候讲过，处理信号有三种类型：

+ 进程接收到信号后**退出**；
+ 进程**忽略**该信号；
+ 进程收到信号后**执行<big>用户</big>设定用系统调用signal的函数**。

​	当进程接收到一个它**忽略的信号**时，进程丢弃该信号，就象没有收到该信号似 的继续运行。*<small>（忽略`task_struct`上对应被标志为"忽略"的信号，不需要接着另外调用什么函数）</small>*

​	如果进程收到一个**要捕捉的信号**，那么进程**从内核态返回用户态时**<u>执行<big>用户定义</big>的函数</u>。而且执行用户定义的函数的方法很巧妙，**内核**是在**用户栈**上创建一个新的层，该层中将返回地址的值设置成用户定义的处理函数的地址，这样进程从内核返回弹出栈顶时就返回到用户定义的函数处，从函数返回再弹出栈顶时， 才返回原先进入内核的地方。这样做的原因是**用户定义的处理函数不能且不允许在内核态下执行（如果用户定义的函数在内核态下运行的话，用户就可以获得任何权限）**。

#### 内核对信号处理的注意点

1.  <u>在**一些系统中**，当一个进程处理完中断信号返回用户态之前，内核**清除**用户区中设定的对该信号的处理例程的地址，即**下一次**进程对该信号的处理方法又改为**默认值**</u>，除非在下一次信号到来之前再次使用signal系统调用。这可能会使得进程在调用signal之前又得到该信号而导致退出。在BSD中，内核不再清除该地址。但**不清除该地址可能使得进程因为过多过快的得到某个信号而导致<big>堆栈溢出</big>**。为了避免出现上述情况。<u>在BSD系统中，内核模拟了对硬件中断的处理方法，即**在处理某个中断时，阻止接收新的该类中断**</u>。

2. 如果**要捕捉的信号**发生于<u>进程正在一个系统调用中</u>时，并且该**进程睡眠在可中断的优先级上**，这时该信号引起进程作一次`longjmp`，**跳出睡眠状态**，**返回用户态并执行信号处理例程**。当从信号处理例程返回时，<u>进程就象从系统调用返回一样，但返回了一个错误代码，指出该次系统调用曾经被中断</u>。这要注意的是，**BSD系统中内核可以自动地重新开始系统调用**。

3. **若进程睡眠在可中断的优先级上**，则当它收到一个**要忽略的信号**时，该进程**被唤醒**，但不做`longjmp`，一般是**继续睡眠**。但<u>用户感觉不到进程曾经被唤醒，而是象没有发生过该信号一样</u>。

4. 内核对**子进程终止**（**SIGCLD**）信号的处理方法与其他信号有所区别。

   当进程检查出收到了一个子进程终止的信号时*<small>(PCB中对应标志位变1之类的)</small>*:

   + **缺省情况**下，该进程就象没有收到该信号似的*<small>（即当作"要忽略的信号"）</small>*

   + **如果父进程执行了系统调用wait**，进程将从系统调用wait中醒来并返回wait调用，执行一系列wait调用的后续操作（**找出僵死的子进程，释放子进程的PCB进程表项**），然后从wait中返回。**SIGCLD信号的作用是唤醒一个睡眠在<big>可被中断优先级上</big>的进程**。

     + 如果该进程**捕捉**了这个信号，就象普通信号处理一样转到处理例程。

     + 如果进程**忽略**该信号*(默认就是忽略，or父进程中执行了`signal(SIGCHLD,SIG_IGN);`)*，那么系统调用wait的动作就有所不同。因为SIGCLD的作用仅仅是**唤醒一个**睡眠在可被中断优先级上的进程，那么执行wait调用的父进程被唤醒继续执行wait调用的后续操作，然后**等待其他的子进程**。

     + 如果一个进程**调用signal系统调用，并设置了SIGCLD的处理方法**，并且该进程有子进程处于僵死状态，则内核将向该进程发一个`SIGCLD`信号。

       (比如父进程执行`signal(SIGCHLD,一个自定义handler处理函数);`，那么根据前面2.提到的遇到要捕捉的信号时，这个`signal()`可能只对某一次收到的SIGCLD处理，下一次不再生效；也可能每次收到信号，都触发注册的"自定义handler处理函数")

#### setjmp和longjmp的作用

​	<small>前面在介绍信号处理机制时，多次提到了`setjmp`和`longjmp`，但没有仔细说明它们的作用和实现方法。这里就此作一个简单的介绍。</small>
　在介绍信号的时候，我们看到多个地方**要求进程在检查收到信号后，从原来的系统调用中直接返回，而不是等到该调用完成**。这种进程突然改变其上下文的情况，就是使用`setjmp`和`longjmp`的结果。

+ `setjmp`将**保存的上下文存入用户区**，并继续在旧的上下文中执行。这就是说，进程执行一个**系统调用**，当因为资源或其他原因要去睡眠时，内核为进程作了一次`setjmp`。
+ 如果**在睡眠中被信号唤醒，进程不能再进入睡眠时**，内核为进程调用`longjmp`。该操作是内核把进程的上下文恢复成，原先`setjmp`调用保存在进程用户区的上下文，这样就使得进程可以恢复等待资源前的状态。而且内核为`setjmp`返回1，使得进程知道该次系统调用失败。这就是它们的作用。

*简言之，就是当用户进程因为需要转内核态去执行某些操作而进入阻塞队列时，内核通过`setjump`保存用户进程当前的上下文信息(保存在用户区）。*

*然后用户等待内核完系统调用/中断处理函数等操作后**正常**返回，则从`setjump`保存的上下文继续往下执行即可；*

*如果用户在等待内核完成内核态操作中，突然有信号中断了原本的系统调用等操作，等内核在用户进程上处理完信号后，使用`longjump`恢复之前用户态进程最后一次用`setjmp`保存的上下文信息，并让`setjmp`返回1，提醒进程上次的系统调用被中断，可能需要重新申请系统调用等操作。*

#### 与信号相关的系统调用

+ 系统调用`signal`是进程用来**设定某个信号的处理方法**
+ 系统调用`kill`是用来**发送信号**给指定进程的

`signal`和`kill`这两个调用可以形成信号的基本操作。**后两个调用`pause`和`alarm`是通过信号实现的进程暂停和定时器**，调用`alarm`是通过信号**通知进程定时器到时**。

1. `signal`系统调用

   系统调用`signal`用来**设定某个信号的处理方法**。该调用声明的格式如下：
   `void (*signal(int signum, void (*handler)(int)))(int);`
   在使用该调用的进程中加入以下头文件：
   `#include <signal.h>`
   上述声明格式比较复杂，如果不清楚如何使用，也可以通过下面这种类型定义的格式来使用（POSIX的定义）：
   `typedef void (*sighandler_t)(int);`
   `sighandler_t signal(int signum, sighandler_t handler);`
   <u>但这种格式在不同的系统中有不同的类型定义，所以要使用这种格式，最好还是参考一下联机手册</u>。
   在调用中，参数`signum`指出**要设置处理方法的信号**。第二个参数`handler`是一个**处理函数**，或者是

   + `SIG_IGN`：**忽略**参数`signum`所指的**信号**。
   + `SIG_DFL`：**恢复**参数`signum`所指信号的**处理方法为默认值**。

   传递给信号处理例程的**整数参数**是**信号值**，这样可以使得**一个信号处理例程处理多个信号**。

   系统调用`signal`返回值是指定信号`signum`前一次的处理例程，或者错误时返回错误代码`SIG_ERR`。

   下面来看一个简单的例子：

   ```c
   #include <signal.h>
   #include <unistd.h>
   #include<stdio.h>
   void sigroutine(int dunno)
   { /* 信号处理例程，其中dunno将会得到信号的值 */
     　　switch (dunno)
     {
       　　case 1 :
   　　printf("Get a signal -- SIGHUP ");
       　　break;
       　　case 2 :
   　　printf("Get a signal -- SIGINT ");
       　　break;
       　　case 3 :
   　　printf("Get a signal --a SIGQUIT ");
       　　break;
       　　
     }
     　　return;
     　　
   }
   int main()
   {
     　　printf("process id is %d ", getpid());
     　　signal(SIGHUP, sigroutine); //* 下面设置三个信号的处理方法
     　　signal(SIGINT, sigroutine);
     　　signal(SIGQUIT, sigroutine);
     　　for (;;);　
   }
   ```

   其中信号`SIGINT`由按下`Ctrl-C`发出，信号`SIGQUIT`由按下`Ctrl-\`发出。

   `SIGHUP`，当用户退出shell时，由该shell启动的所有进程将收到这个信号，默认动作为终止进程。
   
   该程序执行的结果如下：
   
   ```shell
   localhost:~$ ./sig_test
   process id is 463
   Get a signal -SIGINT //按下Ctrl-C得到的结果
   Get a signal -SIGQUIT //按下Ctrl-\得到的结果
   //按下Ctrl-z将进程置于后台
   [1]+ Stopped ./sig_test
   localhost:~$ bg
   [1]+ ./sig_test &
   localhost:~$ kill -HUP 463 //向进程发送SIGHUP信号
localhost:~$ Get a signal – SIGHUP
   kill -9 463 //向进程发送SIGKILL信号，终止进程。
   localhost:~$
   ```

2. `kill`系统调用

   系统调用`kill`**用来向进程发送一个信号**。该调用声明的格式如下：
   `int kill(pid_t pid, int sig);`
   在使用该调用的进程中加入以下头文件：

   ```c
   #include <sys/types.h>
   #include <signal.h>
   ```

   **该系统调用可以用来向任何进程或进程组发送任何信号**。

   + 如果参数`pid`是正数，那么该调用将信号sig发送到进程号为`pid`的进程。
   + 如果`pid`等于0，那么信号sig将发送给**当前进程所属进程组里的所有进程**。
   + 如果参数`pid`等于-1，信号sig将发送给**除了进程1和自身以外的所有进程**。
   + 如果参数`pid`小于-1，信号sig将发送给**属于进程组-pid的所有进程**。
   + 如果参数`sig`为0，将不发送信号。

   该调用执行成功时，返回值为0；错误时，返回-1并设置相应的错误代码`errno`。

   下面是一些可能返回的错误代码：

   + `EINVAL`：指定的信号`sig`无效。
   + `ESRCH`：参数`pid`指定的进程或进程组不存在。**注意，在进程表项中存在的进程，可能是一个还没有被wait收回，但已经终止执行的僵死进程。**
   + `EPERM`：进程**没有权力**将这个信号发送到指定接收信号的进程。因为，一个进程被允许将信号发送到进程`pid`时，必须拥有`root`权力，或者是发出调用的进程的`UID`或`EUID`与指定接收的进程的`UID`或保存用户ID（`savedset-user-ID`）相同。**如果参数`pid`小于-1，即该信号发送给一个组，则该错误表示组中有成员进程不能接收该信号**。

3. `pause`系统调用

   系统调用`pause`的作用是**等待一个信号**。该调用的声明格式如下：
   `int pause(void);`
   在使用该调用的进程中加入以下头文件：
   `#include <unistd.h>`
   **该调用使得发出调用的进程进入睡眠，直到接收到一个信号为止**。

   **该调用总是返回-1，并设置错误代码为`EINTR`（接收到一个信号）。**

   下面是一个简单的范例：

   ```c
   #include <unistd.h>
   #include <stdio.h>
   #include <signal.h>
   void sigroutine(int unused) {
       printf("Catch a signal SIGINT ");
   }
   int main() {
       signal(SIGINT, sigroutine);
       pause();
       printf("receive a signal ");
   }
   ```

   在这个例子中，程序开始执行，就象进入了死循环一样，这是因为**进程正在等待信号**，当我们按下`Ctrl-C`时，信号被捕捉，并且使得`pause`退出等待状态。

4. `alarm`和 `setitimer`系统调用   <small>(很多程序不再使用`alarm`，而是使用`setitimer`)</small>

   系统调用`alarm`的功能是**设置一个定时器**，**当定时器计时到达时，将发出一个信号给进程**。

   该调用的声明格式如下：
   `unsigned int alarm(unsigned int seconds);`
   在使用该调用的进程中加入以下头文件：
   `#include <unistd.h>`
   **系统调用`alarm`安排内核为调用进程在指定的seconds秒后发出一个`SIGALRM`的信号。**

   <u>如果指定的参数seconds为0，则不再发送 SIGALRM信号。后一次设定将取消前一次的设定。该调用返回值为上次定时调用到发送之间剩余的时间，或者因为没有前一次定时调用而返回0</u>。
   注意，在使用时，`alarm`**只设定为发送一次信号**，如果要多次发送，就要多次使用alarm调用。
   对于`alarm`，这里不再举例。**系统中很多程序不再使用`alarm`调用，而是使用`setitimer`调用来设置定时器，用`getitimer`来得到定时器的状态**，这两个调用的声明格式如下：

   + `int getitimer(int which, struct itimerval *value);`
   + `int setitimer(int which, const struct itimerval *value, struct itimerval *ovalue);`

   在使用这两个调用的进程中加入以下头文件：
   `#include <sys/time.h>`
   该**系统调用给进程提供了三个定时器**，它们各自有其独有的计时域，当其中任何一个到达，就发送一个相应的信号给进程，并使得计时器重新开始。

   三个计时器由参数which指定，如下所示：

   + `TIMER_REAL`：按实际时间计时，计时到达将给进程发送`SIGALRM`信号。
   + `ITIMER_VIRTUAL`：仅当进程执行时才进行计时。计时到达将发送`SIGVTALRM`信号给进程。
   + `ITIMER_PROF`：当进程执行时和系统为该进程执行动作时都计时。与`ITIMER_VIRTUAL`是一对，**该定时器经常用来统计进程在用户态和内核态花费的时间**。计时到达将发送`SIGPROF`信号给进程。

   定时器中的参数`value`用来指明定时器的时间，其结构如下：

   ```c
   struct itimerval {
       struct timeval it_interval; /* 下一次的取值 */
       struct timeval it_value; /* 本次的设定值 */
   };
   ```

   该结构中`timeval`结构定义如下：

   ```c
   struct timeval {
       long tv_sec; /* 秒 */
       long tv_usec; /* 微秒，1秒 = 1000000 微秒*/
   };
   ```

   在`setitimer`调用中，参数`ovalue`如果不为空，则其中保留的是上次调用设定的值。

   定时器将`it_value`递减到0时，产生一个信号，并将`it_value`的值设定为`it_interval`的值，然后重新开始计时，如此往复。

   当`it_value`设定为0时，计时器停止，或者当它计时到期，而`it_interval`为0时停止。

   调用成功时，返回0；错误时，返回-1，并设置相应的错误代码`errno`：

   + `EFAULT`：参数`value`或`ovalue`是无效的指针。
   + `EINVAL`：参数which不是`ITIMER_REAL`、`ITIMER_VIRT`或`ITIMER_PROF`中的一个。

   下面是关于`setitimer`调用的一个简单示范，在该例子中，每隔1.0秒发出一个`SIGALRM`，每隔0.5秒发出一个`SIGVTALRM`信号：

   ```c
   #include <signal.h>
   #include <unistd.h>
   #include <stdio.h>
   #include <sys/time.h>
   int sec;
   void sigroutine(int signo) {
       switch (signo) {
           case SIGALRM:
               printf("Catch a signal -- SIGALRM ");
               break;
           case SIGVTALRM:
               printf("Catch a signal -- SIGVTALRM ");
               break;
       }
       return;
   }
   int main() {
       struct itimerval value,ovalue,value2;
       sec = 5;
       printf("process id is %d ",getpid());
       signal(SIGALRM, sigroutine);
       signal(SIGVTALRM, sigroutine);
       value.it_value.tv_sec = 1;
       value.it_value.tv_usec = 0;
       value.it_value.tv_sec = 1;
       value.it_value.tv_usec = 0;
       setitimer(ITIMER_REAL, &value, &ovalue);
       value2.it_value.tv_sec = 0;
       value2.it_value.tv_usec = 500000;
       value2.it_value.tv_sec = 0;
       value2.it_value.tv_usec = 500000;
       setitimer(ITIMER_VIRTUAL, &value2, &ovalue);
       for (;;) ;
   }
   ```

   该例子的屏幕拷贝如下：

   ```shell
   localhost:~$ ./timer_test
   process id is 579
   Catch a signal – SIGVTALRM
   Catch a signal – SIGALRM
   Catch a signal – SIGVTALRM
   Catch a signal – SIGVTALRM
   Catch a signal – SIGALRM
   Catch a signal –GVTALRM
   ```

   本文（百度百科）简单介绍了Linux下的信号，如果希望了解其他调用，请参考联机手册或其他文档

> [MYSQL中对信号的处理(SIGTERM,SIGQUIT,SIGHUP等)](http://blog.itpub.net/7728585/viewspace-2142060/)  <= 挺详细的，不亚于百度上各种信号的解释

### 11.5.6 Linux的exit()、wait()和SIGCHLD信号(优质文章转载)

> [linux系统调用：exit()与_exit()函数详解](https://blog.csdn.net/drdairen/article/details/51896141)
>
> [[Linux]关于SIGCHLD](https://blog.csdn.net/xxpresent/article/details/73028750)
>
> [linux的SIGCHLD信号](https://blog.csdn.net/oguro/article/details/53857376)
>
> [SIGCHLD--百度百科](https://baike.baidu.com/item/SIGCHLD/1829827?fr=aladdin)
>
> [详解wait和waitpid函数](https://blog.csdn.net/kevinhg/article/details/7001719)
>
> 以下内容整合，百度百科、CSDN文章。

#### `exit()`和`_exit()`调用、执行过程

**Calling exit()**
The exit() function causes normal program termination.
The exit() function performs the following functions:
\1. All functions registered by the Standard C atexit() function are called in the reverse
order of registration. If any of these functions calls exit(), the results are not portable.
\2. **All open output streams are flushed (data written out) and the streams are closed.**
\3. All files created by tmpfile() are deleted.
\4. **The _exit() function is called.**

**Calling _exit()**
The _exit() function performs operating system-specific program termination functions.
These include:
\1. All open file descriptors and directory streams are closed.
\2. **If the parent process is executing a wait() or waitpid(), the parent wakes up and**
**status is made available.**
\3. **If the parent is not executing a wait() or waitpid(), the status is saved for return to**
**the parent on a subsequent wait() or waitpid().**
\4. Children of the terminated process are assigned a new parent process ID. Note: the
termination of a parent does not directly terminate its children.
\5. **If the implementation supports the SIGCHLD signal, a SIGCHLD is sent to the parent.**
\6. Several job control signals are sent.

`exit()`就是退出，传入的参数是<u>程序退出时的状态码</u>，**0表示正常退出，其他表示非正常退出，一般都用-1或者1**，标准C里有EXIT_SUCCESS和EXIT_FAILURE两个宏，用exit(EXIT_SUCCESS);可读性比较好一点

#### `exit()`和`_exit()`异同点

**`exit()`函数定义在`stdlib.h`中，而`_exit()`定义在`unistd.h`中**

+ `_exit()`函数的作用最为简单：直接使进程停止运行，清除其使用的内存空间，并销毁其在内核中的各种数据结构；

+ `exit() `函数则在这些基础上作了一些包装，在执行退出之前加了若干道工序，也是因为这个原因，有些人认为exit已经不能算是纯粹的系统调用。

`exit()`函数与`_exit()`函数最大的区别就在于`exit()`函数在调用`_exit`系统调用之前要检查文件的打开情况，把文件缓冲区中的内容写回文件，就是”清理I/O缓冲”。

exit()在结束调用它的进程之前，主要进行如下步骤：

1. 调用`atexit()`注册的函数（出口函数）；按ATEXIT注册时相反的顺序调用所有由它注册的函数,这使得我们可以指定在程序终止时执行自己的清理动作.例如,保存程序状态信息于某个文件,解开对共享数据库上的锁等.

2. `cleanup()`；关闭所有打开的流，这将导致写所有被缓冲的输出，删除用TMPFILE函数建立的所有临时文件.

3. 最后调用`_exit()`函数终止进程。

`_exit()`主要做3件事：

1. Any open file descriptors belonging to the process are closed

2. any children of the process are inherited by process 1, init

3. the process’s parent is sent a SIGCHLD signal

exit执行完清理工作后就调用_exit来终止进程。

#### wait和waitpid

​	进程一旦调用了`wait`，就立即**阻塞**自己，由`wait`自动分析是否当前进程的某个子进程已经退出，如果让它找到了这样一个**已经变成僵尸的子进程**，`wait`就会收集这个子进程的信息，并把它彻底销毁后返回；**如果没有找到这样一个子进程，wait就会一直阻塞在这里，直到有一个出现为止。**

​	`pid_t wait(int *status)`的参数`status`用来保存"被收集进程"退出时的一些状态，它是一个指向`int`类型的指针。

​	但如果我们对这个子进程是如何死掉的毫不在意，只想把这个僵尸进程消灭掉，（事实上绝大多数情况下，我们都会这样想），我们就可以设定这个参数为NULL，就象下面这样：

```c
#include <sys/types.h> /* 提供类型pid_t的定义 */
#include <sys/wait.h>

pid_t wait(int *status);
pid = wait(NULL);
```

- 如果成功，`wait`会返回被收集的子进程的进程ID
- 如果调用进程没有子进程，调用就会失败，此时`wait`返回-1，同时`errno`被置为`ECHILD`。

`wait`调用例程

```c
/* wait1.c */
#include <sys/types.h>
#include <sys/wait.h>
#include <unistd.h>
#include <stdlib.h>

main()
{

    pid_t pc,pr;
    pc=fork();

    if(pc<0) /* 如果出错 */
        printf("error ocurred!\n");
    else if(pc==0){ /* 如果是子进程 */
        printf("This is child process with pid of %d\n",getpid());
        sleep(10); /* 睡眠10秒钟 */
    }
    else{ /* 如果是父进程 */
        pr=wait(NULL); /* 在这里等待 */
        printf("I catched a child process with pid of %d\n",pr);
    }
    exit(0);
}
```

运行结果如下：

```shell
$ gcc wait1.c -o wait1
$ ./wait1
This is child process with pid of 1508
I catched a child process with pid of 1508
```

*在第2行结果打印出来前有10 秒钟的等待时间，只有子进程从睡眠中苏醒过来，它才能正常退出，也才能被父进程捕捉到。不管设定子进程睡眠的时间有多长，父进程都会一直等待下去。*

参数`status`：

如果参数status的值不是NULL，wait就会把子进程退出时的状态取出并存入其中，这是一个整数值（`int`），指出了子进程是正常退出还是被非正常结束的（一个进程也可以被其他进程用信号结束），以及正常结束时的返回值，或被哪一个信号结束的等信息。由于这些信息被存放在一个整数的不同二进制位中，所以用常规的方法读取会非常麻烦，人们就设计了一套专门的宏（`macro`）来完成这项工作，下面我们来学习一下其中最常用的两个：

1. `WIFEXITED(status)`

   ​	这个宏用来**指出子进程是否为正常退出的，如果是，它会返回一个非零值**<small>（请注意，虽然名字一样，这里的参数`status`并不同于wait唯一的参数---指向整数的指针`status`，而是那个指针所指向的整数，切记不要搞混了）</small>

2. `WEXITSTATUS(status)`

   ​	**当`WIFEXITED`返回非零值时，我们可以用这个宏来提取子进程的返回值**，如果子进程调用exit(5)退出，`WEXITSTATUS(status)` 就会返回5；如果子进程调用exit(7)，WEXITSTATUS(status)就会返回7。请注意，**如果进程不是正常退出的，也就是说， `WIFEXITED`返回0，这个值就毫无意义。**

例程：

```c
/* wait2.c */
#include <sys/types.h>
#include <sys/wait.h>
#include <unistd.h>

main()
{
    int status;
    pid_t pc,pr;

    pc=fork();
    if(pc<0) /* 如果出错 */
        printf("error ocurred!\n");
    else if(pc==0){ /* 子进程 */
        printf("This is child process with pid of %d.\n",getpid());
        exit(3); /* 子进程返回3 */
    }
    else{ /* 父进程 */
        pr=wait(&status);
        if(WIFEXITED(status)){ /* 如果WIFEXITED返回非零值 */
            printf("the child process %d exit normally.\n",pr);
            printf("the return code is %d./n",WEXITSTATUS(status));
        }else /* 如果WIFEXITED返回零 */
            printf("the child process %d exit abnormally.\n",pr);
    }
}
```

运行结果如下：

```shell
$ gcc wait2.c -o wait2
$ ./wait2
This is child process with pid of 1538.
the child process 1538 exit normally.
the return code is 3.
```

父进程准确捕捉到了子进程的返回值3，并把它打印了出来。

*当然，处理进程退出状态的宏并不止这两个，但它们当中的绝大部分在平时的编程中很少用到，有兴趣可以自己参阅[Linux man pages](https://www.kernel.org/doc/man-pages/)去了解它们的用法。*

##### wait实现进程同步

​	有时候，父进程要求子进程的运算结果进行下一步的运算，或者子进程的功能是为父进程提供了下一步执行的先决条件（如：子进程建立文件，而父进程写入数据），此时父进程就必须在某一个位置停下来，等待子进程运行结束，而如果父进程不等待而直接执行下去的话，可以想见，会出现极大的混乱。这种情况称为进程之间的同步，更准确地说，这是进程同步的一种特例。**进程同步就是要协调好2个以上的进程，使之以安排好地次序依次执行。**解决进程同步问题有更通用的方法，但对于该假设的情况，则完全可以用wait系统调用简单的予以解决。请看下面这段程序：

```c
#include <sys/types.h>
#include <sys/wait.h>

main()
{
    pid_t pc, pr;
    int status;
    
    pc=fork();
    if(pc<0)
        printf("Error occured on forking./n");
    else if(pc==0){
        /* 子进程的工作 */
        exit(0);
    }else{
        /* 父进程的工作 */
        pr=wait(&status);
        /* 利用子进程的结果 */
    }
}
```

​	<small>这段程序只是个例子，不能真正拿来执行，但它却说明了一些问题，首先，当fork调用成功后，父子进程各做各的事情，但当父进程的工作告一段落，需要用到子进程的结果时，它就停下来调用wait，一直等到子进程运行结束，然后利用子进程的结果继续执行，这样就圆满地解决了我们提出的进程同步问题。</small>

`waitpid`系统调用在Linux函数库中的原型是：

```c
#include <sys/types.h> /* 提供类型pid_t的定义 */
#include <sys/wait.h>
pid_t waitpid(pid_t pid,int *status,int options)
```

​	从本质上讲，系统调用`waitpid`和`wait`的作用是完全相同的，但`waitpid`多出了两个可由用户控制的参数`pid`和`options`，从而为我们编程提供了另一种更灵活的方式。下面我们就来详细介绍一下这两个参数：

+ `pid`：从参数的名字`pid`和类型`pid_t`中就可以看出，这里需要的是一个进程ID。但当`pid`取不同的值时，在这里有不同的意
  + `pid`>0时，**只等待进程ID等于`pid`的子进程**，不管其它已经有多少子进程运行结束退出了，只要指定的子进程还没有结束，`waitpid`就会一直等下去。
  + `pid`=-1时，等待**任何一个子进程**退出，没有任何限制，此时`waitpid`和`wait`的作用一模一样。
  +  `pid`=0时，等待**同一个进程组中的任何子进程**，如果子进程已经加入了别的进程组，`waitpid`不会对它做任何理睬。
  + `pid`<-1时，等待一个**指定进程组中的任何子进程**，这个进程组的ID等于`pid`的绝对值。

+ `options`：提供了一些额外的选项来控制`waitpid`，目前在Linux中只支持`WNOHANG`和`WUNTRACED`两个选项，这是两个常数，可以用"|"运算符把它们连接起来使用。

  示例：`ret=waitpid(-1,NULL,WNOHANG | WUNTRACED);`，如果不想使用`options`，设置为0=>`ret=waitpid(-1,NULL,0);`

  **如果使用了`WNOHANG`参数调用`waitpid`，即使没有子进程退出，它也会立即返回，不会像wait那样永远等下去。**

  而`WUNTRACED`参数，由于涉及到一些跟踪调试方面的知识，加之极少用到，有兴趣可以自行查阅相关材料。

`wait`不就是经过包装的`waitpid`吗？没错，察看`<内核源码目录>/include/unistd.h`文件349-352行就会发现以下程序段：

```c
static inline pid_t wait(int * wait_stat)
{
    return waitpid(-1,wait_stat,0);
}
```

`waitpid`的返回值比`wait`稍微复杂一点，一共有3种情况：

1. 当正常返回的时候，`waitpid`返回收集到的子进程的进程ID；
2. 如果设置了选项`WNOHANG`，而调用中`waitpid`**发现没有已退出的子进程可收集，则返回0**；
3. 如果调用中出错，则返回-1，这时`errno`会被设置成相应的值以指示错误所在；

当`pid`所指示的子进程不存在，或此进程存在，但不是调用进程的子进程，`waitpid`就会出错返回，这时`errno`被设置为`ECHILD`；

```c
/* waitpid.c */
#include <sys/types.h>
#include <sys/wait.h>
#include <unistd.h>

main()
{
    pid_t pc, pr;

    pc=fork();
    
    if(pc<0) /* 如果fork出错 */
        printf("Error occured on forking.\n");
    else if(pc==0){ /* 如果是子进程 */
        sleep(10); /* 睡眠10秒 */
        exit(0);
    }
    /* 如果是父进程 */
    do{
        pr=waitpid(pc, NULL, WNOHANG); /* 使用了WNOHANG参数，waitpid不会在这里等待 */
        if(pr==0){ /* 如果没有收集到子进程 */
            printf("No child exited\n");
            sleep(1);
        }
    }while(pr==0); /* 没有收集到子进程，就回去继续尝试 */
    if(pr==pc)
        printf("successfully get child %d\n", pr);
    else
        printf("some error occured\n");
}
```

运行结果如下：

```shell
$ cc waitpid.c -o waitpid
$ ./waitpid
No child exited
No child exited
No child exited
No child exited
No child exited
No child exited
No child exited
No child exited
No child exited
No child exited
successfully get child 1526
```

父进程经过10次失败的尝试之后，终于收集到了退出的子进程。

因为这只是一个例子程序，不用写得太复杂，所以我们就让父进程和子进程分别睡眠了10秒钟和1秒钟，代表它们分别作了10秒钟和1秒钟的工作。父子进程都有工作要做，父进程利用工作的简短间歇察看子进程的是否退出，如退出就收集它。

提示：可以尝试在最后一个例子中把`pr=waitpid(pc, NULL, WNOHANG); `改为`pr=waitpid(pc, NULL, 0);`或者`pr=wait(NULL);`看看运行结果有何变化？（修改后的结果使得父进程将自己阻塞，直到有子进程退出为止！）

DESCRIPTION和NOTES： <==摘自[Linux man pages](https://man7.org/linux/man-pages/man2/wait.2.html)，详情自己看看。

```txt
DESCRIPTION 
All of these system calls are used to wait for state changes in a
child of the calling process, and obtain information about the child
whose state has changed.  A state change is considered to be: the
child terminated; the child was stopped by a signal; or the child was
resumed by a signal.  In the case of a terminated child, performing a
wait allows the system to release the resources associated with the
child; if a wait is not performed, then the terminated child remains
in a "zombie" state (see NOTES below).

If a child has already changed state, then these calls return
immediately.  Otherwise, they block until either a child changes
state or a signal handler interrupts the call (assuming that system
calls are not automatically restarted using the SA_RESTART flag of
sigaction(2)).  In the remainder of this page, a child whose state
has changed and which has not yet been waited upon by one of these
-------------------------------
NOTES

A child that terminates, but has not been waited for becomes a
"zombie".  The kernel maintains a minimal set of information about
the zombie process (PID, termination status, resource usage
information) in order to allow the parent to later perform a wait to
obtain information about the child.  As long as a zombie is not
removed from the system via a wait, it will consume a slot in the
kernel process table, and if this table fills, it will not be
possible to create further processes.  If a parent process
terminates, then its "zombie" children (if any) are adopted by
init(1), (or by the nearest "subreaper" process as defined through
the use of the prctl(2) PR_SET_CHILD_SUBREAPER operation); init(1)
automatically performs a wait to remove the zombies.

POSIX.1-2001 specifies that if the disposition of SIGCHLD is set to
SIG_IGN or the SA_NOCLDWAIT flag is set for SIGCHLD (see
sigaction(2)), then children that terminate do not become zombies and
a call to wait() or waitpid() will block until all children have
terminated, and then fail with errno set to ECHILD.  (The original
POSIX standard left the behavior of setting SIGCHLD to SIG_IGN
unspecified.  Note that even though the default disposition of
SIGCHLD is "ignore", explicitly setting the disposition to SIG_IGN
results in different treatment of zombie process children.)

Linux 2.6 conforms to the POSIX requirements.  However, Linux 2.4
(and earlier) does not: if a wait() or waitpid() call is made while
SIGCHLD is being ignored, the call behaves just as though SIGCHLD
were not being ignored, that is, the call blocks until the next child
terminates and then returns the process ID and status of that child.
```



#### SIGCHLD

​	在一个进程终止或者停止时，将SIGCHLD信号发送给其父进程。按**系统默认将忽略此信号**。如果[父进程](https://baike.baidu.com/item/父进程)希望被告知其子系统的这种状态，则应捕捉此信号。**信号的捕捉函数中通常调用wait函数以取得进程ID和其终止状态**。

系统默认忽略`SIGCHLD`：即唤醒一个收到`SIGCHLD`信号且可被中断的进程（父进程）。

1. 如果被唤醒的父进程，原本执行的是阻塞一直等待直到有某子进程exit执行的wait操作，那么继续执行wait内部的后续操作，释放该子进程的内核资源（内部系统调用请求内核回收子进程PCB资源）；
2. 如果被唤醒的父进程接下去执行的不是wait，且还没exit，将导致子进程仍处于zombie状态，因为子进程占用的内核资源PCB等还没释放。由于忽略策略，父进程没有其他处理`SIGCHLD`的handler注册到内核，所以继续睡眠（回到原本的状态）。

**如果在父进程中显式声明`signal(SIGCHLD,SIG_IGN)`，那么子进程退出时发出`SIGCHLD`直接被父进程忽略，且子进程无需进入zombie状态，直接释放。如果父进程设置SIGCHLD的其他handler处理方法（还是需要执行wait，否则一直zombie），或执行wait，那么子进程退出会进入zombie状态，直到wait内部释放子进程资源后，子进程退出zombie状态，真正消亡。**

之前我们就学过，关于`wait`和`waitpid`来**处理僵尸进程**，父进程等待子进程结束后自己才退出。

<small>如果父进程不处理子进程中的僵尸进程，那么子进程将一直处于Zomibie状态，直到子进程PCB等内核态资源被释放，用户态的子进程本身没法释放这些资源，需要父进程执行wait才能协助子进程释放资源。<=如果父进程早于子进程exit，父进程exit中会把其子进程的父进程改成init进程。而init进程又会周期性执行wait清理zombie进程。</small>

1. 父进程使用**阻塞**的方式等待子进程退出
2. 父进程通过**非阻塞**，即**轮询**的方式。

**子进程终止后会给父进程发送一个`SIGCHLD`信号，操作系统默认为父进程忽略该信号（Linux会进入Zombie，除非显式声明`signal(SIGCHLD,SIG_IGN);`，SIG_IGN表明父进程无需获取子进程退出的信号，从而子进程exit后直接释放资源，无进入zombie状态，亲测如此）**。

如果你想要知道那个信号，那就用用户自定义的处理函数即可。

下面编写一个代码来验证一下子进程终止，确实是会给父进程发送`SIGCHLD`信号。

```c
#include<stdio.h>
#include<unistd.h>
#include<stdlib.h>
#include<signal.h>
void myhandler(int sig)
{
    printf("my sig is %d\n",sig);
}
int main()
{
    pid_t id = fork();
    signal(SIGCHLD,myhandler); // 等于父/子进程都注册了该handler,实际只有在父进程中才有意义
    if(id == 0)//child
    {
        // signal(SIGCHLD,myhandler);  // 如果改写在这，无作用，因为子进程是要求内核发送给父进程信号；而父进程才是真正捕获到SIGCHLD，且需要handler处理的一方。
        printf("i am child!pid:%d\n",getpid());
        sleep(5);
        exit(1);
    }
    else //father
    {
        // signal(SIGCHLD,myhandler); // 改写到这里，输出一样。因为这个本来就是针对捕获到SIGCHLD的进程，去注册myhanlder，让内核下次内核态切回用户态时，先看到父进程有收到信号<=其实就是某信号标志位变1，于是在父进程调用栈顶部插入事先注册的myhandler函数，并且函数出口对应原本栈顶。即内核态运行完myhandler处理函数后，再切回用户态，真正让父进程继续上次保存的PCB的调用栈往下执行代码。这样用户态进程无感知信号处理，且不是用户态运行内核方法，更安全。
        while(id = waitpid(id,NULL,0) > 0)
        {
            printf("wait child success:%d       pid:%d\n",id,getpid());
        }
        printf("child is quit!%d\n",getpid());
    }
    return 0;
}
```

运行结果：（通过`kill -l`可以看到17对应的正好是`SIGCHLD`信号）

```shell
# 将设编译链接出来的程序叫 child
localhost:~$ ./child
i am child!pid:4488
my sig is 17 # sleep(5),所以这行及往后的是5s后才输出的
wait child success:1       pid:4487
child is quit!4487
```

也可以使用`sigaction`来捕捉`SIGCHLD`信号，在这里<u>对于这个例子来说也叫做父进程等待子进程的异步版本</u>，具体实现如下：

```c
#include<stdio.h>
#include<unistd.h>
#include<stdlib.h>
#include<signal.h>

void myhandler(int sig)
{
    printf("my sig is %d\n",sig);
}

int main()
{
    signal(SIGCHLD,myhandler);
    pid_t cid;
    if((cid = fork())== 0)//child
    {
        printf("i am child!pid:%d\n",getpid());
        sleep(5);
        exit(1);
    }
    else
    {

        struct sigaction act;   
        act.sa_handler = myhandler;
        sigemptyset(&act.sa_mask);
        act.sa_flags = 0;
        sigaction(SIGCHLD,&act,NULL);
        while(1)
        {
            printf("i am parent,my pid is %d\n",getpid());
            sleep(1);
        }

    }
    //  else //father
    //  {
    //      while(id = waitpid(id,NULL,0) > 0)
    //      {
    //          printf("wait child success:%d       pid:%d\n",id,getpid());
    //      }
    //      printf("child is quit!%d\n",getpid());
    //  }
    return 0;
}

```

运行结果如下：

当子进程运行5秒后，`sigaction`函数捕捉到`SIGCHLD`信号，也即子进程退出的信号，然后父进程运行。

```shell
i am parent,my pid is 4515
i am child!pid:4516
i am parent,my pid is 4515
i am parent,my pid is 4515
i am parent,my pid is 4515
i am parent,my pid is 4515
my sig is 17
i am parent,my pid is 4515
i am parent,my pid is 4515
i am parent,my pid is 4515
i am parent,my pid is 4515
i am parent,my pid is 4515
i am parent,my pid is 4515
i am parent,my pid is 4515
^C
```

另一篇文章中对`signal`讲解的文章，[linux的SIGCHLD信号](https://blog.csdn.net/oguro/article/details/53857376)的代码和执行结果。（文章其余内容这里不展示了，感兴趣可以点进去看看）

**借助`SIGCHLD`信号回收子进程**

```c
#include <stdio.h>
#include <stdlib.h>
#include <unistd.h>
#include <errno.h>
#include <sys/types.h>
#include <sys/wait.h>
#include <signal.h>

void sys_err(char *str)
{
    perror(str);
    exit(1);
}

void do_sig_child(int signo)
{
    int status;
    pid_t pid;

    //    if ((pid = waitpid(0, &status, WNOHANG)) > 0) {
    while ((pid = waitpid(0, &status, WNOHANG)) > 0) {
        if (WIFEXITED(status))
            printf("------------child %d exit %d\n", pid, WEXITSTATUS(status));
        else if (WIFSIGNALED(status))
            printf("child %d cancel signal %d\n", pid, WTERMSIG(status));
    }
}

int main(void)
{
    pid_t pid;
    int i;
    //阻塞SIGCHLD
    for (i = 0; i < 10; i++) {
        if ((pid = fork()) == 0)
            break;
        else if (pid < 0)
            sys_err("fork");
    }

    if (pid == 0) {     //10个子进程
        int n = 1;
        while (n--) {
            printf("child ID %d\n", getpid());
            sleep(1);
        }
        return i+1;
    } else if (pid > 0) {
        //SIGCHLD阻塞
        struct sigaction act;

        act.sa_handler = do_sig_child;
        sigemptyset(&act.sa_mask);
        act.sa_flags = 0;
        sigaction(SIGCHLD, &act, NULL);
        //解除对SIGCHLD的阻塞

        while (1) {
            printf("Parent ID %d\n", getpid());
            sleep(1);
        }
    }

    return 0;
}
```

CentOS7的单核CPU环境，运行结果如下：

```shell
Parent ID 4553
child ID 4554
child ID 4555
child ID 4556
child ID 4557
child ID 4558
child ID 4559
child ID 4560
child ID 4561
child ID 4562
child ID 4563
Parent ID 4553
------------child 4554 exit 1
Parent ID 4553
------------child 4555 exit 2
Parent ID 4553
------------child 4556 exit 3
Parent ID 4553
------------child 4557 exit 4
Parent ID 4553
------------child 4558 exit 5
Parent ID 4553
------------child 4559 exit 6
Parent ID 4553
------------child 4560 exit 7
Parent ID 4553
------------child 4561 exit 8
Parent ID 4553
------------child 4562 exit 9
Parent ID 4553
------------child 4563 exit 10
Parent ID 4553
Parent ID 4553
Parent ID 4553
Parent ID 4553
Parent ID 4553
Parent ID 4553
^C
```

> [C语言中exit(0)与exit(1)有什么区别](https://blog.csdn.net/yyfwd/article/details/50548359) <= 简单明了代码，推荐阅读
>
> **_exit()；和exit()主要区别是exit()退出进程会清理I/O缓冲区，而\_exit()直接结束进程进入到内核中。**
>
> exit()将缓冲区的数据写完后才能退出来，所以调用exit()函数后程序并不会马上退出，这就是有些出现的僵尸程序，而_exit是直接退出进入到内核中去。
>
> return是语言级别的，它表示了调用堆栈的返回；而exit是系统调用级别的，它表示了一个进程的结束。
>
> return是返回函数调用，如果返回的是main函数，则为退出程序  
>
> exit是在调用处强行退出程序，运行一次程序就结束  
>
> [如何处理SIGCHLD信号？](https://zhuanlan.zhihu.com/p/56520942)
>
> [glibc-download](https://ftp.gnu.org/gnu/glibc/?C=M;O=A) <== 附上C库源代码下载地址。
>
> [The Linux *man-pages* project](https://www.kernel.org/doc/man-pages/)
>
> [__attribute__((noreturn))的用法](https://www.cnblogs.com/sea-stream/p/11233641.html)
>
> 这个属性**告诉编译器函数不会返回**，这可以用来抑制关于未达到代码路径的错误。 C库函数abort（）和exit（）都使用此属性声明：
>
> ```c
> extern void exit(int)   __attribute__((noreturn));
> extern void abort(void) __attribute__((noreturn));
> ```

### 11.5.7 Linux进程间通信IPC(优质网文转载)

CPU提供"test-and-set"or"Exchange"机制的锁原语指令，而操作系统具体两种实现方案：有忙等待（spin自旋）or五忙等待（使用阻塞队列）。

CPU提供fence系列原语，用于不同形式的禁止CPU指令重排序；而语言的fence系列函数，用于提供编译器编译期禁止代码优化的重排序。

CPU支持的fence：

1. LoadFence：fence之前的读操作都必须完成
2. StoreFence：fence之前的写操作都必须完成
3. MemoryFence：fence之前所有Load、Store都必须完成

> [How does a mutex work? What does it cost?](https://mortoray.com/2019/02/20/how-does-a-mutex-work-what-does-it-cost/)
>
> [CPU Memory – Why do I need a mutex?](https://mortoray.com/2010/11/18/cpu-memory-why-do-i-need-a-mutex/)
>
> CPU能够保证多核之间访问内存不会产生异常，保证每条机械指令具有原子性，多核访问同一片内存具有互斥性。
>
> 因为CPU对进程、线程等无感知，就只负责执行一条条原子性的机械码指令，所以早期OS为了进程间某些资源同步，使用CPU提供的实现"test-and-set"原语的Mutex，需要消耗CPU资源进行自旋spin-lock（也就是while循环判断某一块内存的数值等），当某进程while自旋条件满足test-and-set的期望值后，该进程才能继续往下执行其他操作。（对CPU来说不过是普通地重复执行某几条机械指令）。
>
> 大部分CPU架构能够保证一个基础数值类型integer, byte, long和double的读和写是原子操作。也就是说，不会出现某进程写入一个Integer的4bytes到内存时，另一个进程正好读到该新写入内存的Integer的前2Bytes和原本位于该内存位置的后2Bytes，这个Integer要么完整4个Byte被写入内存，要么就不被写入。
>
> 随着后续CPU升级优化，CPU指令流水线执行，会打乱数个原子性机械码指令的执行顺序。（比如原本传给CPU的机械码指令是a，b，c。但是CPU根据一些优化调度算法，最后选择按照c、a、b的顺序执行，以保证CPU指令流水线执行效率）。
>
> CPU经常重排序机械指令以获取更高的执行效率，仅仅对某块内存的访问进行锁定是不能保证互斥的进程正常执行的。所以Mutex除了保证互斥以外，还提供了fence功能。当CPU遇到一个fence时，它必须保证通过fence之前的所有重排序过的读入和写入内存的操作能够先于后续的指令完成。这确保如果两个线程都使用fence，另一个线程下次读取内存时不会只读取到部分内容。（即保证Mutex保证互斥的代码段一定被完全执行，下次CPU调度时才会让另一个test-and-set获取Mutex的线程读取到该内存区域。否则轮到了也读不到继续自旋，直到另一个Mutex的指令真正被执行完且释放Mutex=>计数值改变）
>
> 如果是线程的话，内存指针指向同一个主线程所在内存区域，test-and-set可以只监听用户态内存，而无需自旋时反复切换用户态/内核态。如果是进程，提前设置子进程和父进程共享Mutex的内存空间，那同样可以保证test-and-set自旋时无需反复切换用户态/内核态。
>
> 回顾前面第9章可知道，锁通常可通过"test-and-set"or"Exchange"实现，具体实现形式，又分为有忙等待（内部自旋）or无忙等待（使用等待队列）的方式。
>
> [CPU同步机制漫谈](https://blog.csdn.net/better0332/article/details/3635652)
>
> [X86/GCC memory fence的一些见解](https://zhuanlan.zhihu.com/p/41872203) <==推荐阅读
>
> ​	在编写single writer lock-free代码的时候，通常需要手动使用memory fence/barrier来确保修改对其他core可见并防止乱序（对于multiple writer的情况一般需要atomic RMW操作，隐含了memory fence，不需手动加）。一般来说memory fence分为两层：
>
> + compiler fence
> + CPU fence
>
> ​	**前者只在编译期生效，目的是防止compiler生成乱序的内存访问指令；后者通过插入或修改特定的CPU指令，在运行时防止内存访问指令乱序执行。**
>
> [Linux 锁与进程间通讯](https://zhuanlan.zhihu.com/p/94779989) <== 精品文，内容较多。建议阅读
>
> [【Linux】进程间同步（进程间互斥锁、文件锁）](https://blog.csdn.net/qq_35396127/article/details/78942245) <== C代码实例，建议阅读
>
> [Linux 锁与进程间通讯](https://zhuanlan.zhihu.com/p/94779989) <== 建议阅读，从锁到 各种IPC策略都有涉及。

# 12. 文件、IO、虚拟文件系统

## 12.1 基本概念

### 12.1.1 文件系统和文件

#### 概述

+ 文件系统：一种用于**持久性存储**的系统抽象  *<small>(比如硬盘，闪存（U盘、SD卡）)</small>*
  + 在存储器上：组织、控制、导航、访问和检索数据
  + 大多数计算机系统包含文件系统
  + 个人电脑、服务器、笔记本电脑
  + iPod、Tivo/机顶盒、手机/掌上电脑
  + Google可能是由一个文件系统构成的

  *<small>(内存是RAM，CPU里缓存Cache用的SRAM，我们内存条DRAM，都需要通电，后者需要周期充电=>定时刷新，所以比前者SRAM读写效率低。内存断电丢数据，非持久性存储。)</small>*

+ 文件：文件系统中一个单元的相关数据在操作系统中的抽象

*(大至大型服务器，小到嵌入式系统，还有常用桌面级计算机，只要想把数据永久保存到物理存储器介质上，就要用到文件系统。)*

---

#### 文件系统

+ 分配文件磁盘空间
  + 管理**文件块**（哪一块属于哪一个文件）
  + 管理**空闲空间**（哪一块是空闲的）
  + 分配**算法** （策略）

+ 管理文件集合
  + **定位**文件及其内容
  + **命名**：通过名字找到文件的接口
  + **最常见**：分层文件系统
  + 文件系统类型（组织文件的不同方式）

+ 提供的便利及特征
  + **保护**：分层来保护数据安全
  + **可靠性/持久性**：保护文件的持久即使发生崩溃、媒体错误、攻击等

---

#### 文件

+ 文件属性   *<small>(详细才方便文件系统检索文件)</small>*
  + 名称、类型、位置、大小、保护、创建者、创建时间、最近修改时间、...

+ 文件头
  + 在存储元数据中保存了每个文件的信息
  + <u>保存了文件的属性</u>
  + 跟踪哪一块存储块属于逻辑上文件结构的哪个偏移

### 12.1.2 文件描述符

*<small>(如果说前面是从普通用户的角度看待文件，那么这里"文件描述符"可以说是针对编程角度去看文件)</small>*

#### 文件使用模式

- 进程访问文件数据前必须先“打开”文件

  ```c
  f = open(name, flag);
  ...
  ... = read(f, ...);
  ...
  close(f);
  ```

  *(编程时，打开一个文件，会返回一个"整形描述符"，其本质是一个整形数字。这个数字就代表这个文件，这里`f`就是一个整形数字)*

- **内核跟踪进程打开的所有文件**

  + 操作系统为**每个进程**维护一个**打开文件表**

  + 一个**打开文件描述符**是这个表中的**索引**

![img](https://img-blog.csdn.net/20180425095701906?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0FsYXRlYmxvb21lcg==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)

+ 需要元数据数据来管理打开的文件：
  + **文件指针**：指向最近的一次读写位置，每个打开了这个文件的进程都有一个文件指针
  + **文件打开计数**：当前打开文件的次数，当最后一个进程关闭了文件时，允许将其从打开文件表中移除
  + **文件的磁盘位置**：缓存数据访问信息
  + **访问权限**：每个程序访问模式信息

  *(文件打开计数，因为文件是共享资源，允许多个进程共享访问，所以需要记录。直到没有进程访问了才可以移除该计数。)*

  *(文件磁盘位置，才能直到从硬盘哪个目录哪个文件块读取的文件数据等。)*

+ 用户视图：
  
+ 持久的**数据结构**
  
+ 系统访问接口

  + **字节**的集合(UNIX)
  + 系统不会关心你想存储在磁盘上的任何的数据结构

  *（程序里读文件，相当把文件从磁盘加载到内存，而写文件时，其实修改的是内存中加载到的文件数据，等到程序关闭文件后，才操作系统才会把内存里的文件数据重新写回磁盘中。毕竟磁盘比内存慢几个数量级，内存中进程不可能是直接与硬盘交互，那样太慢。）*

+ 操作系统内部视角
  + **块的集合（块是逻辑转换单元，而扇区是物理转换单元）**   *<small>(和内存的逻辑页，对应物理帧，类似。)</small>*
  + 块大小<>扇区大小：在Unix中，块的大小是4kB

  *<small>（内存一般用页来表示，而硬盘用扇区来表示。一般一个扇区4KB大小，而内存的页一般是4KB或更大的单位。在我们自己操作的时候，一般以字节为单位读写，而对磁盘的读写，则一般以扇区为单位。文件系统需要保证这两种差异的转换，而不用让用户关心页数据到扇区数据的单位转换等问题。）</small>*

----

当用户说：给我2-12字节空间时会发生什么

+ 获取字节所在的块
+ 返回块内对应部分

如果说要写2-12字节呢？

+ 获取块
+ 修改块内对应部分
+ 写回块

**在文件系统中的所有操作都是在<big>整个块空间</big>上进行的**。

+ <u>举个例子，`getc()`和`putc()`：即使每次只访问1字节的数据，也需要缓存目标数据4096字节</u>

---

#### 文件访问模式

+ 用户怎么访问文件
  
+ 在系统层面需要知道用户的**访问模式**
  
+ **顺序访问**：按字节依次读取
  
+ 几乎所有的访问都是这种方式
  
+ **随机访问**：从中间读写

  + 不常用，但是仍然重要。例如，<u>虚拟内存支持文件：内存页存储在文件中</u>
  + 更加快速 - 不希望获取文件中间的内容的时候也必须先获取块内所有字节

+ **基于内容访问**：通过特征

  + 许多系统不提供此种访问方式，相反，**数据库是建立在索引内容的磁盘访问上（需要高效的随机访问）**

  ![img](https://img-blog.csdn.net/20180425105023645?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0FsYXRlYmxvb21lcg==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)

  *（典型的"基于内容访问"，它有两个文件，一个是index file，另一个是relative file。通过index文件找到在relative关系文件中对应关系的记录项信息。所以要取出信息，需要访问2个文件，先访问index file获取logical record number，再从对应的relative file的指定位置查找到对应记录项。）*

  *（这是典型的数据库访问模式，一般文件系统不会用"基于内容访问"，基本就数据库会这么干了。）*

---

#### 文件结构

- 无结构
  - 单词、比特序列
- 简单记录结构
  - 列
  - 固定长度
  - 可变长度 
- 复杂结构
  - 格式化文档（如，MS Word, PDF）
  - 可执行文件
  - ...

*<small>（我们平时用的`.doc`等文件，都有复杂的文件结构，文件内结构、数据的具体含义，交由应用程序去识别。所以打开`.doc`文件需要用Word程序去打开，单纯用C语言读写文件的库就没法识别文件里面的数据，看上去就是杂乱无章的字节。）</small>*

*<small>（对操作系统而言，不管文件实现用的什么结构，它都需要统一、简洁的方式去理解文件的内容。所以，操作系统把所有文件都当作串行字节流操作，具体的数据含义等，就交给应用程序去处理了。）</small>*

​	操作系统把所有文件都当成普通的串行字节流来操作，这样系统更具通用性，而应用程序可以根据情况实际应用场景自行设计和实现合适的文件结构来存取数据。

---

#### 文件共享、文件访问

*<small>(由于文件是共享资源，所以要考虑安全性，比如实现文件的访问控制。)</small>*

- **多用户系统**中的文件共享是很必要的

- 访问控制

  + 谁能够获得哪些文件的哪些访问权限

  + 访问模式: 读、写、执行、删除、列举等

- 文件访问控制列表(ACL)

  - <文件实体，权限>

- Unix模式
  - <用户|组|所有人，读|写|可执行>
  - **用户ID**识别用户，表明每个用户所允许的权限及保护模式
  - **组ID**允许用户组成组，并指定了组访问权限

- 指定多用户/客户如何同时访问共享文件

  + 和进程同步算法相似

  + 因磁盘I/O和网络延迟而设计简单

- Unix 文件系统(UFS)语义

  + 对打开文件的写入内容立即对其他打开同一文件的其他用户可见

  + **共享文件指针允许多用户同时读取和写入文件**

+ 会话语义
  + 写入内容只有当文件关闭时可见

+ 锁
  + 一些操作系统和文件系统提供该功能

*<small>（UNIX共享文件指针允许多用户同时读取和写入文件，所以多用户操作同一个文件时就要保证编程同步、互斥，否则可能出现后来者数据覆盖前者修改的情况。）</small>*

*<small>(会话语义，从打开文件到关闭文件，这个过程是一个会话。只有当操作文件时，执行关闭文件的操作，才会把数据从内存的缓存区域写回到硬盘的文件中去。这样接下来打开文件的进程就能读到刚被写入的数据。)</small>*

*<small>(锁，操作系统可以根据不同的情况提供不同力度的锁，比如可以设置让某些进程打开文件后，其他进程就不能打开该文件。或者更细粒的锁，比如并发的多个进程对文件写操作，只要写的文件块不是同一个，也没关系。=>根据文件不同块加锁。)</small>*

*<small>（操作系统提供不同粒度的文件锁，方便用户根据应用场景使用不同的锁进行开发。）</small>*

​	<u>数据库是"基于内容访问"的文件系统，常见MySQL等数据库也有对数据库元组加锁的指令。比如粒度上有行级锁、表级锁，而访问权限上有读锁、写锁。</u>

> [十二、文件系统](https://blog.csdn.net/Alatebloomer/article/details/80075297)

### 12.1.3 目录

+ 文件以目录的方式组织起来

- 目录是一类特殊的文件
  - 每个目录都包含了一张表<name，pointer to file header>
- 目录和文件的**树型**结构
  + 早期的文件系统是扁平的 (只有一层目录)

+ 层次名称空间

  ![img](https://img-blog.csdn.net/20180425110810806?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0FsYXRlYmxvb21lcg==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)

- 典型目录操作
  - 搜索文件
  - 创建文件
  - 删除文件
  - 枚举目录
  - 重命名文件
  - 在文件系统中遍历一个路径 
- **操作系统应该只允许内核模式修改目录**
  - 确保映射的完整性
  - 应用程序能够读目录（如ls）

---

+ 文件名的线性列表，包涵了指向数据块的指针
  + 编程简单
  + 执行耗时

- Hash表– hash数据结构的线性表

  + 减少目录搜索时间

  + 碰撞 – 两个文件名的hash值相同

  + 固定大小

---

+ 名字解析: 逻辑名字转换成物理资源（如文件）的过程

  + 在文件系统中，到实际文件的文件名（路径）

  + 遍历文件目录直到找到目标文件

+ 举例: 解析"/bin/ls"

  + 读取root的文件头（在磁盘固定位置）

  + 读取root的数据块；搜索"bin"项

  + 读取bin的文件头

  + 读取bin的数据块；搜索"ls"顶

  + 读取ls的文件头

+ 当前工作目录

  + **每个进程都会指向一个文件目录用于解析文件名**

  + 允许用户指定相对路径来代替绝对路径

*<small>（路径遍历：从目录到最后的文件。文件系统中访问的很大一部分开销，就是来自路径的遍历，因为路径的遍历可能很长，需要遍历很多层才能找到对应的文件）</small>*

*<small>(访问规则，如果是绝对路径，那就是从根目录开始访问，根目录就是一个反斜杠`/`，)</small>*

*<small>(由于系统会对当前目录缓存，所以直接从当前目录`pwd`开始访问就更快<=相对路径)</small>*

---

+ <u>一个文件系统需要先**挂载**才能被访问</u>

- 一个未挂载的文件系统被挂载在**挂载点**上

![img](https://img-blog.csdn.net/20180425113040366?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0FsYXRlYmxvb21lcg==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)![img](https://img-blog.csdn.net/20180425113106501?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0FsYXRlYmxvb21lcg==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)

*（文件系统的挂载。特别是UNIX系统，不同的文件系统需要挂载在不同的目录下，来形成一个分层次的跨文件系统访问。）*

*（挂载点，用户视角看来就是一个普通的目录。而对文件系统而言是一个根的起始位置。）*

*（挂载从用户视角看来就是简单的目录结构，而且用户可以通过简单的指令`mount`和`unmount`挂载或卸载文件系统。）*

> [十二、文件系统](https://blog.csdn.net/Alatebloomer/article/details/80075297)

### 12.1.4 文件别名

- 两个或多个文件名关联同一个文件

![img](https://img-blog.csdn.net/20180425113435297?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0FsYXRlYmxvb21lcg==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)

- 硬链接: 多个文件项指向一个文件
- 软链接: 以"快捷方式"指向其他文件
- 通过存储真实文件的逻辑名称来实现

+ 如果删除了一个有别名的文件会如何呢？
  + 如果是软链接，这个别名会成为一个"悬空指针"（NULL）
  + 如果是硬链接，文件的链接数-1（只有文件的链接数是0，文件才真正被执行删除操作。源文件默认链接数1,也就是自己）

+ Backpointers 方案：
  + 每个文件有一个包含多个backpointers的列表，所以删除所有的backpointers，该文件才真正被执行删除操作。
  + backpointers使用菊花链管理

+ 添加一个间接层：目录项数据结构  <small>（先指向间接层，间接层再指向真实的文件。很少被使用，还是引用计数实现的硬链接和文件路径实现的软连接更常用）</small>
  + 链接 - 已存在文件的另一个名字（指针）
  + 链接处理 - 跟随指针来定位文件

![img](https://img-blog.csdn.net/20180425114056492?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0FsYXRlYmxvb21lcg==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)

+ 我们如何保证没有循环？
  + **只允许到文件的链接，不允许在子目录的链接**
  + 每增加一个新的链接时都用循环检测算法确定是否合理

+ 更多实践
  + 限制路径可遍历文件目录的数量

*<small>（别名机制，可能导致原本的树状文件目录变成类似图的结构，形成环。这样可能导致查找路径时陷入一个死循环。）</small>*

> [十二、文件系统](https://blog.csdn.net/Alatebloomer/article/details/80075297)
>
> [在Linux的两种链接文件中，只能实现对文件链接的一种方式是：](https://www.nowcoder.com/questionTerminal/edc8163b5821438993a377855cc85274)
>
> Linux中是根据iNode值对文件进行操作的，iNode是区分不同文件的标志，一个原文件，其他文件和该原文件iNode值一样，文件名不一样，这些文件是原文件的硬链接
>
> 软连接是一个文件，有自己的iNode值，只是这个文件用户数据域存储的是原文件的路径指向，称为原文件的软链接 
>
> **软链接能链接目录**，链接不存在的文件，**能够跨越不同的文件系统链接文件**，他依赖于原文件而存在，当原文件被删除，软链接也就失效变为死链接hanging link，当重新建立原文件，软链接又变为有效的链接 
>
> **硬链接不能链接目录**，**不能链接不同文件系统的文件**，因为硬链接基于iNode索引值，iNode在当前文件系统下才有效，(因为不同文件系统下iNode值有可能相同)
>
> [硬链接--百度百科]([https://baike.baidu.com/item/%E7%A1%AC%E9%93%BE%E6%8E%A5/2088758?fr=aladdin](https://baike.baidu.com/item/硬链接/2088758?fr=aladdin))
>
> 1. [硬连接](https://baike.baidu.com/item/硬连接)适用于在同一个卷的文件级别，不允许给目录创建硬链接；
>
> 2. 硬连接是不能跨卷的，只有在同一文件系统中的文件之间才能创建链接。
>
> Linux 文件系统最重要的特点之一是它的文件链接。链接是对文件的引用，这样您可以让文件在文件系统中多处被看到。不过，在 **Linux 中，链接可以如同原始文件一样来对待**。链接可以与普通的文件一样被执行、编辑和访问。对系统中的其他应用程序而言，链接就是它所对应的原始文件。**当您通过链接对文件进行编辑时，您编辑的实际上是原始文件。链接不是副本**。有两种类型的链接：硬链接和[符号链接](https://baike.baidu.com/item/符号链接)（[软链接](https://baike.baidu.com/item/软链接)）。
>
> 硬链接只能引用同一文件系统中的文件。它引用的是文件在文件系统中的物理索引（也称为inode）。当您移动或删除原始文件时，硬链接不会被破坏，因为它所引用的是文件的物理数据而不是文件在文件结构中的位置。硬链接的文件不需要用户有访问原始文件的权限，也不会显示原始文件的位置，这样有助于文件的安全。如果您删除的文件有相应的硬链接，那么这个文件依然会保留，直到所有对它的引用都被删除。
>
> **与软链接的区别**
>
> 在Linux的文件系统中，保存在磁盘分区中的文件不管是什么类型都给它分配一个编号，称为[索引节点](https://baike.baidu.com/item/索引节点)号inode 。<u>软连接，其实就是新建立一个文件，这个文件就是专门用来指向别的文件的</u>（那就和windows下的快捷方式的那个文件有很接近的意味）。软链接产生的是一个新的文件，但这个文件的作用就是专门指向某个文件的，删了这个软连接文件，那就等于不需要这个连接，和原来的存在的实体原文件没有任何关系，但删除原来的文件，则相应的软连接不可用（cat那个软链接文件，则提示“没有该文件或目录“）
>
> <u>[硬连接](https://baike.baidu.com/item/硬连接)是不会建立inode的，他只是在文件原来的inode link count域再增加1而已，也因此硬链接是不可以跨越文件系统的</u>。相反都是软连接会重新建立一个inode，当然inode的结构跟其他的不一样，他只是一个指明源文件的字符串信息。一旦删除源文件，那么软连接将变得毫无意义。而硬链接删除的时候，系统调用会检查inode link count的数值，如果他大于等于1，那么inode不会被回收。因此文件的内容不会被删除。
>
> 创建硬链接实际上是为**原文件在储存器中的资源**再新建一个入口，所以硬链接和原文件指向的其实是**储存器中的同一处资源**（可以理解为硬链接和原文件就是同一个文件）。可以通过ls -i来查看一下，这两个文件的inode号是同一个，说明它们是同一个文件；而[软链接](https://baike.baidu.com/item/软链接)建立的是一个指向，即链接文件内的内容是指向原文件的指针，它们是两个文件。
>
> **软链接可以跨文件系统，硬链接不可以；软链接可以对一个不存在的文件名（filename）进行链接（当然此时如果你vi这个软链接文件，linux会自动新建一个文件名为filename的文件），硬链接不可以（其文件必须存在，inode必须存在）；软链接可以对目录进行连接，硬链接不可以。**两种链接都可以通过命令 ln 来创建。ln 默认创建的是硬链接。使用 -s 开关可以创建[软链接](https://baike.baidu.com/item/软链接)。

### 12.1.5 文件系统种类

+ 磁盘文件系统
  + 文件存储在数据存储设备上, 如磁盘
  + 例如：FAT<small>(早期Windows)</small>，NTFS<small>(现在Windows)</small>，ext2/3<small>(UNIX/LINUX)</small>，ISO9660<small>(光盘)</small>等
+ 数据库文件系统
  + 文件根据其特征是可被寻址（辨识）的
  + 例如: WinFS
+ **日志文件系统**
  + 记录文件系统的修改/事件
  + 例如：journaling file system
+ 网络/分布式文件系统
  + 例如: NFS（网络）, SMB（网络）, AFS（分布式）, GFS(google文件系统，分布式）
+ **特殊/虚拟文件系统**

*<small>（早期的传统FAT文件系统，由于计算机经常会断电，导致原本内存中还有很多数据没有写回磁盘中去，进而文件系统内的内容混乱。为此，操作系统在每次开机时会进行一次检测check，确保文件系统内容的一致性，这个开销很大。所以后来的NTFS、ext2/3/4等添加了一个日志文件系统功能，确保掉电后能快速恢复数据。其类似数据库日志，确保一个写、读操作，要么万完成要么不完成，避免中间状态被打断的情况，因为修改数据前先修改文件头的元数据信息，再写入实际的文件数据，就算写文件数据时系统突然断电，之后也可根据元数据信息发现丢失的位置，进而通知相关程序重新写数据。）</small>*

*<small>（特殊/虚拟文件系统，其目的通常不是单纯的存数据，而是以文件的形式提供读or写的接口来访问内核中的一些数据。比如我想知道操作系统当前运行多少个进程，占用多少CPU等，Linux中有专门的文件系统的目录叫作proc文件系统。只要进入该目录，就可以看到丰富的内核层面的信息。这些看上去和文件每关系，但是操作系统根据这种类似文件的方式展示内核信息，让我们可以更方便的读or写一些参数来管理管操作系统。）</small>*

---

网络/分布式文件系统

+ 文件可以通过网络被共享
  + 文件位于远程服务器
  + 客户端远程挂载服务器文件系统
  + 标准系统文件访问被转换成远程访问
  + 标准文件共享协议：NFS for Unix，CIFS for Windows

+ 分布式文件系统的挑战
  + 客户端和客户端上的用户辨别起来很复杂
  + 例如, NFS是不安全的
  + 一致性问题
  + 错误处理模式

> [Linux 日志文件系统剖析](https://www.ibm.com/developerworks/cn/linux/l-journaling-filesystems/)
>
> 日志文件系统是使用日志来缓冲文件系统的修改（同时也可以应用于紧急故障恢复）的，但可以根据记录的时间与内容采取不同的策略。其中，**三种常见的策略为：回写（writeback）、预定（ordered）和数据（data）**。
>
> 在*回写模式* 中，仅有元数据被记录到日志，数据块则被直接写入到磁盘位置上。这样可以保存文件系统结构，防止崩溃，但却有可能发生数据崩溃（比如：在元数据记录到日志后，数据块写入磁盘前，系统崩溃）。要想解决这个问题，您可以使用预定模式。*预定模式* 只将元数据记录到日志，但是在此之前将数据写入到磁盘。这样就可以保证系统恢复后数据和文件系统的一致性。最后一种模式将数据也记录到了日志中。在*数据模式* 中，元数据和数据都被记录到日志中。这种模式可以最大限度地防止文件系统崩溃与数据丢失，但由于全部数据都写入了两次（先写入日志，再写入磁盘），系统性能可能会降低。
>
> 日志的提交也有很多种不同的策略。比如，是在日志将满时，还是在超时后？
>
> ......（下面还有具体的各种文件系统的介绍，文章2008年的，较早但优质，建议自己看看）
>
> [日志式文件系统--百度百科]([https://baike.baidu.com/item/%E6%97%A5%E5%BF%97%E5%BC%8F%E6%96%87%E4%BB%B6%E7%B3%BB%E7%BB%9F/799374?fromtitle=%E6%97%A5%E5%BF%97%E6%96%87%E4%BB%B6%E7%B3%BB%E7%BB%9F&fromid=16769925&fr=aladdin](https://baike.baidu.com/item/日志式文件系统/799374?fromtitle=日志文件系统&fromid=16769925&fr=aladdin))
>
> 日志文件系统比传统的文件系统安全，因为它用独立的日志文件跟踪磁盘内容的变化。
>
> 日志文件系统（journaling file system）是一个具有故障恢复能力的文件系统，在这个文件系统中，因为对目录以及位图的更新信息总是在原始的磁盘日志被更新之前写到磁盘上的一个连续的日志上，所以它保证了数据的完整性。当发生系统错误时，一个全日志文件系统将会保证磁盘上的数据恢复到发生系统崩溃前的状态。同时，它还将覆盖未保存的数据，并将其存在如果计算机没有崩溃的话这些数据可能已经遗失的位置，这是对关键业务应用来说的一个很重要的特性。
>
> 尽管[Linux](https://baike.baidu.com/item/Linux/27050)可以支持种类繁多的文件系统，但是**几乎所有的现代Linux发行版都用[ext4](https://baike.baidu.com/item/ext4/1858450)作为默认的文件系统**。<small>（看了下，至少我租的服务器CentOS确实用的ext4文件系统）</small>
>
> <small>ext4的设计者主要考虑的是文件系统性能方面的问题。ext4在写入文件内容的同时并没有同时写入文件的meta-data（和文件有关的信息，例如：[权限](https://baike.baidu.com/item/权限)、[所有者](https://baike.baidu.com/item/所有者)以及创建和访问时间）。换句话说，Linux先写入文件的内容，然后等到有空的时候才写入文件的meta-data。这样若出现写入文件内容之后但在写入文件的meta-data之前系统突然断电，就可能造成在文件系统就会处于不一致的状态。在一个有大量文件操作的系统中出现这种情况会导致很严重的后果。</small>
>
> **日志文件系统比传统的文件系统安全，因为它用独立的日志文件跟踪磁盘内容的变化。就像关系型数据库（[RDBMS](https://baike.baidu.com/item/RDBMS/1048260)），日志文件系统可以用[事务处理](https://baike.baidu.com/item/事务处理/217482)的方式，提交或撤消文件系统的变化**。
>
> 在分区中保存有一个日志[记录文件](https://baike.baidu.com/item/记录文件)，文件系统写操作首先是对记录文件进行操作，若整个写操作由于某种原因(如系统掉电)而中断，则在下次系统启动时就会读日志记录文件的内容来恢复没有完成的写操作，这个过程一般只需要几秒钟。

### 12.1.6 Linux文件锁(优质文章推荐)

> [小何讲Linux： 文件锁及其实例 ](https://blog.csdn.net/rl529014/article/details/51336161) <=内容较多，包含代码。这里只截取部分内容

​	Linux中软件、硬件资源都是文件（一切皆文件），文件在多用户环境中是可共享的。

​	文件锁是用于解决资源的共享使用的一种机制：当多个用户需要共享一个文件时，Linux通常采用的方法是给文件上锁，来避免共享的资源产生竞争的状态。

文件锁包括**建议性锁**和**强制性锁**：

+ 建议性锁：要求每个使用上锁文件的进程都要检查是否有锁存在，并且尊重已有的锁。在**一般情况下，内核和系统都不使用建议性锁，它们依靠程序员遵守这个规定**。
+ 强制性锁：是由内核执行的锁，当一个文件被上锁进行写入操作的时候，内核将阻止其他任何文件对其进行读写操作。采用强制性锁对性能的影响很大，每次读写操作都必须检查是否有锁存在。

在Linux中，实现文件上锁的函数有`lockf()`和`fcntl()`

+ `lockf()`用于对文件施加建议性锁

+ `fcntl()`不仅可以施加建议性锁，还可以施加强制锁。

+ `fcntl()`还能对文件的某一记录上锁，也就是记录锁。

+ 记录锁又可分为读取锁和写入锁，其中读取锁又称为**共享锁**，它能够使多个进程都能在文件的同一部分建立读取锁。

+ 写入锁又称为**排斥锁**，在任何时刻只能有一个进程在文件的某个部分建立写入锁。

+ 在文件的同一部分不能同时建立读取锁和写入。

其余代码部分，建议直接阅读[原文](https://blog.csdn.net/rl529014/article/details/51336161) (如果需要看代码的话)



> [Linux以下的两种文件锁](https://www.cnblogs.com/yjbjingcha/p/6932162.html)
>
> 下面是Linux系统中两种经常使用的文件锁：
>
> 1、 协同锁
>
> 协同锁要求參与操作的进程之间协同合作。
>
> 如果进程“A”获得一个WRITE锁，并開始向文件里写入内容；此时，进程“B”并没有试图获取一个锁，它仍然能够打开文件并向文件里写入内容。
>
> 在此过程中，进程“B”就是一个非合作进程。如果进程“B”试图获取一个锁，那么整个过程就是一个合作的过程，从而能够保证操作的“序列化”。
>
> 仅仅有当參与操作的进程是协同合作的时候，协同锁才干发挥作用。协同锁有时也被称为“非强制”锁。
>
> 2、 强制锁
>
> 强制锁不须要參与操作的进程之间保持协同合作。它利用内核来查检每一个打开、读取、写入操作，从而保证在调用这些操作时不违反文件上的锁规则。关于强制锁的很多其它信息，能够在[kernal.org](http://kernel.org/doc/Documentation/filesystems/mandatory-locking.txt)上找到。
>
> 为了使能Linux中的强制锁功能。你须要在文件系统级别上打开它。同一时候在单个文件上打开它。其步骤是：
>
> 1、 挂载文件系统时使用“-o mand”參数。
>
> 2、 对于要打开强制锁功能的文件lock_file。必须打开set-group-ID位。关闭group-execute位。
>
> （选择此方法的原因是，当你关闭group-execute时，设置set-group-ID就没有实际的意义了）
>
> [文件锁的本质核心和原理](https://blog.csdn.net/junwua/article/details/80576433) <=== 图文并茂，建议阅读。
>
> 进程1对同一个文件打开了两次，分别对应本进程中的文件描述符 fd0 和 fd2。而下面的进程对这个文件又打开了一次，对应此进程中的 fd1描述符。要注意的是，**不论是同一进程还是不同的进程，对同一文件打开时，都建立了与各`fd`对应的独立的文件表项。**
>
> **用 dup 复制文件描述符时，新的文件描述符和旧的文件描述符共享同一个文件表表项**：
>
> 调用 dup 后，两个描述符指向了相同的文件表项，而flock 的文件锁是加在了文件表项上，因而如果对 fd0 加锁那么 fd1 就会自动持有**同一把锁，释放锁时，可以使用这两个描述符中的任意一个**。

## 12.2 虚拟文件系统

### 12.2.1 虚拟文件系统概述

*（虚拟文件系统，前面我们知道不同的文件系统差别很大，如果要应用程序针对不同的文件系统去写不同版本的程序，那显然费时又费力。操作系统把复杂文件系统抽象出一层，虚拟文件系统层，其向上层"文件/文件系统API"提供统一的接口。用户只要访问"文件/文件系统API"，就可一套代码适用于好几套文件系统。）*

*（虚拟文件系统层，屏蔽了具体文件系统底层的差异性。我们写一套程序能访问不同文件系统，无需关系操作的是什么文件系统。之所以如此，因为底层不同的文件系统，都按照虚拟文件系统层提出的API接口规范进行对应的实现。<small>换言之，不同文件系统需要的不同代码实现，这工作由底层文件系统开发者替我们做了，使得我们上层开发调用文件系统API，就无需再关系底层差异。</small>）*

+ 分层结构
  + 上层：虚拟（逻辑）文件系统
  + 底层：特定文件系统模块

  ![img](https://img-blog.csdn.net/20180425153707305?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0FsYXRlYmxvb21lcg==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)

  ​	如图可以看出，虚拟系统层适用面很广，示例中除了普通磁盘的Device I/O，连网卡的Network I/O也适用。虽然它们之间底层文件系统实现大不相同，但是通过虚拟文件系统层，统一向上层提供了一致的API接口（指open、write、read等）。

+ 目的
  
+ **对所有不同文件系统的抽象**
  
+ 功能 
  + 提供相同的文件和文件系统**接口** 
  + 管理所有文件和文件系统关联的**数据结构**
  +  高效查询**例程**，遍历文件系统 
  + 与特定文件系统模块的**交互**

*虚拟文件系统层，存在于内存，而不是硬盘上。开机，BIOS->BootLoader->OS，然后OS又会启动很多系统管理进程/线程。文件系统、虚拟文件系统等子系统就是由OS启动的。*

抛开每个文件系统的具体差异，通常文件系统基本的构成部分，包括"卷控制块"、"文件控制块"、"目录结点"。

- 卷控制块（Unix："superblock" ）     <= <small>表现一些文件系统级的特征。</small>
  - 每个文件系统一个
  - 文件系统详细信息
  - 块，块大小，空余块，计数/指针等 
- 文件控制块（Unix："`vnode`" or "`inode`"）
  -  每个文件一个
  - 文件详细信息
  - 许可、拥有者、大小、数据库位置等
- 目录节点（Linux："dentry"）     <= <small>目录的内容，存放该目录下存在的各种目录、文件的元数据信息，而不是直接的文件数据。</small>
  - 每个目录项一个（目录和文件）
  - 将目录项数据结构及树性布局编码成树型数据结构
  - 指向文件控制块、父节点、项目列表等

![img](https://img-blog.csdn.net/20180425154746563?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0FsYXRlYmxvb21lcg==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)

+ 文件系统数据结构

  + **卷控制块（每个文件系统一个）**
  + 文件控制块（每个文件一个）
  + 目录节点（每个目录项一个）

  *<small>（这些都是存放在磁盘中的）</small>*

+ 持续存储在二级存储中
  
+ 分配在存储设备的数据块中
  
+ **当需要时加载进内存**
  + **卷控制块：当文件系统挂载时进入内存**
  + 文件控制块：当文件被访问时进入内存
  + **目录节点： 在遍历一个文件路径时进入内存**

![img](https://img-blog.csdn.net/20180425155447703?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0FsYXRlYmxvb21lcg==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)

> [十二、文件系统](https://blog.csdn.net/Alatebloomer/article/details/80075297)
>
> [虚拟文件系统--百度百科]([https://baike.baidu.com/item/%E8%99%9A%E6%8B%9F%E6%96%87%E4%BB%B6%E7%B3%BB%E7%BB%9F/10986803?fr=aladdin](https://baike.baidu.com/item/虚拟文件系统/10986803?fr=aladdin))
>
> 虚拟文件系统(VFS)是由Sun microsystems公司在定义[网络文件系统](https://baike.baidu.com/item/网络文件系统/9719420)(NFS)时创造的。它是一种用于网络环境的[分布式文件系统](https://baike.baidu.com/item/分布式文件系统/1250388)，是允许和操作系统使用不同的文件系统实现的接口。<u>虚拟文件系统（VFS）是物理文件系统与服务之间的一个接口层，它对Linux的每个文件系统的所有细节进行抽象，使得不同的文件系统在[Linux](https://baike.baidu.com/item/Linux)核心以及系统中运行的其他进程看来，都是相同的</u>。**严格说来，VFS并不是一种实际的[文件系统](https://baike.baidu.com/item/文件系统/4827215)。它只存在于[内存](https://baike.baidu.com/item/内存/103614)中，不存在于任何外存空间。VFS在系统启动时建立，在系统关闭时消亡。**
>
> VFS的使用者是进程(用户访问文件系统总是需要启动进程)。 描述进程的`task_struct`结构中`files`指针指向了一个`files_struct`结构, 后者**描述了进程已打开的文件集合**。 
>
> `files_struct`结构维护了一个已打开文件所对应的`file`结构的指针数组，数组下标被用作用户程序操作**已打开文件的句柄**(通常称作`fd`)。`files_struct`还维护着已使用的`fd`位图, 以便在需要打开文件时, 为其分配一个未使用的`fd`。
>
> [Linux 的虚拟文件系统(强烈推荐)](https://www.cnblogs.com/feng9exe/p/8383950.html) <= 特别详细的一篇文章。<small>建议如果不是很熟悉的先大致看看即可(因为要是没有一定基础，多看了过几天一样忘)</small>

### 12.2.2 LVM 逻辑卷管理

> [LVM管理](https://www.cnblogs.com/diantong/p/10554831.html)
>
> [Linux LVM磁盘分区管理](https://www.cnblogs.com/vincenshen/p/8313307.html)
>
> [Linux LVM卷组管理](https://www.cnblogs.com/xiangsikai/p/10684828.html)
>
> [逻辑卷增加，扩容，缩小，删除操作步骤](https://blog.csdn.net/j_ychen/article/details/79404197)	<=	详细的操作 + 截图
>
> [Logical Volume Manager (Linux)](https://en.wikipedia.org/wiki/Logical_Volume_Manager_(Linux))

#### 1. Logical Volume Manager (Linux)

> [Logical Volume Manager (Linux)](https://en.wikipedia.org/wiki/Logical_Volume_Manager_(Linux))

##### 1.0 概述

​	In [Linux](https://en.wikipedia.org/wiki/Linux), **Logical Volume Manager** (**LVM**) is a [device mapper](https://en.wikipedia.org/wiki/Device_mapper) framework that provides [logical volume management](https://en.wikipedia.org/wiki/Logical_volume_management) for the [Linux kernel](https://en.wikipedia.org/wiki/Linux_kernel). Most modern [Linux distributions](https://en.wikipedia.org/wiki/Linux_distribution) are LVM-aware to the point of being able to have their [root file systems](https://en.wikipedia.org/wiki/Root_file_system) on a [logical volume](https://en.wikipedia.org/wiki/Logical_volume).[[3\]](https://en.wikipedia.org/wiki/Logical_Volume_Manager_(Linux)#cite_note-3)[[4\]](https://en.wikipedia.org/wiki/Logical_Volume_Manager_(Linux)#cite_note-auto1-4)[[5\]](https://en.wikipedia.org/wiki/Logical_Volume_Manager_(Linux)#cite_note-5)

​	<small>在Linux中，Logical Volume Manager（LVM）是一个设备映射器框架，它为Linux内核提供逻辑卷管理。大多数现代Linux发行版都支持LVM，（我们）能够将根文件系统放在逻辑卷上。</small>

​	<small>Heinz Mauelshagen wrote the original LVM code in 1998, when he was working at [Sistina Software](https://en.wikipedia.org/wiki/Sistina_Software), taking its primary design guidelines from the [HP-UX](https://en.wikipedia.org/wiki/HP-UX)'s volume manager.[[1\]](https://en.wikipedia.org/wiki/Logical_Volume_Manager_(Linux)#cite_note-auto-1)</small>

##### 1.1 Uses用途

​	LVM is used for the following purposes:

- Creating single [logical volumes](https://en.wikipedia.org/wiki/Logical_volume) of multiple physical volumes or entire hard disks (somewhat similar to [RAID 0](https://en.wikipedia.org/wiki/RAID_0), but more similar to [JBOD](https://en.wikipedia.org/wiki/JBOD)), **allowing for dynamic volume resizing**.

- **Managing large hard disk farms by allowing disks to be added and replaced without downtime or service disruption, in combination with [hot swapping](https://en.wikipedia.org/wiki/Hot_swapping).**

- On small systems (like a desktop), instead of having to estimate at installation time how big a partition might need to be, LVM allows filesystems to be easily resized as needed.

  *<small>（xfs文件系统一般只能增加空间，不能减少，减少则需要格式化后重新挂载！）</small>*

- **Performing consistent backups by taking snapshots of the logical volumes.**

  <small>可通过拍摄逻辑卷快照来保证一致性备份</small>

- Encrypting multiple physical partitions with one password.

​	<u>LVM can be considered as a thin software layer on top of the hard disks and partitions, which creates an abstraction of continuity and ease-of-use for managing hard drive replacement, repartitioning and backup.</u>

##### 1.2 Features特点

![File:Lvm.svg](https://upload.wikimedia.org/wikipedia/commons/thumb/e/e6/Lvm.svg/596px-Lvm.svg.png)

###### 1.2.1 Basic functionality 基础功能

- Volume groups (VGs) can be **resized online** by absorbing new [physical volumes](https://en.wikipedia.org/wiki/Physical_volume) (PVs) or ejecting existing ones.

- Logical volumes (LVs) can be **resized online** by concatenating [extents](https://en.wikipedia.org/wiki/Extent_(file_systems)) onto them or truncating extents from them.

  <small>逻辑卷（LV）可以通过连接扩展数据块或截断逻辑卷的扩展数据块来在线调整大小。</small>

- **LVs can be moved between PVs**.

- Creation of **read-only** [snapshots](https://en.wikipedia.org/wiki/Snapshot_(computer_storage)) of logical volumes (**LVM1**), leveraging a copy on write (CoW) feature[[6\]](https://en.wikipedia.org/wiki/Logical_Volume_Manager_(Linux)#cite_note-6), or **read/write** snapshots (**LVM2**)

  <small>创建逻辑卷（LVM1）的只读快照，利用写入时拷贝（CoW）功能[6]，或读/写快照（LVM2）</small>

- <u>VGs can be split or merged *in situ* as long as no LVs span the split.</u> This can be useful when migrating whole LVs to or from offline storage.

  <small>VG可以就地拆分或合并，只要没有LV跨越分割。这在将整个lv迁移到离线存储或从离线存储迁移时非常有用。</small>

- LVM objects can be tagged for administrative convenience.[[7\]](https://en.wikipedia.org/wiki/Logical_Volume_Manager_(Linux)#cite_note-7)

- **VGs and LVs can be made active as the underlying devices become available through use of the `lvmetad` daemon.**[[8\]](https://en.wikipedia.org/wiki/Logical_Volume_Manager_(Linux)#cite_note-8)

  <small>通过使用lvmetad守护程序，可以在基础设备可用时激活VG和LV</small>

###### 1.2.2 Advanced functionality 高级功能

+ **[Hybrid volumes](https://en.wikipedia.org/wiki/Hybrid_volume) can be created using the [dm-cache](https://en.wikipedia.org/wiki/Dm-cache) target, which allows one or more fast storage devices, such as flash-based [SSDs](https://en.wikipedia.org/wiki/Solid-state_drive), to act as a [cache](https://en.wikipedia.org/wiki/Cache_(computing)) for one or more slower [hard disk drives](https://en.wikipedia.org/wiki/Hard_disk_drive).**[[9\]](https://en.wikipedia.org/wiki/Logical_Volume_Manager_(Linux)#cite_note-9)

  <small>可以使用dm-cache目标创建混合卷，该目标允许一个或多个快速存储设备（例如基于闪存的SSD）充当一个或多个速度较慢的硬盘驱动器的缓存。</small>

+ Thinly provisioned LVs can be allocated from a pool.[[10\]](https://en.wikipedia.org/wiki/Logical_Volume_Manager_(Linux)#cite_note-10)

+ On newer versions of [device mapper](https://en.wikipedia.org/wiki/Device_mapper), LVM is integrated with the rest of device mapper enough to ignore the individual paths that back a dm-multipath device if `devices/multipath_component_detection=1` is set in `lvm.conf`. This prevents LVM from activating volumes on an individual path instead of the multipath device.[[11\]](https://en.wikipedia.org/wiki/Logical_Volume_Manager_(Linux)#cite_note-11)

###### 1.2.3 RAID

+ **LVs can be created to include [RAID](https://en.wikipedia.org/wiki/RAID) functionality, including [RAID 1](https://en.wikipedia.org/wiki/RAID_1), [5](https://en.wikipedia.org/wiki/RAID_5) and [6](https://en.wikipedia.org/wiki/RAID_6).[[12\]](https://en.wikipedia.org/wiki/Logical_Volume_Manager_(Linux)#cite_note-12)**

+ Entire LVs or their parts can be striped across multiple PVs, similarly to [RAID 0](https://en.wikipedia.org/wiki/RAID_0).

+ A RAID 1 backend device (a PV) can be configured as "write-mostly", resulting in reads being avoided to such devices unless necessary.[[13\]](https://en.wikipedia.org/wiki/Logical_Volume_Manager_(Linux)#cite_note-13)

  <small>raid1后端设备（PV）可以配置为“以写为主”，从而避免了对这些设备的读取，除非有必要。</small>

+ Recovery rate can be limited using `lvchange --raidmaxrecoveryrate` and `lvchange --raidminrecoveryrate` to maintain acceptable I/O performance while rebuilding a LV that includes RAID functionality.

###### 1.2.4 High availability

​	**<u>The LVM also works in a shared-storage [cluster](https://en.wikipedia.org/wiki/Computer_cluster) in which disks holding the PVs are shared between multiple host computers, but can require an additional daemon to mediate metadata access via a form of locking.</u>**

​	<small>LVM还可以在共享存储集群中工作，在该集群中，保存pv的磁盘在多台主机之间共享，但可能需要一个额外的守护进程通过锁定的形式来协调元数据访问。</small>

1. CLVM

   A [distributed lock manager](https://en.wikipedia.org/wiki/Distributed_lock_manager) is used to broker concurrent LVM metadata accesses. Whenever a cluster node needs to modify the LVM metadata, it must secure permission from its local `clvmd`, which is in constant contact with other `clvmd` daemons in the cluster and can communicate a desire to get a lock on a particular set of objects.

   <small>分布式锁管理器用于代理并发LVM元数据访问。每当集群节点需要修改LVM元数据时，它必须确保来自其本地clvmd的权限，该本地clvmd与集群中的其他clvmd守护进程保持联系，并且可以传递获取特定对象集锁的愿望。</small>

2. HA-LVM

   Cluster-awareness is left to the application providing the high availability function. For the LVM's part, HA-LVM can use CLVM as a locking mechanism, or can continue to use the default file locking and reduce "collisions" by restricting access to only those LVM objects that have appropriate tags. Since this simpler solution avoids contention rather than mitigating it, no concurrent accesses are allowed, so HA-LVM is considered useful only in active-passive configurations.

3. **lvmlockd**

   As of 2017, a stable LVM component that is designed to replace `clvmd` by making the locking of LVM objects transparent to the rest of LVM, without relying on a distributed lock manager.[[14\]](https://en.wikipedia.org/wiki/Logical_Volume_Manager_(Linux)#cite_note-14) It saw massive development during 2016.

​	The above described mechanisms only resolve the issues with LVM's access to the storage. The file system selected to be on top of such LVs must either support clustering by itself (such as [GFS2](https://en.wikipedia.org/wiki/GFS2) or [VxFS](https://en.wikipedia.org/wiki/Veritas_File_System)) or it must only be mounted by a single cluster node at any time (such as in an active-passive configuration).

###### 1.2.5 Volume group allocation policy

​	<u>LVM VGs must contain a default allocation policy for new volumes created from it</u>. This can later be changed for each LV using the `lvconvert -A` command, or on the VG itself via `vgchange --alloc`. To minimize fragmentation, LVM will attempt the strictest policy (contiguous) first and then progress toward the most liberal policy defined for the LVM object until allocation finally succeeds.

​	In RAID configurations, almost all policies are applied to each leg in isolation. For example, even if a LV has a policy of *cling*, expanding the file system will not result in LVM using a PV if it is already used by one of the other legs in the RAID setup. LVs with RAID functionality will put each leg on different PVs, making the other PVs unavailable to any other given leg. If this was the only option available, expansion of the LV would fail. In this sense, the logic behind *cling* will only apply to expanding each of the individual legs of the array.

Available allocation policies are:

- *Contiguous* - forces all [LEs](https://en.wikipedia.org/wiki/Logical_extent) in a given LV to be adjacent and ordered. This eliminates fragmentation but severely reduces a LV expandability.
- *Cling* - forces new LEs to be allocated only on PVs already used by an LV. This can help mitigate fragmentation as well as reduce vulnerability of particular LVs should a device go down, by reducing the likelihood that other LVs also have extents on that PV.
- *Normal* - implies near-indiscriminate selection of PEs, but it will attempt to keep parallel legs (such as those of a RAID setup) from sharing a physical device.
- *Anywhere* - imposes no restrictions whatsoever. Highly risky in a RAID setup as it ignores isolation requirements, undercutting most of the benefits of RAID. For linear volumes, it can result in increased fragmentation.

##### 1.3 Implementation实现

![img](https://upload.wikimedia.org/wikipedia/commons/0/0d/Example_LVM_head.png)

![File:LVM1.svg](https://upload.wikimedia.org/wikipedia/commons/thumb/b/ba/LVM1.svg/500px-LVM1.svg.png)

​	<u>Typically, the first megabyte of each physical volume contains a mostly [ASCII](https://en.wikipedia.org/wiki/ASCII)-encoded structure referred to as an "LVM header" or "LVM head"</u>. Originally, the LVM head used to be written in the first and last megabyte of each PV for redundancy (in case of a partial hardware failure); however, this was later changed to only the first megabyte. <u>Each PV's header is a complete copy of the entire volume group's layout, including the UUIDs of all other PVs and of LVs, and allocation map of [PEs](https://en.wikipedia.org/wiki/Physical_extent) to [LEs](https://en.wikipedia.org/wiki/Logical_extent). This simplifies data recovery if a PV is lost.</u>

​	In the 2.6-series of the Linux Kernel, the LVM is implemented in terms of the [device mapper](https://en.wikipedia.org/wiki/Device_mapper), a simple block-level scheme for creating virtual block devices and mapping their contents onto other block devices. This minimizes the amount of relatively hard-to-debug kernel code needed to implement the LVM. It also allows its I/O redirection services to be shared with other volume managers (such as [EVMS](https://en.wikipedia.org/wiki/Enterprise_Volume_Management_System)). Any LVM-specific code is pushed out into its user-space tools, which merely manipulate these mappings and reconstruct their state from on-disk metadata upon each invocation.

To bring a volume group online, the "vgchange" tool:

1. Searches for PVs in all available block devices.
2. Parses the metadata header in each PV found.
3. Computes the layouts of all visible volume groups.
4. Loops over each logical volume in the volume group to be brought online and:
   1. **Checks if the logical volume to be brought online has all its PVs visible.**
   2. Creates a new, empty device mapping.
   3. Maps it (with the "linear" target) onto the data areas of the PVs the logical volume belongs to.

To move an online logical volume between PVs on the same Volume Group, use the "pvmove" tool:

1. Creates a new, empty device mapping for the destination.
2. Applies the "mirror" target to the original and destination maps. The kernel will start the mirror in "degraded" mode and begin copying data from the original to the destination to bring it into sync.
3. Replaces the original mapping with the destination when the mirror comes into sync, then destroys the original.

**These device mapper operations take place transparently, without applications or file systems being aware that their underlying storage is moving.**

##### 1.4 Caveats警告

- Until Linux kernel 2.6.31,[[16\]](https://en.wikipedia.org/wiki/Logical_Volume_Manager_(Linux)#cite_note-16) [write barriers](https://en.wikipedia.org/wiki/Write_barrier) were not supported (fully supported in 2.6.33). This means that the guarantee against filesystem corruption offered by [journaled file systems](https://en.wikipedia.org/wiki/Journaled_file_system) like [ext3](https://en.wikipedia.org/wiki/Ext3) and [XFS](https://en.wikipedia.org/wiki/XFS) was negated under some circumstances.[[17\]](https://en.wikipedia.org/wiki/Logical_Volume_Manager_(Linux)#cite_note-17)
- As of 2015, no online or offline defragmentation program exists for LVM. This is somewhat mitigated by fragmentation only happening if a volume is expanded and by applying the above-mentioned allocation policies. Fragmentation still occurs, however, and if it is to be reduced, non-contiguous extents must be identified and manually rearranged using the `pvmove` command.[[18\]](https://en.wikipedia.org/wiki/Logical_Volume_Manager_(Linux)#cite_note-18)
- **On most LVM setups, only one copy of the LVM head is saved to each PV, which can make the volumes more susceptible to failed disk sectors. This behavior can be overridden using `vgconvert --pvmetadatacopies`. If the LVM can not read a proper header using the first copy, it will check the end of the volume for a backup header. Most Linux distributions keep a running backup in `/etc/lvm/backup`, which enables manual rewriting of a corrupted LVM head using the `vgcfgrestore` command.**

### 12.2.3 proc虚拟文件系统(优质网文转载)

> [Linux-proc文件系统介绍](https://www.cnblogs.com/jiangtongxue/p/11212175.html) <= Linux系统，我们经常需要这个proc虚拟文件系统去查看内核信息、硬件信息、进程运行状态信息等。
>
> 以下内容皆出自上诉文章，支持原创，建议阅读原文。这里我图个方便，就在这里复制内容过来了。

1. 操作系统级别的调试
   1. 简单程序单步调试

   2. 复杂程序printf打印信息调试
   3. 框架体系日志记录信息调试

   4. 内核调试的困境

2. proc虚拟文件系统的工作原理
   1. Linux内核是一个非常庞大、复杂的一个单独的程序，对于这样的一个程序来说，调试是非常的复杂的。
   2. 像kernel这样庞大的项目，给里边添加/更改一个功能是非常麻烦的，因为你这个添加的新功能可能会影响其他已经有的功能。
   3. 早期内核版本中尽管调试很麻烦，但是高手们还可以凭借自己超凡脱俗的能力去驾驭。但是到了2.4左右的版本的时候，这个难度就非常大了。
   4. 为了降低内核调试和学习的难度，内核开发者们在内核中添加了一些属性专门用于调试内核，proc文件系统就是一个尝试。
   5. **proc文件系统的思路就是：在内核中构建一个虚拟文件系统/proc，内核运行时将内核中一些关键的数据结构以文件的方式呈现在/proc目录中的一些特定文件中，这样相当于将不可见的内核中的数据结构以可视化的方式呈现给内核的开发者**。
   6. proc文件系统给了开发者一种调试内核的方法：我们通过实时的观察/proc/xxx文件，来观看内核中特定数据结构的值，在我们添加一个新功能的前后来对比，就可以知道这个新功能产生的影响对不对了。
7. proc目录下的文件大小都是0，因为这些文件本身并不存在于硬盘中，他也不是一个真实文件，他只是一个接口，当我们去读取这个文件时，其实内核并不是去硬盘上找这个文件，而是映射为内核内部一个数据结构被读取并且格式化为字符串返回给我们。所以**尽管我们看到的还是一个文件内容字符串，和普通文件一样的；但是实际上我们知道这个内容是实时的从内核中数据结构来的，而不是从硬盘中来的。**

## 12.3 数据块缓存

### 12.3.1 数据块缓存概述

​	因为磁盘读写比内存读写慢好几个数量级，所以进程需要读写硬盘数据时，那么内存中开辟一块区域存放磁盘读写的文件数据，对应的内存区域就是缓存，这叫做buffer数据缓存机制。进程访问的文件数据其实都来自内存的缓存区，而不是直接与硬盘交互，这样提高CPU获取数据的效率。

![img](https://img-blog.csdn.net/20180425160826830?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0FsYXRlYmxvb21lcg==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)

+ 数据块按需读入内存
  + 提供read()操作
  + <u>预读：预先读取后面的数据块</u>    <= <small>数据缓存需要考虑一些读写、缓存策略</small>

+ 数据块使用后被缓存
  + 假设数据将会再次被使用
  + 写操作可能被缓存和延迟写入

+ 两种数据块缓存方式
  + 普通数据块缓存
  + 页缓存：统一缓存数据块和内存页

![img](https://img-blog.csdn.net/20180425163633939?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0FsYXRlYmxvb21lcg==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)

​	把文件缓存到内存后，考虑让CPU访问数据时，就像平时访问内存的页一样，不需要再经过虚拟文件系统层那类的系统调用API。*（也就是CPU对缓存到内存的文件读写，就像平时读写普通内存数据一样操作。等文件关闭，需要写数据回磁盘时，再经过虚拟文件系统层。）*

- 分页要求
  - 当需要一个页才将其载入内存
- 支持存储
  - 一个页（在虚拟地址空间中）可以被映射到一个本地文件中（在二级存储中）



![img](https://img-blog.csdn.net/20180425163031188?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0FsYXRlYmxvb21lcg==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)

- **文件数据块的页缓存**

  - **在虚拟内存中文件数据块被映射成页**

  + 文件的读/写操作被转换成对内存的访问

  + 可能导致缺页和/或设置为脏页

  + 问题: 页置换 - 从进程或文件页缓存中？

![img](https://img-blog.csdn.net/20180425163659548?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0FsYXRlYmxvb21lcg==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)

​	*（如何缓存，缓存的页管理，实现上类似前面的页管理算法。经常访问的页就放到内存中，不经常访问的页就先移除缓存。这里不再展开介绍。页缓存机制，同样需要通过一系列算法机制，保证磁盘文件的访问效率更高，尽量减少直接对硬盘的读写次数。）*

> [十二、文件系统](https://blog.csdn.net/Alatebloomer/article/details/80075297)

### 12.3.2 内存映射文件

> [内存映射文件原理](https://blog.csdn.net/mengxingyuanlove/article/details/50986092) <= 图文，建议阅读
>
> [内存映射文件](https://www.cnblogs.com/hanhandaren/p/11146811.html) <= 图片出处

![img](https://img2018.cnblogs.com/blog/1695681/201907/1695681-20190707152532062-1490966549.png)

![img](https://img2018.cnblogs.com/blog/1695681/201907/1695681-20190707155942646-716228852.png)

​	内存映射文件，本质就是将某一片文件"块"加载到内存的某一片"虚拟页"。之后该虚拟页对应的实际物理内存再通过其他虚拟页分配给多个进程。

​	这样咋一看上去似乎和普通的文件操作没有区别，但是回顾内存用户态和内核态，以及系统IO调度，便可以得出以下分析结果：

+ 传统文件操作：

  + **系统调用**申请IO文件读写，文件加载到**内核态内存**，又从**内核态复制到用户态内存**（用户空间）。

  + 经历**两次**复制。

+ 内存映射文件：

  + **系统调用**获取文件句柄，**<u>不需要立即加载文件</u>**，将**磁盘文件块映射到用户态的虚拟页**，文件加载到物理内存（逻辑地址处于用户态的范围）
  + 经历**一次**复制。

很明显，读写内存映射文件，效率会比传统的文件IO操作来的快。

注意：进程读写内存映射文件时，实际也是**只面向内存操作**。

（这点和传统文件IO操作一样，只要不触发清空内存文件缓存并写回磁盘的函数，就不会真正影响磁盘文件。因为读写前，进程会拷贝内存中的文件数据到自己独立的物理内存中，只在拷贝/缓存的数据上操作。）

## 12.4 打开文件的数据结构

### 12.4.1 打开文件的数据结构-概述

​	打开文件，首先要知道文件的位置（路径）。再者，所谓的打开文件，其实就是把文件控制块的元数据信息加载到内存中。相关的关键信息放入"打开文件表"里，然后对应的索引，即文件描述符，返回给进程用于文件操作。

​	*之后用文件描述符，就能读写文件，因为通过"打开文件表"能找到对应的磁盘文件位置等信息，先写入内存的文件缓存区域，之后操作系统在我们关闭文件后，将内存缓冲区的数据再写回磁盘。*

+ 打开文件描述符
  + **每个被打开的文件一个**

  + 文件状态信息
  + 目录项、当前文件指针、文件操作设置等

+ 打开文件表
  + **一个进程一个**
  + **一个系统级的**
  + 每个卷控制块也会保存一个列表
  + 所以如果有文件被打开将不能被卸载

  ![img](https://img-blog.csdn.net/20180425164720287?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0FsYXRlYmxvb21lcg==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)
   *<small>（我们对文件描述符进行读写操作，操作系统根据"系统打开文件表"上的索引（文件描述符）查找到对应的文件结点具体是编号多少的扇区被进行读写操作。操作系统把文件数据加载到内存缓冲区中，应用程序"进程打开文件表"中的多个文件描述符都是指向"系统打开文件表"的具体索引项，应用程序对内存缓存区进行的读写操作，待关闭文件后，操作系统会把数据从内存缓存区再写回磁盘。）</small>*

​	打开文件的时候，还要考虑**锁**的问题。因为文件是共享资源，如果没有考虑同步、互斥操作，那么文件数据的读写可能多个同时操作的进程相互覆盖文件数据，导致意料外的读写结果。

+ 一些操作系统和文件系统提供该功能（指文件锁）

+ 调节对文件的访问

+ 强制和劝告：
  + 强制 – 根据锁保持情况和需求拒绝访问
  + 劝告 – 进程可以查找锁的状态来决定怎么做

  *<small>(强制即要求同一个文件只能被一个进程占用，可以理解为文件级别的加锁。而劝告，就是进程建议操作系统加锁，但是操作系统办不办得到就得视情况而言了。)</small>*

> [十二、文件系统](https://blog.csdn.net/Alatebloomer/article/details/80075297)

## 12.5 文件分配

### 12.5.1 文件分配概述

+ 大多数文件都很小
  + 需要对小文件提供强力的支持
  + **块空间不能太大**

+ 一些文件非常大
  + 必须支持大文件 (64-bit 文件偏移）
  + 大文件访问需要相当高效

----

+ 如何为一个文件分配数据块  <small>(保证不管大文件按还是小文件都能正常且较高效地访问）</small>

+ 分配方式
  + **连续分配**
  + **链式分配**
  + **索引分配**

+ 指标   <small>(存储空间利用和读写访问效率都要考虑)</small>
  + 高效：如存储利用（外部碎片）
  + 表现：如访问速度

**连续分配**

- 文件头指定起始块和长度
- 位置/分配策略
  - 最先匹配，最佳匹配，...
- 优势
  - 文件读取表现好
  - 高效的顺序和随机访问 
- 劣势
  - 碎片
  - 文件增长问题
    - 预分配？
    - 按需分配？

![img](https://img-blog.csdn.net/20180425171707465?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0FsYXRlYmxvb21lcg==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)

*（连续分配有个问题，就是需要拓展的时候，要是后面和别人紧挨着，就不好扩展了。但是有些使用情况挺适合的，比如光盘等，写一次数据就不再修改了，所以连续分配正好，也不会浪费空间。但如果文件系统对磁盘空间分配的灵活性要求高，那么连续分配的方式就不合适了。）*

*（具体的文件分配策略，最先匹配、最佳匹配等等，可以参考靠内存分配的算法实现方式。）*

*（连续分配，空间连续，所以读效果好。但是扩展时如果后续连续空间不够，还需要挪动位置，在其他空间重新分配等长连续空间，再把执勤的数据复制过来，效率低。）*

**链式分配**

- 文件以数据块链表方式存储
- 文件头包含了到第一块和最后一块的指针
- 优点
  - 创建、增大和缩小容易
  - 没有碎片
- 缺点
  - 不可能进行真正的随机访问  <= <small>因为下一个结点依赖上一个结点，具有串行访问的特征</small>
  - 可靠性
    - 破坏一个链然后...(整个文件分配信息丢失)

![img](https://img-blog.csdn.net/20180425171857983?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0FsYXRlYmxvb21lcg==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)

**索引分配**

- 为每个文件创建一个名为**索引数据块**的非数据数据块

  - 到文件数据块的指针列表

- 文件头包含了索引数据块

- 优点

  - 创建、增大、缩小很容易
  - 没有碎片
  - 支持直接访问 

- 缺点

  - 当文件很小时，存储索引的开销
  - 如何处理大文件？

  ![img](https://img-blog.csdn.net/2018042517215322?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0FsYXRlYmxvb21lcg==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)

  *(索引数据块中，每个索引指向一个具体的数据块。索引数据块说明了其指向的文件数据所存在的数据块的位置。<=一个文件所占用的磁盘空间，由多个数据块(扇区)组成，类似进程占用的内存空间由好几个页组成。）*

  *（一般把索引数据块作为文件的元数据放到文件头中保存起来。使得获取数据块后可以直接将其加载到内存中，然后再根据说因数据块的信息查找到磁盘中对应的文件数据块。接着就按需加载磁盘数据块(扇区)到内存中就好了。）*

  *（不需要像链表一样需要把每个结点都加载到内存才能查找到下一个结点。这个一次性加载整个索引数据块，接着根据索引数据块的信息就可查找磁盘对应扇区了。）*

  *（有时候可能文件本身比索引数据块小，就显得加载的文件信息冗余量大。而且本身单个索引数据块能表示的数据块大小有限。如果拓展索引数据块，那就可能又得以数组、链表等形式扩展。）*

+ 链式索引块（IB+IB+...）

   <small>(大文件的索引数据块表示方式，数组or链表等。链式存储同样存在断链丢失问题。)</small>

+ 多级索引块（IB\*IB\*...）

  <small>（类似多级页表。访问开销可能更大，因为可能需要多次访问页表，才能找到最后的文件信息。但是相对灵活，支持大文件、小文件。）</small>

![img](https://img-blog.csdn.net/20180425172438654?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0FsYXRlYmxvb21lcg==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)

早期的Unix，采用多级索引方式（UFS多级索引分配）。<small>现在的操作系统页采取类似的形式来实现支持不同大小文件索引的机制。</small>

![img](https://img-blog.csdn.net/20180425172540513?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0FsYXRlYmxvb21lcg==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)

+ 文件头包含13个指针
  + 10 个指针指向数据块
  + 第11个指针指向间接数据块
  + 第12个指针指向二重间接数据块
  + 第13个指针指向三重间接数据块

+ 影响
  + 提高了文件大小限制阀值
  + 动态分配数据块，文件扩展很容易
  + 小文件开销小
  + 只为大文件分配间接数据块，大文件在访问数据块时需要大量查询

> [十二、文件系统](https://blog.csdn.net/Alatebloomer/article/details/80075297)

## 12.6 空闲空间列表

### 12.6.1 空闲空间列表-概述

*<small>(从磁盘角度看，空闲空间的管理)</small>*

*因为能分配给系统去放文件的一定得是空闲的数据块，所以操作系统要管理好空闲的数据块（列表）。*

+ 跟踪在存储中的所有未分配的数据块
+ 空闲空间列表存储在哪里？
+ 空闲空间列表的最佳数据结构是什么样的？

---

一种简单方法就是用位图表示数据块。用1bit来表示磁盘扇区是空闲还是被占用了。

+ 用位图代表空闲数据块列表

  + 111111111111111001110101011101111...

  +  如果`i = 0`表明数据块`i`是空闲, 反之则已分配

+ 使用简单但是可能会是一个big vector：

  + 160GB disk -> 40Mblocks -> 5MB worth of bits

  + 然而，如果空闲空间在磁盘中均匀分布，则找到“0”之前要扫描n/r 

    + n = 磁盘上数据块的总数
    + r = 空闲块的数目

    *<small>（采用位图机制，必须一开始就把位图从磁盘导入到内存中去，然后定期从内存更新位图信息到磁盘中去，以保证数据的一致性。避免突然断电，文件数据信息丢失，结果本来内存写了数据标记1，但是磁盘还没写入，其标记仍为空闲0，导致后面恢复过程该磁盘数据丢失，因为内存一看磁盘位图标记0，以为这块区域没有操作过，实际是操作过了，但是内存没来得及更新磁盘对应的位图数据而已。）</small>*

    *<small>（扫描n/r，也就是空闲块数量越多，扫描空闲块所消耗的事件相对也短。n/r<=大致的开销比例。）</small>*

+ 需要保护：

  + 指向空闲列表的指针
  + 位图
    + 必须保存在磁盘上
    + 在内存和磁盘拷贝可能有所不同
    + 不允许`block[i]`在内存中的状态为`bit[i]=1`而在磁盘中`bit[i]=0`。
  + 解决：
    + 在磁盘上设置`bit[i]=1`
    + 分配`block[i]`
    + 在内存中设置`bit[i]=1`

  *<small>(主要讲怎么解决内存表示的空闲块信息和磁盘记录的空闲块信息不一致的问题。因为可能突然断电什么的，导致内存修改的位图信息还没来得及写回硬盘。)</small>*

  *<small>(解决方法，确保修改位图信息的操作先在磁盘进行，然后才分配具体的`block[i]`，最后才是内存对应的位图信息`bit[i]`修改。这样就保证了一致性。就算突然断电，顶多就是磁盘哪里多个几个被标记成"已使用"但是实际未被使用的数据块，但是内存也没有往硬盘写数据，没有存在数据丢失的问题，那就ok了。)</small>*

  *<small>(注意：如果先分配`block[i]`再修改磁盘`bit[i]`，就可能出现分配了`block[i]`，但是磁盘没来得及设置`bit[i]=1`，这块磁盘又被其他进程再次分配`block[j]`，可能导致后续文件访问冲突、异常。)</small>*

其他管理磁盘空闲空间的数据结构

+ 链式列表
+ 分组列表

![img](https://img-blog.csdn.net/2018042517412824?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0FsYXRlYmxvb21lcg==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)

---

回顾前面内容。

+ 虚拟文件系统

  对上层屏蔽底层不同文件系统实现的差异性，为上层提供统一的、简洁的文件操作API接口。

+ 数据块缓存

  有效的较少操作系统对硬盘(磁盘)直接的IO访问次数，而是把数据先缓存到内存对应的文件缓冲区，加快文件访问效率。

+ 打开文件的数据结构

  通过文件描述符(操作系统维护的"打开文件表"的索引)查找到对应文件的各种信息(文件索引数据块等)，进而快速查找到对应的文件位置，快速访问文件。(当然也会加载到内存缓冲区，之后也是对内存操作，关闭文件后，操作系统再把数据写回硬盘/磁盘中)。

+ 文件分配

  文件不同的分配方式，应对不同的读写方式（顺序、随机），文件拓展（缩小、扩大、修改等），还有就是以什么数据结构/形式存储（数组、链表、文件索引数据块等形式实现文件分配）。整体实现思想，其实可以参考内存分配的页表管理等机制。

+ 空闲空间列表

  基于链式管理（链表）、基于位图`bit[i]`（小数组）等形式管理磁盘中空闲空间。

> [十二、文件系统](https://blog.csdn.net/Alatebloomer/article/details/80075297)

## 12.7 多磁盘管理 - RAID

### 12.7.1 RAID概述

#### 磁盘分区

+ 通常磁盘通过分区来最大限度减小寻道时间
  + 一个分区是一组柱面的集合
  + 每个分区都可视为逻辑上独立的磁盘

![img](https://img-blog.csdn.net/20180425183401657?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0FsYXRlYmxvb21lcg==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)

*（盘片不断旋转，上面磁头前后来回移动。整个过程中最慢的就是，磁头的前后移动。）*

*<small>(寻完一个道以后，在一个圈里有不同的扇区，探针读取扇区中的数据(因为磁头在旋转)，将读取到的数据送到内存中去。)</small>*

一个典型的文件系统，由分区组成。磁盘可以拆分成不同的分区，不同的分区可以由不同的文件系统组成。

+ 分区：硬件磁盘的一种适合操作系统指定格式的划分

+ 卷：一个拥有一个文件系统实例的可访问的存储空间

  + 通常常驻在磁盘的单个分区上。

  *<small>（更一步扩展，可以把多个disk当成一个卷来管理。这样就可以把一个文件系统拓展到多个磁盘上去。）</small>*

![img](https://img-blog.csdn.net/20180425185150224?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0FsYXRlYmxvb21lcg==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)

*（图片左侧把磁盘1号拆分成2个区，分别存放不同的文件系统；右侧两个磁盘合成一个分区，一个文件系统管理两个磁盘。引出问题，如果两个磁盘并行工作，会不会提高祠旁的访问效率？如果两个磁盘存同样的内容，是不是能提高磁盘的可靠性？）*

​	早期提出RAID之前，磁盘访问效率不高，且容易损坏（早期磁盘）。所以就有人提出，能不能用多个便宜的磁盘并行工作，通过冗余的方式提高数据的可靠性，进而实现一种又高效、又可靠的磁盘访问存储介质。

+ 使用多个并行磁盘来增加
  + 吞吐量(通过并行)
  + 可靠性和可用性 (通过冗余)

+ RAID - 冗余磁盘阵列（Redundant Array of Inexpensive Disks)
  + 各种磁盘管理技术
  + RAID levels：不同RAID分类（如，RAID-0，RAID-1，RAID-5等）

+ 实现
  + 操作系统内核：存储/卷管理
  + RAID硬件控制器(I/O)

  *（可以把RAID功能在硬件上实现，也可软件上实现。所谓软RAID和硬RAID。软RAID，操作系统在文件系统之下，磁盘驱动之上，多一个RAID层，完成磁盘阵列的管理。软RAID，把实现放在芯片里，一般是硬件主板上，这样操作系统对RAID无感知，看到的就是一个已经被RAID控制器映射出来的虚拟硬盘。硬RAID实现的话，操作系统把这个虚拟硬盘照常当作普通的文件系统来使用，而RAID的阵列管理则交由底层一硬件RAID控制器完成。）*

---

#### RAID-0

​	<small>为什么RAID-0能提高磁盘访问速度？与计算机组曾原理的内存组织交叉类似，把数据放到每个独立的硬盘里面，而每个硬盘能相对独立地并行工作。这样就可以实现数据并行访问。</small>

+ 数据分成多个子块，存储在独立的磁盘中
  + 和内存交叉相似
+ 通过更大的有效块大小来提供更大的磁盘带宽

RAID-0，把数据均匀地分布到不同的磁盘中去，这样访问磁盘数据时，就能并行访问，速度快，这就是RAID-0提高吞吐率的方法。

*（如果要求读取的数据，分布在不同的三个硬盘上，那么可以并行读取三个硬盘的数据（到内存缓存中），相当于把原本文件数据访问速度提高到原本的3倍。）*

![img](https://img-blog.csdn.net/20180425190654181?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0FsYXRlYmxvb21lcg==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)

---

#### RAID-1

​	RAID-1的组成方式，提高磁盘数据的可靠性。原理简单，写数据的时候，往每个RAID阵列里的硬盘都写一份。读取的时候只要从其中一个RAID磁盘读取。可靠性提升，代价就是本来一次写操作，变成多次写操作。RAID-1，磁盘起到镜像作用，目的即提高数据的可靠性。

+ 可靠性成本增长
+ 读取性能线性增加
  + 向两个磁盘写入，从任何一个读取

![img](https://img-blog.csdn.net/20180425190818399?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0FsYXRlYmxvb21lcg==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)

---

#### RAID-4

RAID-4，整合RAID-0和RAID-1的优点。取特定的某个RAID盘用于奇偶校验（出错时纠错用），其他的盘像RAID-0一样均匀存放数据。

+ 数据块级磁带配有专用奇偶校验磁盘
  + 允许从任意一个故障磁盘中恢复
  + 例如：存储8，9，10，11，12，13，14，15，0，1，2，3

![img](https://img-blog.csdn.net/20180425191012420?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0FsYXRlYmxvb21lcg==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)

​	这里Disk1~Disk4均匀存储数据（RAID-0），可以提供数据的并行加载。1~4其中一个盘故障时，可以通过Parity Disk（存放奇偶校验信息的盘）根据另外3个完好的盘计算出故障的盘的信息。（仅一个盘故障时能恢复）

​	不管往Disk1~Disk4哪一个写数据，都需要往Parity Disk再写一份奇偶校验信息，性能瓶颈就是Parity Disk的读写太过频繁。

---

#### RAID-5

​	RAID-5，在RAID-4的基础上，把原本奇偶校验块给均匀分布到RAID阵列的磁盘中。这样校验是均匀的。数据访问也依然是并行的，保持了RAID-0和RAID-1的优点。既保证了可靠性（指最多能坏一个硬盘），又提高了效率（指阵列内所有盘并行访问）

![img](https://img-blog.csdn.net/20180425191257253?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0FsYXRlYmxvb21lcg==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)

---

+ 条带化和奇偶校验按"byte-by-byte"或者"bit-by-bit"
  + RAID-0/4/5：block-wise
  + RAID-3：bit-wise
+ 例如：在RAID-3系统中存储bit-string 101

奇偶校验信息，以块存储还是以bit存储？早期RAID-3采用bit形式来存储奇偶校验信息。但是bit实现的奇偶校验，太细粒，不如RAID-4和RAID-5实用，所以RAID-3理论上可行，但实际用的较多的还是RAID-5。

---

#### RAID-6

如何让RAID提高更多的容错性（指可以同时坏更多的硬盘且保证恢复），那么可以用RAID-6，通过两个冗余块的方式来实现。

+ RAID-5：每个条带块有一个奇偶校验块
  + 允许一个磁盘错误

+ RAID-6：两个冗余块
  + 有一种特殊的编码方式
  + 允许两个磁盘错误

---

+ RAID 0+1
+ RAID 1+0

可以使用RAID-0和RAID-1通过特殊的组合方式，即保证数据的容错率，也保证磁盘的访问效率。（不像RAID-5那么复杂，但是也实用）

![img](https://img-blog.csdn.net/20180425191710349?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0FsYXRlYmxvb21lcg==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)

> [十二、文件系统](https://blog.csdn.net/Alatebloomer/article/details/80075297)

## 12.8 磁盘调度

### 12.8.1 磁盘调度-概述

多次盘管理-RAID，提高磁盘访问效率和数据可靠性。

磁盘调度，在OS操作系统层面，重新组织IO请求的顺序，来有效减少磁盘IO访问产生的开销。

![img](https://img-blog.csdn.net/20180426104658366?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0FsYXRlYmxvb21lcg==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)

​	磁盘结构，循环来寻道，磁头移动来找位置，读取对应的扇区，完成整个数据的读取操作。一个磁盘可以有多个盘片，每个盘片都有对应的一个or两个磁头（正反面）。

​	所有的操作，旋转和磁头移动，都是机械操作。机械操作比内存的电子单元访问，速度慢超多个数量级。

---

#### 磁盘性能表示

+ 读取或写入时，磁头必须被定位在**期望的磁道**，并从所**期望的扇区**开始

+ 寻道时间
  + 定位到期望的磁道所花费的时间

+ 旋转延迟
  + 从零扇区开始处到到达目的处所花费的时间

**平均旋转延迟时间=磁盘旋转一周时间的一半**

*找到对应扇区的旋转延迟和具体的读扇区时间，整个综合就是磁盘的访问开销。*

![img](https://img-blog.csdn.net/20180426105258324?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0FsYXRlYmxvb21lcg==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)

寻道开销相对较大，因为磁头前后移动很慢。<small>（如果一i此顺序从里到外or从外到里就能直接获取到指定的数据，而不用反复移动磁头，那寻道时间就可减少。）</small>

- **寻道时间**是性能上区别的原因
- 对单个磁盘，会有一个I/O请求数目
- 如果请求是随机的，那么会表现很差

---

#### 先进先出(FIFO)算法

一种磁盘调度策略就是FIFO先进先出。多个进程发起磁盘IO请求，操作系统整理顺序后，交给磁盘IO去访问。

<small>由于本身进程对IO请求随机，所以磁盘调度序列也有很强的随机性。</small>

- 按顺序处理请求
- 公平对待所有进程

+ 在有很多进程的情况下，接近随机调度的性能

![img](https://img-blog.csdn.net/20180426105512959?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0FsYXRlYmxvb21lcg==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)

​	由于采用FIFO磁盘调度算法， 而进程的IO调度具有很强的随机性，使得磁盘IO调度序列随机性很高，磁头也就需要频繁来回移动。使得磁头总的移动距离很长，也就意味着磁盘IO访问开销很大。

---

#### 最短服务时间优先(SSTF)

最短服务优先<small>（类似前面进程调度的最短任务优先）</small>，使得磁盘IO调度顺序满足每次移动的磁盘距离是最少的。

- 选择**从磁臂当前位置需要移动最少**的I/O请求
- 总是选择最短寻道时间

![img](https://img-blog.csdn.net/20180426105655698?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0FsYXRlYmxvb21lcg==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)

​	如果磁盘有一小片区域是经常访问的，会导致磁头一直在这附近来回走动，而需要磁盘移动较远的磁盘IO访问请求久久得不到满足。产生饥饿现象（和前面磁盘调度一样，容易产生饥饿问题）

---

#### SCAN算法

SCAN方法，有个约束，磁头往某个方向移动时，满足完所有同方向的需求并到达边缘后，才反方向回来。

- **磁臂在一个方向上移动**，访问所有未完成的请求，直到磁臂到达**该方向上最后的磁道**<small>（可以优化，比如不一定每次都要访问到某一个方向的边缘后才返回，而是同一个方向没有任务后就返回。）</small>
- 调换方向
- 有时被称为elevator algorithm （类似电梯一样，直上直下）

![img](https://img-blog.csdn.net/20180426105856811?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0FsYXRlYmxvb21lcg==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)

SCAN算法，相对公平，每个磁盘IO访问请求都能及时得到满足。

---

#### C-SCAN算法

C-SCAN，在原本SCAN上修改，只往一个方向，比如原本从外到里，到中间了直接继续向前，变成从里到外。

- 限制了**仅在一个方向**上扫描
- 当最后一个磁道也被访问过了后，磁臂返回到磁盘的**另外一端**再次进行

![img](https://img-blog.csdn.net/20170912155041262?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdGp1eWFubWluZw==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center)

对于某些请求来说更加公平，且效率更高

---

#### C-LOOK算法

+ C-SCAN的改进

- 磁臂先到达该方向上**最后一个请求处**，然后立即反转。

![img](https://img-blog.csdn.net/20170912155547829?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdGp1eWFubWluZw==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center)

不再是到达磁盘的某个终点边界再返回，而是处理完某一个方向的最后一个请求就立即切换方向移动。

---

+ 在SSTF、SCAN及CSCAN几种调度算法中，都可能出现磁臂停留在某处不动的情况，例如进程反复请求对某一磁道的I/O操作。我们把这一现象称为"磁臂粘着"(arm stickiness)。

+ N-Step-SCAN算法是将磁盘请求队列分成若干个长度为N的子队列，磁盘调度按照FCFS算法依次处理这些子队列。而每处理一个队列时又是按SCAN算法，对一个队列处理完后，再处理其他队列。

+ 当正在处理某子队列时，如果又出现新的磁盘I/O请求，便将新请求进程放入其他队列，这样就可避免出现黏着现象。
+ FSCAN算法实质上时对N步SCAN算法的简化，即FSCAN只将磁盘请求队列分成两个子队列。<small>（当前不少操作系统就采用这种方式实现磁盘调度，保证公平性、均匀性和高效率的磁盘请求，使整体磁头寻道的开销降低。）</small>
+ 一个是由当前所有请求磁盘I/O的进程形成的队列，由磁盘调度按SCAN算法进行处理。在处理某队列期间，将新出现的所有请求磁盘I/O的进程，放入另一个等待处理的请求队列。这样，所有的新请求都将被推迟到下一次扫描时处理。

> [十二、文件系统](https://blog.csdn.net/Alatebloomer/article/details/80075297)
>
> [[操作系统] - No.3 磁盘调度算法：FCFS算法、SSTF算法、SCAN算法、 C-SCAN算法、 电梯算法等](https://blog.csdn.net/tjuyanming/article/details/77945887)

